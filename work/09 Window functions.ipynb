{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fe3dbd3-0822-481c-9ab8-c6e6e6246dd9",
   "metadata": {},
   "source": [
    "# 08 Window functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac34614-6d7c-498b-a3ab-f8cc04f21823",
   "metadata": {},
   "source": [
    "Window functions operate on a group of rows, referred to as a window, and *calculate a return value for each row based on the group of rows*. Window functions are useful for processing tasks such as calculating a moving average, computing a cumulative statistic, or accessing the value of rows given the relative position of the current row.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0320607-caf4-4b6a-805c-a0524de94487",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* take some from https://www.databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html  ( from 2015 )\n",
    "* https://medium.com/expedia-group-tech/deep-dive-into-apache-spark-window-functions-7b4e39ad3c86\n",
    "* Add examples\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f943f9-abf5-4c7d-b709-0f888b9f38b8",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Event time processing\n",
    "[SDG] chapter 22. [TODO:Maybe move this part to sdg folder?]\n",
    "\n",
    "Event time is the time that is embedded in the data itself. It is most often, *though not required\n",
    "to be*, the time that an event actually occurs. This is important to use because it provides a\n",
    "more robust way of comparing events against one another. The challenge here is that event\n",
    "data can be late or out of order. This means that the stream processing system must be able to\n",
    "handle out-of-order or late data.\n",
    "\n",
    "\n",
    "The [SDG] has a full chapter on this topic. Here we just touch the windowing functionality.\n",
    "\n",
    "Contrary to the SQL window functions which aggregate same values [TODO: is it correct?], the event time window runs on a time span.\n",
    "\n",
    "Note: It is also possible to build windows based on amount of rows (e.g. \"take the average of the last 500 rows\"). Look for \"count-based windows\"\n",
    "\n",
    "\n",
    "# Windows on Event Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179523cf-bc2c-43bf-8085-0a2aaf03d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col\n",
    "\n",
    "datapath = \"../data/sdg/\"\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "#static = spark.read.json(\"/data/activity-data\")\n",
    "\n",
    "static = spark.read.json(datapath + \"/activity-data/part-00000*.json\")\n",
    "\n",
    "streaming = spark\\\n",
    ".readStream\\\n",
    ".schema(static.schema)\\\n",
    ".option(\"maxFilesPerTrigger\", 10)\\\n",
    ".json(datapath + \"/activity-data\")\n",
    "#streaming.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ec7536-dffb-404d-8388-4b07e1d1b0c5",
   "metadata": {},
   "source": [
    "The first step in event-time analysis is to convert the timestamp column into the proper Spark\n",
    "SQL timestamp type. Our current column is unixtime nanoseconds (represented as a long),\n",
    "therefore we’re going to have to do a little manipulation to get it into the proper format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cddbe2-4ba8-4dab-b11c-5b05dbbde88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "withEventTime = streaming.selectExpr(\"*\",\n",
    "\"cast(cast(Creation_Time as double)/1000000000 as timestamp) as event_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965269e3-5a46-4fe0-92ca-e1090b0e35cb",
   "metadata": {},
   "source": [
    "## Tumbling Windows - non overlapping intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd0a902-d5fc-4248-8047-fe197ccac254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many events in every 10 minute interval\n",
    "withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\")).count()\\\n",
    ".writeStream\\\n",
    ".queryName(\"events_per_window\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dde825-ddc1-435a-aecd-aa7d19ca9aa3",
   "metadata": {},
   "source": [
    "The output data is in the memory sink **for debug only**, so we can use SQL to query it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc58cf3f-f645-44a1-8b18-268fc7ef583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM events_per_window\").show(truncate=False)\n",
    "# you can of course do 'SELECT count'   or 'SELECT window.start' etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917373b4-af29-4ebf-8887-77124140f2b9",
   "metadata": {},
   "source": [
    "Perform aggregation on multiple columns (event_time,User):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b26687-1c70-4828-ae14-593ba29c4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "withEventTime.groupBy(window(col(\"event_time\"), \"10 minutes\"), \"User\").count()\\\n",
    ".writeStream\\\n",
    ".queryName(\"events_per_window_user\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c49a7-2436-4788-aa96-3a5fbf38ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM events_per_window_user\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ce4887-0324-4e43-8f0a-bc9616f07960",
   "metadata": {},
   "source": [
    "## Sliding Windows\n",
    "Let's count events during the last 60 minutes, moving by an 8 minutes window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e15e3-e12b-4d28-b261-c033aa6ba671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col\n",
    "withEventTime.groupBy(window(col(\"event_time\"), \"60 minutes\", \"8 minutes\"))\\\n",
    ".count()\\\n",
    ".writeStream\\\n",
    ".queryName(\"events_per_window_60_8\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d265eed-fa42-48b2-accf-dbb5ac958fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM events_per_window_60_8 ORDER BY window.start ASC\").show(33,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4475e06b-982b-4cb8-aafa-f3f94a7a25d9",
   "metadata": {},
   "source": [
    "### Handling Late Data with Watermarks\n",
    "The preceding examples are great, but they have a flaw. We never specified how late we expect\n",
    "to see data. This means that Spark is going to need to store that intermediate data forever because\n",
    "we never specified a watermark, or a time at which we don’t expect to see any more data. This\n",
    "applies to all stateful processing that operates on event time. We must specify this watermark in\n",
    "order to age-out data in the stream (and, therefore, state) so that we don’t overwhelm the system over a long period of time.\n",
    "\n",
    "\n",
    "A **watermark** is an amount of time following a given event or set of events after\n",
    "which we do not expect to see any more data from that time.\n",
    "\n",
    "We need to specify watermarks because if we did not, we’d need to keep all of our windows around forever, expecting them to be updated forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60862362-6122-4e8d-aa12-75ca21cdb22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "withEventTime\\\n",
    ".withWatermark(\"event_time\", \"30 minutes\")\\\n",
    ".groupBy(window(col(\"event_time\"), \"10 minutes\", \"5 minutes\"))\\\n",
    ".count()\\\n",
    ".writeStream\\\n",
    ".queryName(\"events_per_window_WM\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1548b6d-316b-4c39-9fe5-5acef9894538",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM events_per_window_WM ORDER BY window.start ASC\").show(33,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857fa2d4-edcc-4042-92a4-4bfecb7fb044",
   "metadata": {},
   "source": [
    "### Dropping Duplicates in a Stream\n",
    "\n",
    "In this example, we consider a row as duplicate if it has the same User and event_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c5d2e-b7f7-4fcf-a782-68dafc081dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "query=withEventTime\\\n",
    ".withWatermark(\"event_time\", \"5 seconds\")\\\n",
    ".dropDuplicates([\"User\", \"event_time\"])\\\n",
    ".groupBy(\"User\")\\\n",
    ".count()\\\n",
    ".writeStream\\\n",
    ".queryName(\"pydeduplicated\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7870f4bc-7f28-473b-9400-d0b191ae7ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM pydeduplicated\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f0347c-f7cf-498a-b790-e99325caacef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the status of our queries?\n",
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dfb5de-befd-4a77-985a-af2b050ef454",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.recentProgress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4546df80-f789-45b9-a26c-38cdf74afe8f",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "The following topics also appear in the chapter, but not enough time to discuss them.\n",
    "\n",
    "## Arbitrary Stateful Processing\n",
    "### Time-Outs\n",
    "### Output Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a2ef95-853b-45de-8d71-bea9085b8728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
