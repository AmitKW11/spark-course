{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Apache Spark UI Tutorial\n",
    "## Introduction\n",
    "\n",
    "This tutorial will help you understand Spark's Web UI - your dashboard for monitoring and debugging Spark applications. The Spark UI provides valuable insights into your application's performance, resource usage, and execution flow.\n",
    "\n",
    "All information on this notebook is based on Apache Spark 3.5 UI Guide, available at:\n",
    "\n",
    "https://spark.apache.org/docs/latest/web-ui.html\n",
    "\n",
    "## Getting Started with Spark UI\n",
    "\n",
    "The Spark UI automatically launches when you start a Spark application and is typically available at http://localhost:4040."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T19:09:37.204889200Z",
     "start_time": "2025-05-13T19:09:37.103116600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 23:15:24 WARN Utils: Your hostname, ashrafk resolves to a loopback address: 127.0.1.1; using 192.168.1.205 instead (on interface wlo1)\n",
      "25/05/13 23:15:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/13 23:15:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI is available at: http://localhost:4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkUIJobsTab\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark UI is available at: http://localhost:4040\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. The Jobs Tab: Understanding Your Spark Workload\n",
    "\n",
    "The Jobs tab is your primary view into Spark's execution. A \"job\" is created whenever you execute an action on your data (like `collect()`, `count()`, or `save()`).\n",
    "\n",
    "When you visit the Jobs tab, you'll see:\n",
    "\n",
    "- A list of all jobs with their status (running, completed, or failed)\n",
    "- Execution times showing how long each job took\n",
    "- Breakdown of stages within each job\n",
    "- Links to more detailed views\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Jobs Tab Description](res/jobs_tab.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Key Components of the Jobs Tab:\n",
    "\n",
    "- **Job ID**: Each job gets a unique numerical identifier, starting at 0 for the first job in your application. In our example, we see jobs with IDs 4, 5, 6, and 7.\n",
    "- **Description**: This column shows which operation triggered the job. Notice how operations like count() and show() appear here with line numbers from your code. This helps you connect specific actions in your code to the jobs they create.\n",
    "- **Duration**: How long each job took to execute, measured in seconds. This is invaluable for identifying performance bottlenecks. In our example, durations range from 0.4 seconds to 17 seconds.\n",
    "- **Stages**: Spark breaks each job into \"stages\" that can be executed independently. The \"Succeeded/Total\" format shows how many stages have completed out of the total. Notice how some jobs have just 1 stage while others have 3 stages - more complex operations typically require more stages.\n",
    "- **Tasks**: These are the individual units of work distributed across your cluster. Simple jobs might have few tasks, while complex operations on large datasets can have thousands. The \"Succeeded/Total\" format shows completion progress.\n",
    "\n",
    "### Active vs. Completed Jobs:\n",
    "The Jobs tab separates currently running jobs from completed ones:\n",
    "\n",
    "- **Active Jobs**: Job #7 is still running, as indicated by the \"running\" status in the Tasks column and incomplete stages (0/2).\n",
    "- **Completed Jobs**: Jobs #4, 5, and 6 have finished execution. You can see all their tasks have succeeded (e.g., 9/9 and 1/1) and their stages are complete.\n",
    "\n",
    "### What This Tells Us About Spark Execution:\n",
    "\n",
    "Simple actions like show with small results (Job #6) are quick and require minimal processing (just 0.4s and one stage).\n",
    "More complex operations (Jobs #4 and #5) require multiple stages and more tasks, which often means data shuffling between stages.\n",
    "In-progress jobs can be monitored in real-time to track their execution progress.\n",
    "\n",
    "When developing Spark applications, pay close attention to jobs with long durations or many stages, as these are prime candidates for optimization.\n",
    "In the next sections, we'll look at what happens inside these jobs by examining the Stages tab and visualizations of execution."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's create a code example to demonstrates :\n",
    "run this code:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Jobs Tab Example\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [(i, f\"Product_{i % 5}\", i * 10) for i in range(10000)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"product\", \"value\"])\n",
    "\n",
    "# Register as a temporary view\n",
    "df.createOrReplaceTempView(\"products\")\n",
    "\n",
    "# Job 1: Count the total number of records\n",
    "print(\"Job 1: Counting records\")\n",
    "record_count = df.count()\n",
    "print(f\"Total records: {record_count}\")\n",
    "time.sleep(1)  # Pause to separate jobs in the UI\n",
    "\n",
    "# Job 2: Filter and aggregate\n",
    "print(\"Job 2: Filtering and aggregating\")\n",
    "filtered = df.filter(df.value > 50)\n",
    "filtered_count = filtered.count()\n",
    "print(f\"Records with value > 50: {filtered_count}\")\n",
    "time.sleep(1)  # Pause to separate jobs in the UI\n",
    "\n",
    "# Job 3: Group by operation (creates a shuffle)\n",
    "print(\"Job 3: Performing groupBy operation\")\n",
    "summary = df.groupBy(\"product\").count()\n",
    "summary.show()\n",
    "time.sleep(1)  # Pause to separate jobs in the UI\n",
    "\n",
    "# Job 4: SQL query execution\n",
    "print(\"Job 4: Executing SQL query\")\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        product,\n",
    "        AVG(value) as avg_value,\n",
    "        MAX(value) as max_value\n",
    "    FROM products\n",
    "    GROUP BY product\n",
    "    ORDER BY avg_value DESC\n",
    "\"\"\")\n",
    "sql_result.show()\n",
    "\n",
    "print(\"\\nCheck the Jobs tab in the Spark UI at http://localhost:4040\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What to Look for in the Jobs Tab:\n",
    "After running this code, go to the Jobs tab in the Spark UI where you'll see:\n",
    "[SCREENSHOT 1: Jobs Overview]\n",
    "Take a screenshot of the main Jobs tab showing all 4 jobs with their descriptions, durations, and stages. This clearly shows how each action in your code creates a separate job in Spark.\n",
    "\n",
    "Multiple Jobs Listed: You'll see 4 distinct jobs created by the actions in your code\n",
    "Job Descriptions: Each job will have a description that maps directly to your code:\n",
    "\n",
    "Job for df.count() (Job 1)\n",
    "Job for filtered.count() (Job 2)\n",
    "Job for summary.show() (Job 3)\n",
    "Job for SQL query execution through sql_result.show() (Job 4)\n",
    "\n",
    "\n",
    "Job Details: For each job, you'll see:\n",
    "\n",
    "Duration: How long each job took to execute\n",
    "Stages: Number of stages each job was broken into\n",
    "Tasks: Number of parallel tasks executed\n",
    "\n",
    "\n",
    "\n",
    "Direct Connection to Your Code:\n",
    "Each job in the Jobs tab directly corresponds to an action in your code:\n",
    "\n",
    "Job 1 comes from the df.count() action - a simple action that typically results in a single stage\n",
    "Job 2 comes from the filtered.count() action - another simple job, but preceded by a filter operation\n",
    "\n",
    "[SCREENSHOT 2: Job Details Page]\n",
    "Take a screenshot of the detailed view of Job 3 (the groupBy operation) after clicking on it. This shows how a single line of code df.groupBy(\"product\").count() translates into multiple stages with different operations.\n",
    "\n",
    "Job 3 comes from the summary.show() action - this job involves a groupBy operation which creates a shuffle and typically results in multiple stages\n",
    "Job 4 comes from the SQL query execution - the most complex job with grouping, aggregation, and sorting that creates multiple stages\n",
    "\n",
    "[SCREENSHOT 3: DAG Visualization]\n",
    "Take a screenshot of the DAG visualization for the SQL query job (Job 4). This shows the directed acyclic graph of operations that Spark creates from your SQL query, with nodes representing operations and edges showing data flow.\n",
    "The key insight is that transformations in your code (like filter() and groupBy()) don't create jobs on their own - they are only executed when an action (like count() or show()) is called.\n",
    "[SCREENSHOT 4: Event Timeline]\n",
    "Take a screenshot of the Event Timeline section showing the chronological execution of all 4 jobs. This visualizes how jobs execute in sequence but may have internal parallelism.\n",
    "By examining the Jobs tab, you can see exactly:\n",
    "\n",
    "Which parts of your code trigger execution\n",
    "How complex operations are broken into stages\n",
    "How long each operation takes\n",
    "The logical progression of your Spark application\n",
    "\n",
    "[SCREENSHOT 5: Task Distribution]\n",
    "Take a screenshot of the task details for a shuffle stage in Job 4, showing the distribution of work across executors. This helps identify if certain tasks are taking much longer than others (indicating data skew).\n",
    "Understanding this relationship between your code and the Jobs tab helps you identify performance bottlenecks and optimize your Spark applications by focusing on the most expensive operations. The Jobs tab serves as your main window into Spark's execution model, showing the direct translation from your high-level code to Spark's distributed execution plan."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Understanding Execution Through Visualizations\n",
    "\n",
    "### Event Timeline\n",
    "\n",
    "The Event Timeline provides a chronological view of your Spark application's execution, showing:\n",
    "\n",
    "- When each job, stage, and task ran\n",
    "- How long each component took to execute\n",
    "- Parallel execution across your cluster\n",
    "- Wait times and potential bottlenecks\n",
    "\n",
    "Pay attention to \"gaps\" in the timeline, which may indicate scheduling delays or resource contention. If you see tasks executing serially rather than in parallel, you might need to **adjust your partitioning**. Long-running tasks that delay an entire stage completion are often signs of data skew.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Jobs Tab Description](res/Event_timeLine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. **Time Progression**: The horizontal axis shows time, with timestamps marking specific points (17:42 through 17:50 in this example).\n",
    "\n",
    "2. **Component Layers**: The timeline is divided into horizontal sections:\n",
    "   - **Executors**: Shows when executors are added or removed from your application\n",
    "   - **Jobs**: Shows when jobs are running, succeeded, or failed\n",
    "\n",
    "3. **Color Coding**: Different colors represent different states:\n",
    "   - **Blue** (for Executors): Added executors\n",
    "   - **Pink/Red**: Removed executors or failed jobs\n",
    "   - **Blue** (for Jobs): Succeeded jobs\n",
    "   - **Green**: Currently running jobs\n",
    "\n",
    "4. **Key Events Visible**:\n",
    "   - At the beginning, you can see when executors were added to the application\n",
    "   - Throughout the timeline, small blue bars represent completed jobs\n",
    "   - On the right side, a green \"count\" job is currently running\n",
    "\n",
    "### What to Look For in the Event Timeline:\n",
    "\n",
    "The Event Timeline helps you identify:\n",
    "\n",
    "- **Executor Lifecycle**: When executors join or leave your application\n",
    "- **Job Execution Patterns**: When jobs start and finish\n",
    "- **Idle Periods**: Gaps in the timeline where no activity occurs\n",
    "- **Concurrency**: Multiple jobs or tasks running in parallel\n",
    "- **Long-Running Operations**: Jobs that span significant portions of the timeline\n",
    "\n",
    "While this example shows a relatively simple execution pattern, in more complex applications you would see many more parallel activities, helping you visualize how effectively your application utilizes cluster resources.\n",
    "\n",
    "The Event Timeline is particularly valuable when diagnosing performance issues, as it can reveal execution bottlenecks, resource contention, or inefficient scheduling patterns.\n",
    "\n",
    "You can enable zooming (checkbox at the top) to examine specific time periods in more detail, which is especially useful for complex applications with many overlapping activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## DAG Visualization: Understanding Spark's Execution Plan\n",
    "\n",
    "The Directed Acyclic Graph (DAG) visualization provides a clear picture of how Spark structures the operations in your job. This visualization is extremely valuable for understanding the logical flow of data through your Spark application\n",
    "\n",
    "- Each node represents an operation (like map, filter, join)\n",
    "- Arrows show data flow and dependencies\n",
    "- Stages are separated by shuffle boundaries (where data needs to be redistributed)\n",
    "\n",
    "The DAG helps you identify expensive operations like shuffle-heavy joins or cartesian products. Multiple arrows converging on a single operation often indicate a join or aggregation that could become a bottleneck. Wide transformations (those that require shuffles) create stage boundaries and are typically more expensive than narrow transformations..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Jobs Tab Description](res/DAG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### What You're Seeing in This DAG:\n",
    "\n",
    "This visualization shows how a Spark job is broken down into multiple stages (Stage 10 and Stage 11) and the operations within each stage:\n",
    "\n",
    "1. **Stage Boundaries**: Each pink box represents a distinct stage. Stages are separated at points where data must be redistributed across the cluster (called shuffle boundaries).\n",
    "\n",
    "2. **Operations Within Stages**: The blue boxes represent specific operations:\n",
    "   - **Parallelize**: Creating a distributed dataset from data\n",
    "   - **Scan**: Reading data from a source\n",
    "   - **Exchange**: Redistributing data across the cluster (shuffle operation)\n",
    "   - **WholeStageCodegen**: Spark's optimization that compiles multiple operations into efficient bytecode\n",
    "   - **MapPartitionsInternal**: An internal transformation operating on each partition\n",
    "\n",
    "3. **Data Flow Direction**: The arrows show the direction of data flow, always moving downward within stages and then across to the next stage.\n",
    "\n",
    "4. **Shuffle Boundary**: The curved line connecting the \"Exchange\" operation from Stage 10 to Stage 11 represents a shuffle - a point where data must be redistributed across the cluster. Shuffles create stage boundaries in Spark.\n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "The DAG visualization reveals important insights about your Spark application:\n",
    "\n",
    "- **Performance Bottlenecks**: Exchange operations (shuffles) are often performance bottlenecks because they involve network transfer and disk I/O. In this example, there's a shuffle between Stages 10 and 11.\n",
    "\n",
    "- **Optimization Opportunities**: The presence of \"WholeStageCodegen\" indicates that Spark is applying code generation optimizations to improve performance by combining multiple operations.\n",
    "\n",
    "- **Execution Dependencies**: You can see which operations must complete before others can begin. For example, in Stage 10, the \"Scan\" must complete before \"WholeStageCodegen (1)\" can start.\n",
    "\n",
    "- **Parallelization Potential**: Operations within a stage can be parallelized, but stages must execute in sequence when there are dependencies between them.\n",
    "\n",
    "Understanding the DAG helps you reason about how your code translates into actual execution steps. When optimizing Spark applications, you'll often refer to this visualization to identify opportunities for improvement, such as reducing the number of shuffle operations or ensuring that data is properly partitioned.\n",
    "\n",
    "In complex jobs with many stages, the DAG becomes even more valuable as it helps you visualize the execution flow that would otherwise be difficult to comprehend."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's create a code example to demonstrates :\n",
    "run this code:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Simple DAG Example\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [(i, f\"product_{i % 3}\", i * 10) for i in range(50)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"product\", \"value\"])\n",
    "\n",
    "# Apply a series of transformations that will create a visible DAG\n",
    "# Step 1: Filter the data\n",
    "filtered_df = df.filter(col(\"value\") > 20)\n",
    "\n",
    "# Step 2: Group by product\n",
    "summary_df = filtered_df.groupBy(\"product\").agg(\n",
    "    avg(\"value\").alias(\"avg_value\")\n",
    ")\n",
    "\n",
    "# Execute an action to materialize the DAG\n",
    "print(\"Results after transformation:\")\n",
    "summary_df.show()\n",
    "\n",
    "print(\"\\nCheck the Spark UI at http://localhost:4040\")\n",
    "print(\"Go to the Jobs tab, click on the job, then view the DAG Visualization\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. The Stages Tab: Diving Deeper Into Execution\n",
    "\n",
    "Stages are sets of tasks that can be executed in parallel without data shuffling. The Stages tab shows:\n",
    "\n",
    "- Detailed metrics for each stage\n",
    "- Input/output data sizes\n",
    "- Task distribution and execution times\n",
    "- Whether stages completed successfully or failed\n",
    "\n",
    "When examining the Stages tab, look for:\n",
    "\n",
    "- **Task skew**: When some tasks take much longer than others in the same stage\n",
    "- **Spill metrics**: Data spilled to disk indicates memory pressure\n",
    "- **Shuffle read/write sizes**: Large shuffles can slow down your application\n",
    "- **Input/output records**: Help identify data flow bottlenecks\n",
    "\n",
    "Clicking on a specific stage provides a task-level view where you can see outliers and understand the distribution of work across your cluster.\n",
    "This view is invaluable for understanding the actual work happening in your Spark application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Jobs Tab Description](res/Stage_image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Understanding the Stages Tab Structure:\n",
    "\n",
    "The Stages tab is divided into three key sections, each showing stages in different states:\n",
    "\n",
    "1. **Active Stages**: Currently running stages\n",
    "   - In our example, Stage 13 is active with 0/4 tasks completed (4 running)\n",
    "   - Shows real-time progress of execution\n",
    "\n",
    "2. **Pending Stages**: Stages waiting for resources or dependent stages to complete\n",
    "   - Stage 14 is pending with 0/200 tasks completed\n",
    "   - These stages are queued but haven't started execution yet\n",
    "\n",
    "3. **Completed Stages**: Stages that have finished execution\n",
    "   - Our example shows multiple completed stages (5-12)\n",
    "   - Provides historical performance data for analysis\n",
    "\n",
    "### Key Metrics To Monitor:\n",
    "\n",
    "Each stage displays several important metrics:\n",
    "\n",
    "- **Stage ID**: Unique identifier for each stage\n",
    "- **Pool Name**: The scheduler pool handling this stage (affects resource allocation)\n",
    "- **Description**: The operation being performed (like \"show\" or \"foreach\")\n",
    "- **Duration**: How long the stage took to execute (ranging from 0.2s to 64ms in our example)\n",
    "- **Tasks**: Shows completed/total tasks and visualizes progress\n",
    "- **Shuffle Read/Write**: Amount of data transferred during shuffle operations\n",
    "  - Note stages with significant shuffle write (1205.5 KiB) which indicate data redistribution\n",
    "\n",
    "### What This Tells Us About Performance:\n",
    "\n",
    "Looking at the completed stages, we can observe:\n",
    "- Most stages complete quickly (0.3s-0.5s)\n",
    "- Stage 6 took significantly longer (64ms) than others - a potential bottleneck\n",
    "- Several stages have identical shuffle write sizes (1205.5 KiB), suggesting similar data volume processing\n",
    "- Task counts vary (some have 1 task, others have 4), indicating different levels of parallelism\n",
    "\n",
    "By examining these metrics, you can identify which stages contribute most to your job's overall execution time and focus your optimization efforts accordingly.\n",
    "\n",
    "You can click on any stage ID to view more detailed information about that specific stage, including a visual representation of its internal operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stage Detail View: Understanding Operation Flow\n",
    "\n",
    "When you click on a specific stage in the Stages tab, you'll see a detailed view of the operations within that stage, as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Jobs Tab Description](res/stage_image2_dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Decoding the Stage Internals:\n",
    "\n",
    "This visualization shows the execution plan for Stage 15, revealing how data flows through various operations:\n",
    "\n",
    "1. **Parallel Input Paths**: The stage begins with two parallel data paths, each starting with a \"ShuffledRowRDD\" operation followed by \"Exchange\" - indicating data coming from previous stages through a shuffle.\n",
    "\n",
    "2. **Data Transformations**: Each path goes through a series of operations:\n",
    "   - **MapPartitionsRDD**: Applying transformations to each partition\n",
    "   - **WholeStageCodegen**: Spark's optimization that combines operations into efficient bytecode\n",
    "   - **Exchange**: Points where data is redistributed across the cluster\n",
    "\n",
    "3. **Data Flow Convergence**: The two paths merge at the \"ZippedPartitionsRDD2\" operation, combining data from both sources.\n",
    "\n",
    "4. **Final Processing**: After merging, the data goes through additional mapping operations and a final exchange before completing the stage.\n",
    "\n",
    "### Performance Insights From This View:\n",
    "\n",
    "This detailed view reveals important aspects of Spark's execution strategy:\n",
    "\n",
    "- **Code Generation Optimization**: Multiple \"WholeStageCodegen\" boxes show where Spark compiles operations together for efficiency - operations within these boundaries execute much faster.\n",
    "\n",
    "- **Data Shuffling Points**: Each \"Exchange\" operation represents a potential bottleneck where data moves between executors.\n",
    "\n",
    "- **Operation Dependencies**: The arrows show which operations must wait for others to complete, revealing the critical path through your execution.\n",
    "\n",
    "- **Operation Numbering**: Each operation has a unique identifier (like \"[35]\", \"[41]\", etc.) that helps trace specific transformations back to your code.\n",
    "\n",
    "This visualization is invaluable for understanding complex transformations and identifying optimization opportunities. When tuning Spark applications, you'll often examine these diagrams to spot inefficient patterns like excessive shuffling or missed opportunities for pipelining operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. The Storage Tab: Managing Cached Data\n",
    "\n",
    "The Storage tab provides insight into how Spark manages cached data, which is crucial for optimizing performance by avoiding redundant computations.\n",
    "\n",
    "In the Storage tab, you can see:\n",
    "\n",
    "- Which datasets are cached\n",
    "- How much memory they're using\n",
    "- Whether they're stored in memory, on disk, or both\n",
    "- The fraction of the dataset that's cached\n",
    "\n",
    "If the \"Fraction Cached\" is less than 100%, it means some partitions couldn't fit in memory and were either spilled to disk or not cached at all, depending on your storage level. Different storage levels (like MEMORY_ONLY, MEMORY_AND_DISK, or DISK_ONLY) affect both performance and resilience.\n",
    "\n",
    "The \"Size in Memory\" vs \"Size on Disk\" comparison helps you understand serialization overhead and compression efficiency. Partitions that are well-distributed will show more even storage across executors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Jobs Tab Description](res/storage_image1.png)\n",
    "![Jobs Tab Description](res/Storage_tab.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Understanding the Storage Tab:\n",
    "\n",
    "The Storage tab displays all cached RDDs, DataFrames, and Datasets in your application, along with key metrics that help you monitor your memory usage:\n",
    "\n",
    "1. **RDD/Table Information**:\n",
    "   - **ID**: Unique identifier for each cached object\n",
    "   - **Name**: Descriptive name of the dataset (e.g., \"rdd\" or \"LocalTableScan [count#7, name#8]\")\n",
    "\n",
    "2. **Storage Characteristics**:\n",
    "   - **Storage Level**: How data is stored - shown in our example:\n",
    "     - \"Memory Serialized 1x Replicated\": Stored in memory in serialized format\n",
    "     - \"Disk Serialized 1x Replicated\": Stored on disk in serialized format\n",
    "\n",
    "3. **Partition Information**:\n",
    "   - **Cached Partitions**: Number of partitions stored (5 and 3 respectively)\n",
    "   - **Fraction Cached**: Percentage of data actually cached (both at 100%)\n",
    "\n",
    "4. **Size Metrics**:\n",
    "   - **Size in Memory**: Space used in RAM (236.0 B for the first RDD)\n",
    "   - **Size on Disk**: Space used on disk (2.1 KiB for the second RDD)\n",
    "\n",
    "### Why the Storage Tab Is Important:\n",
    "\n",
    "This tab helps you:\n",
    "\n",
    "- **Verify Caching Strategy**: Confirm that your `.cache()` or `.persist()` operations worked as expected\n",
    "- **Monitor Memory Usage**: Ensure you're not caching too much data and risking out-of-memory errors\n",
    "- **Diagnose Performance Issues**: If a query is slow despite caching, check if data was actually cached\n",
    "- **Optimize Storage Levels**: Make informed decisions about which storage level to use (memory-only, disk-only, or combined)\n",
    "\n",
    "For larger datasets, you'll also see when data partially spills to disk or when some partitions couldn't be cached due to memory constraints.\n",
    "\n",
    "You can click on any RDD name to see a more detailed view showing exactly how the data is distributed across executors and individual partition sizes.\n",
    "\n",
    "For large-scale applications, proper caching can dramatically improve performance by reusing data across operations rather than recomputing it each time. The Storage tab gives you visibility into this crucial optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5. The SQL Tab: Analyzing Query Performance\n",
    "\n",
    "The SQL tab in Spark UI provides visibility into all Spark SQL operations in your application, including DataFrame operations which are translated into SQL behind the scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Jobs Tab Description](res/sql_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Understanding the SQL Tab:\n",
    "\n",
    "The SQL tab shows a list of all queries executed in your Spark application, with crucial information about each:\n",
    "\n",
    "1. **Query Identification**:\n",
    "   - **ID**: Each query receives a unique identifier (0, 1, 2 in our example)\n",
    "   - **Description**: Shows the operation executed (e.g., \"count at \\<console\\>:26\", \"createGlobalTempView at \\<console\\>:26\")\n",
    "\n",
    "2. **Execution Timeline**:\n",
    "   - **Submitted**: When the query was sent for execution\n",
    "   - **Duration**: How long each query took to run (ranging from 0.3s to 2s in our example)\n",
    "\n",
    "3. **Related Jobs**:\n",
    "   - **Job IDs**: Links to the Spark jobs created to execute this query\n",
    "   - Note how query ID 2 generated multiple jobs ([1][2][3][4][5]), indicating a more complex execution\n",
    "\n",
    "4. **Details Link**:\n",
    "   - The \"+details\" link allows you to dive deeper into each query\n",
    "   - Clicking it reveals the logical and physical plans for the query\n",
    "\n",
    "### What This Tells Us About Query Execution:\n",
    "\n",
    "Looking at this overview, we can observe:\n",
    "\n",
    "- The \"count\" operation (ID 0) took 2 seconds and generated one job\n",
    "- The \"createGlobalTempView\" operation (ID 1) was relatively quick at 0.3 seconds\n",
    "- The \"show\" operation (ID 2) was more complex, generating 5 different jobs and taking 2 seconds\n",
    "\n",
    "### Why the SQL Tab Is Valuable:\n",
    "\n",
    "The SQL tab helps you:\n",
    "\n",
    "- **Track SQL Performance**: Identify which queries are taking the longest time\n",
    "- **Connect Operations**: Link high-level DataFrame operations to the underlying Spark jobs\n",
    "- **Troubleshoot Issues**: When a query performs poorly, you can examine its execution details\n",
    "- **Optimize Queries**: By understanding the execution plan, you can modify your code for better performance\n",
    "\n",
    "When you click the \"+details\" link for a query, you'll see the logical and physical plans, which reveal how Spark interprets and optimizes your query. This deeper view shows operations like joins, filters, and projections, along with important optimization decisions made by the Spark SQL engine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6. The Environment Tab: Configuration Details\n",
    "\n",
    "The Environment tab provides a comprehensive view of your Spark application's configuration settings, helping you understand exactly how Spark is set up to run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Jobs Tab Description](res/Environment_tab.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Key Components of the Environment Tab:\n",
    "\n",
    "The Environment tab is divided into several sections, with the most important being Runtime Information and Spark Properties:\n",
    "\n",
    "1. **Runtime Information**:\n",
    "   - **Java/Scala Versions**: In our example, we see Java 1.8.0_221 and Scala 2.12.8\n",
    "   - **Java Home**: Shows where the JVM is installed\n",
    "   - This information helps troubleshoot compatibility issues\n",
    "\n",
    "2. **Spark Properties**:\n",
    "   - **Application Configuration**: Settings like `spark.app.id` and `spark.app.name`\n",
    "   - **Execution Settings**: Properties that control how Spark runs your jobs\n",
    "   - **Critical Performance Parameters**:\n",
    "     - `spark.scheduler.mode`: \"FIFO\" in our example\n",
    "     - `spark.sql.catalogImplementation`: \"in-memory\"\n",
    "     - `spark.master`: \"local[*]\" indicating local mode execution\n",
    "\n",
    "### Important Settings to Monitor:\n",
    "\n",
    "Several key properties in this tab directly impact application performance:\n",
    "\n",
    "- **Resource Allocation**: While not explicitly configured in our example, you'd typically see memory settings like `spark.executor.memory` here\n",
    "- **Execution Control**: The `spark.scheduler.mode` determines how concurrent jobs are handled\n",
    "- **SQL Optimization**: `spark.sql.catalogImplementation` shows how Spark manages SQL metadata\n",
    "\n",
    "### Why This Information Matters:\n",
    "\n",
    "The Environment tab serves several crucial purposes:\n",
    "\n",
    "- **Troubleshooting**: When something isn't working, this is often the first place to check\n",
    "- **Verification**: Confirm your configuration settings were properly applied\n",
    "- **Documentation**: Provides a complete record of your application's environment\n",
    "- **Optimization**: Identify opportunities to tune settings for better performance\n",
    "\n",
    "For developers and administrators, the Environment tab is invaluable for understanding the exact conditions under which your Spark application is running. Many performance issues can be traced back to suboptimal configuration settings visible in this tab.\n",
    "\n",
    "In production environments, you'll often compare configurations across different applications to standardize settings or identify differences that might explain performance variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 7. The Executors Tab: Resource Utilization\n",
    "\n",
    "The Executors tab provides insight into the worker processes that execute your Spark jobs, showing how computational resources are allocated and utilized across your cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Jobs Tab Description](res/exector_tab.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Understanding the Executors Tab:\n",
    "\n",
    "The Executors tab is divided into two main sections:\n",
    "\n",
    "1. **Summary Statistics**:\n",
    "   - **Active/Total/Dead Executors**: In our example, there are 3 active executors and 0 dead ones\n",
    "   - **Resource Allocation**: The cluster has 2 cores active and 5.9 KiB / 1.1 GiB of storage memory in use\n",
    "   - **Task Metrics**: 5 completed tasks with a total task time of 4 seconds (including 0.2 seconds of garbage collection)\n",
    "\n",
    "2. **Individual Executor Details**:\n",
    "   - **Executor ID**: Each executor has a unique identifier (0, 1, and \"driver\")\n",
    "   - **Status**: All executors show as \"Active\" in our example\n",
    "   - **Resource Allocation**: Each executor has 1 core and 2 KiB / 366.3 MiB of storage memory\n",
    "   - **Task Distribution**: Executor 1 has completed 3 tasks, Executor 0 has completed 2 tasks\n",
    "   - **Performance Metrics**: Task time and GC time help identify processing bottlenecks\n",
    "   - **Logs**: Links to stdout/stderr logs and thread dumps for debugging\n",
    "\n",
    "### Key Metrics to Monitor:\n",
    "\n",
    "Several important indicators of application health are visible here:\n",
    "\n",
    "- **Task Distribution**: Ideally, tasks should be evenly distributed across executors. In our example, tasks are reasonably balanced between Executors 0 and 1.\n",
    "- **Memory Usage**: Storage memory shows how much data is cached on each executor\n",
    "- **GC Time**: Garbage collection time as a proportion of total task time (0.2s out of 4s here) helps identify memory pressure\n",
    "- **Shuffle Read/Write**: In our example, there's no shuffle activity (all 0.0 B), but in data-intensive applications, high shuffle volumes can indicate potential performance issues\n",
    "\n",
    "### Why the Executors Tab Is Important:\n",
    "\n",
    "This tab helps you:\n",
    "\n",
    "- **Monitor Resource Utilization**: Ensure executors have appropriate memory and CPU allocation\n",
    "- **Identify Skew**: Detect when certain executors are processing significantly more data or tasks than others\n",
    "- **Troubleshoot Failures**: Quickly access logs when executors fail or tasks encounter errors\n",
    "- **Track Performance**: Monitor GC time and task execution metrics to optimize resource allocation\n",
    "\n",
    "For production Spark applications, regularly checking the Executors tab helps ensure your cluster is properly sized and resources are efficiently utilized across all worker nodes.\n",
    "\n",
    "In larger deployments, you might see dozens or hundreds of executors, making this view essential for identifying outliers or problematic nodes that could impact overall application performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## Putting It All Together: Analyzing a Complete Workflow\n",
    "\n",
    "Let's create a more comprehensive example to demonstrate how all aspects of the Spark UI work together:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T19:09:51.234126700Z",
     "start_time": "2025-05-13T19:09:37.234132Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 23:15:27 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+-----------------+-----------------+--------------+\n",
      "|       name|age|        avg_grade|   avg_attendance|subjects_taken|\n",
      "+-----------+---+-----------------+-----------------+--------------+\n",
      "|Student_890| 16|            100.0|             99.0|             1|\n",
      "|Student_127| 15|             97.0|92.83333333333333|             1|\n",
      "|Student_626| 17|             96.0|             98.0|             1|\n",
      "|Student_364| 18|             95.2|             86.0|             2|\n",
      "|Student_149| 18|             95.0|             97.0|             1|\n",
      "|Student_419| 16|            94.25|             89.0|             2|\n",
      "|Student_159| 15|             93.0|90.71428571428571|             2|\n",
      "|Student_764| 18|             93.0|            86.75|             2|\n",
      "|Student_927| 14|92.92307692307692|89.76923076923077|             3|\n",
      "|Student_125| 18|92.85714285714286|85.42857142857143|             2|\n",
      "|Student_398| 17|92.66666666666667|89.33333333333333|             2|\n",
      "| Student_80| 14|             92.3|             86.4|             2|\n",
      "|Student_707| 17|             92.2|             87.8|             2|\n",
      "|Student_222| 17|            91.75|             86.5|             3|\n",
      "|Student_714| 16|             91.7|             86.6|             3|\n",
      "|Student_399| 17|91.66666666666667|             90.5|             2|\n",
      "|Student_157| 17|             91.5|             97.5|             1|\n",
      "|Student_641| 18|             91.5|             86.0|             1|\n",
      "|Student_571| 16|             91.0|             96.0|             1|\n",
      "|Student_838| 17|90.83333333333333|86.66666666666667|             2|\n",
      "+-----------+---+-----------------+-----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "\n",
    "# Initialize a Spark session (if not already created)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkUIComprehensiveExample\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create example datasets\n",
    "students = spark.createDataFrame([\n",
    "    (i, f\"Student_{i}\", random.randint(14, 18))\n",
    "    for i in range(1000)\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "grades = spark.createDataFrame([\n",
    "    (random.randint(1, 1000),\n",
    "     random.choice(['Math', 'Science', 'History']),\n",
    "     random.randint(60, 100))\n",
    "    for _ in range(5000)\n",
    "], [\"student_id\", \"subject\", \"grade\"])\n",
    "\n",
    "attendance = spark.createDataFrame([\n",
    "    (random.randint(1, 1000),\n",
    "     random.choice(['Math', 'Science', 'History']),\n",
    "     random.randint(70, 100))\n",
    "    for _ in range(7000)\n",
    "], [\"student_id\", \"subject\", \"attendance_pct\"])\n",
    "\n",
    "# Cache one of our tables\n",
    "grades.cache()\n",
    "grades.count()  # Materialize the cache\n",
    "\n",
    "# Make them available for SQL\n",
    "students.createOrReplaceTempView(\"students\")\n",
    "grades.createOrReplaceTempView(\"grades\")\n",
    "attendance.createOrReplaceTempView(\"attendance\")\n",
    "\n",
    "# Run a complex query\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        s.name,\n",
    "        s.age,\n",
    "        AVG(g.grade) as avg_grade,\n",
    "        AVG(a.attendance_pct) as avg_attendance,\n",
    "        COUNT(DISTINCT g.subject) as subjects_taken\n",
    "    FROM students s\n",
    "    JOIN grades g ON s.id = g.student_id\n",
    "    JOIN attendance a ON s.id = a.student_id AND g.subject = a.subject\n",
    "    GROUP BY s.name, s.age\n",
    "    HAVING AVG(g.grade) > 80 AND AVG(a.attendance_pct) > 85\n",
    "    ORDER BY avg_grade DESC, avg_attendance DESC\n",
    "\"\"\")\n",
    "\n",
    "# Show the results\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After running this example, explore:\n",
    "1. The Jobs tab to see all created jobs\n",
    "2. The Stages tab to analyze execution stages\n",
    "3. The SQL tab to view the query plans\n",
    "4. The Storage tab to see cached data\n",
    "5. The Executors tab to examine resource usage\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
