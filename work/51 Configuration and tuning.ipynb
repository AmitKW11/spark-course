{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9760a010-d36c-4717-9e7e-1074d89f691a",
   "metadata": {},
   "source": [
    "# Configuration and tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169896f9-5f61-44e3-911c-7ba967745d00",
   "metadata": {},
   "source": [
    "For help on optimizing your programs, the configuration and tuning guides provide information on best practices. They are especially important for making sure that your data is stored in memory in an efficient format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6731463f-dc7b-4966-b275-52ee22f6f9a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tuning\n",
    "\n",
    "STONGLY ADVISED to read https://spark.apache.org/docs/latest/tuning.html\n",
    "\n",
    "[SDG] chapter 19 \"Performance Tuning\"\n",
    "\n",
    "A nice resource calculator at http://spark-configuration.luminousmen.com/\n",
    "\n",
    "## Data Serialization\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*_3-owXxQnlBBvjBwaKD10g.png\" >\n",
    "\n",
    "[https://teepika-r-m.medium.com/serialization-in-apache-spark-cdbb49099a8e]\n",
    "\n",
    "Serialization plays an important role in the performance of any distributed application. Formats that are slow to serialize objects into, or consume a large number of bytes, will greatly slow down the computation. Often, this will be the first thing you should tune to optimize a Spark application.\n",
    "\n",
    "Spark provides two serialization libraries: Java serialization and Kryo.\n",
    "\n",
    "The Kryo can be faster by 10x but is not the default.\n",
    "\n",
    "You can switch to using Kryo by initializing your job with a SparkConf and calling `conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")`. This setting configures the serializer used for not only shuffling data between worker nodes but also when serializing RDDs to disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e13831-71cf-421b-8ba2-04bad480fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO implement a test case that shows the performance diff between default and kryo serializer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0527bf34-1ba6-444f-9351-18a3f14bdfff",
   "metadata": {},
   "source": [
    "## Memory Tuning\n",
    "https://spark.apache.org/docs/latest/tuning.html#memory-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c875e7-26ce-4d15-94ad-d78338651e19",
   "metadata": {},
   "source": [
    "### Determining Memory Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d50645-4468-4787-a8b3-37f814ffdf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.random import RandomRDDs\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName('tuning')\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"512m\")\\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921241b8-043e-4535-88b9-da5ca20ef5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY when running in jupyter:\n",
    "#spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "\n",
    "u = RandomRDDs.normalRDD(spark, 10000000, 8)\n",
    "u = u.map(lambda x: (x,)) # convert to tuple so we can transorm into DF\n",
    "u.cache()\n",
    "schema = StructType([  StructField('c1', DoubleType(), True)])\n",
    "# we can move from RDD to Dataframe and back. \n",
    "df = spark.createDataFrame(u, schema)\n",
    "\n",
    "#we must do something with u before we can see the storage used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c98441e-ab1f-4943-831a-bebbed69a4db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.agg({'c1': 'sum'}).show()\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f162f4-3619-4db6-93b6-b91a4c2413fe",
   "metadata": {},
   "source": [
    "How many bytes are needed to store 10M double precision? . In C it will take 10M * 8 Bytes = 80MB.\n",
    "\n",
    "Open the UI http://localhost:4040 and and look at the “Storage” page in the web UI. \n",
    "In my setup it show 92MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b804c3-5262-4b17-bf5b-fb83bec1498d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's create another RDD to consume more memory and see what's happen.\n",
    "u2 = RandomRDDs.normalRDD(spark, 40000000,4)\n",
    "u2 = u2.map(lambda x: (x,)) # convert to tuple so we can transorm into DF\n",
    "u2.cache()\n",
    "schema = StructType([  StructField('c1', DoubleType(), True)])\n",
    "df2 = spark.createDataFrame(u2, schema)\n",
    "df2.agg({'c1': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c15944-034f-443e-8715-a8917fb7ed09",
   "metadata": {},
   "source": [
    "While computing the above cell, I got\n",
    "```MemoryStore: Not enough space to cache rdd_16_2 in memory! (computed 73.9 MiB so far)\n",
    "\n",
    "23/01/04 11:29:43 WARN BlockManager: Putting block rdd_16_0 failed\n",
    "23/01/04 11:29:43 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
    "java.lang.OutOfMemoryError: Java heap space\n",
    "```\n",
    "\n",
    "This is because the current limitation is 400MB on the Java heap. When filling this amount, some cached content will be flushed or discared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80db3e3-07fd-4e80-a6a8-2f9e839fcece",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df48f43-9712-4f5b-8f56-43dbd3919b98",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "The default configuration might be good for your job, but it also might cause severe bottlenecks.\n",
    "You should be able to identify the major configuration settings and update them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5cc157-6788-40e5-ac53-670c1bb31b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the current configurations of Spark.\n",
    "# This does NOT include the Cluster manager (Yarn for example)!\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac82c4d-7015-4c50-8dbb-c9992eae7efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
