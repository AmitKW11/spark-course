{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9760a010-d36c-4717-9e7e-1074d89f691a",
   "metadata": {},
   "source": [
    "# Configuration and tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169896f9-5f61-44e3-911c-7ba967745d00",
   "metadata": {},
   "source": [
    "For help on optimizing your programs, the configuration and tuning guides provide information on best practices. They are especially important for making sure that your data is stored in memory in an efficient format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6731463f-dc7b-4966-b275-52ee22f6f9a3",
   "metadata": {},
   "source": [
    "# Tuning\n",
    "\n",
    "STONGLY ADVISED to read https://spark.apache.org/docs/latest/tuning.html\n",
    "\n",
    "A nice resource calculator at http://spark-configuration.luminousmen.com/\n",
    "\n",
    "## Data Serialization\n",
    "\n",
    "\n",
    "Serialization plays an important role in the performance of any distributed application. Formats that are slow to serialize objects into, or consume a large number of bytes, will greatly slow down the computation. Often, this will be the first thing you should tune to optimize a Spark application.\n",
    "\n",
    "Spark provides two serialization libraries: Java serialization and Kryo.\n",
    "\n",
    "The Kryo can be faster by 10x but is not the default.\n",
    "\n",
    "You can switch to using Kryo by initializing your job with a SparkConf and calling `conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")`. This setting configures the serializer used for not only shuffling data between worker nodes but also when serializing RDDs to disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2e13831-71cf-421b-8ba2-04bad480fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO implement a test case that shows the performance diff between default and kryo serializer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0527bf34-1ba6-444f-9351-18a3f14bdfff",
   "metadata": {},
   "source": [
    "## Memory Tuning\n",
    "https://spark.apache.org/docs/latest/tuning.html#memory-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c875e7-26ce-4d15-94ad-d78338651e19",
   "metadata": {},
   "source": [
    "### Determining Memory Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0d50645-4468-4787-a8b3-37f814ffdf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.random import RandomRDDs\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName('tuning')\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"512m\")\\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "921241b8-043e-4535-88b9-da5ca20ef5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/04 11:29:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# ONLY when running in jupyter:\n",
    "#spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "\n",
    "u = RandomRDDs.normalRDD(spark, 10000000, 8)\n",
    "u = u.map(lambda x: (x,)) # convert to tuple so we can transorm into DF\n",
    "u.cache()\n",
    "schema = StructType([  StructField('c1', DoubleType(), True)])\n",
    "# we can move from RDD to Dataframe and back. \n",
    "df = spark.createDataFrame(u, schema)\n",
    "\n",
    "#we must do something with u before we can see the storage used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c98441e-ab1f-4943-831a-bebbed69a4db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:============================================>              (3 + 1) / 4]\r"
     ]
    }
   ],
   "source": [
    "df.agg({'c1': 'sum'}).show()\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f162f4-3619-4db6-93b6-b91a4c2413fe",
   "metadata": {},
   "source": [
    "How many bytes are needed to store 10M double precision? . In C it will take 10M * 8 Bytes = 80MB.\n",
    "\n",
    "Open the UI http://localhost:4040 and and look at the “Storage” page in the web UI. \n",
    "In my setup it show 92MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b804c3-5262-4b17-bf5b-fb83bec1498d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/04 11:29:42 WARN MemoryStore: Not enough space to cache rdd_16_0 in memory! (computed 73.9 MiB so far)\n",
      "23/01/04 11:29:43 WARN BlockManager: Block rdd_16_0 could not be removed as it was not found on disk or in memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29.539s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 262146 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/04 11:29:43 WARN BlockManager: Putting block rdd_16_0 failed\n",
      "23/01/04 11:29:43 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n",
      "\tat java.base/java.io.OutputStream.write(OutputStream.java:122)\n",
      "\tat net.razorvine.pickle.Pickler.put_float(Pickler.java:715)\n",
      "\tat net.razorvine.pickle.Pickler.dispatch(Pickler.java:294)\n",
      "\tat net.razorvine.pickle.Pickler.save(Pickler.java:193)\n",
      "\tat net.razorvine.pickle.Pickler.put_arrayOfObjects(Pickler.java:593)\n",
      "\tat net.razorvine.pickle.Pickler.dispatch(Pickler.java:262)\n",
      "\tat net.razorvine.pickle.Pickler.save(Pickler.java:193)\n",
      "\tat net.razorvine.pickle.Pickler.dump(Pickler.java:163)\n",
      "\tat net.razorvine.pickle.Pickler.dumps(Pickler.java:148)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:92)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:80)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:670)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:424)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$Lambda$2638/0x000000084115b440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:259)\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n",
      "\tat java.base/java.io.OutputStream.write(OutputStream.java:122)\n",
      "\tat net.razorvine.pickle.Pickler.put_float(Pickler.java:715)\n",
      "\tat net.razorvine.pickle.Pickler.dispatch(Pickler.java:294)\n",
      "\tat net.razorvine.pickle.Pickler.save(Pickler.java:193)\n",
      "\tat net.razorvine.pickle.Pickler.put_arrayOfObjects(Pickler.java:593)\n",
      "\tat net.razorvine.pickle.Pickler.dispatch(Pickler.java:262)\n",
      "\tat net.razorvine.pickle.Pickler.save(Pickler.java:193)\n",
      "\tat net.razorvine.pickle.Pickler.dump(Pickler.java:163)\n",
      "\tat net.razorvine.pickle.Pickler.dumps(Pickler.java:148)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:92)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:80)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:670)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:424)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$Lambda$2638/0x000000084115b440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:259)\n",
      "[Stage 4:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2958/3206890007.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m  \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'c1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoubleType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'c1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# let's create another RDD to consume more memory and see what's happen.\n",
    "u2 = RandomRDDs.normalRDD(spark, 40000000,4)\n",
    "u2 = u2.map(lambda x: (x,)) # convert to tuple so we can transorm into DF\n",
    "u2.cache()\n",
    "schema = StructType([  StructField('c1', DoubleType(), True)])\n",
    "df2 = spark.createDataFrame(u2, schema)\n",
    "df2.agg({'c1': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c15944-034f-443e-8715-a8917fb7ed09",
   "metadata": {},
   "source": [
    "While computing the above cell, I got\n",
    "```MemoryStore: Not enough space to cache rdd_16_2 in memory! (computed 73.9 MiB so far)\n",
    "\n",
    "23/01/04 11:29:43 WARN BlockManager: Putting block rdd_16_0 failed\n",
    "23/01/04 11:29:43 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
    "java.lang.OutOfMemoryError: Java heap space\n",
    "```\n",
    "\n",
    "This is because the current limitation is 400MB on the Java heap. When filling this amount, some cached content will be flushed or discared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80db3e3-07fd-4e80-a6a8-2f9e839fcece",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df48f43-9712-4f5b-8f56-43dbd3919b98",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "The default configuration might be good for your job, but it also might cause severe bottlenecks.\n",
    "You should be able to identify the major configuration settings and update them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f5cc157-6788-40e5-ac53-670c1bb31b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.kryoserializer.buffer.max', '512m'),\n",
       " ('spark.app.id', 'local-1673113119910'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'tuning'),\n",
       " ('spark.app.startTime', '1673113118915'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Dio.netty.tryReflectionSetAccessible=true'),\n",
       " ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'),\n",
       " ('spark.driver.port', '42699'),\n",
       " ('spark.driver.host', '489ad7d3834c'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.sql.warehouse.dir', 'file:/home/jovyan/work/spark-warehouse'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Dio.netty.tryReflectionSetAccessible=true'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all the current configurations of Spark.\n",
    "# This does NOT include the Cluster manager (Yarn for example)!\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ac82c4d-7015-4c50-8dbb-c9992eae7efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://489ad7d3834c:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>tuning</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f078468f160>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
