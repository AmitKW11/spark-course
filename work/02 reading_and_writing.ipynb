{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "241b3477-2842-41f0-a6e0-7b22087f19cf",
   "metadata": {},
   "source": [
    "# Reading from CSV / json file and writing to Parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7532a67b-b82a-4e1b-af78-7ac75163311c",
   "metadata": {},
   "source": [
    "This sample code reads a few fields from nested json, and created a dataframe.\n",
    "\n",
    "Then write the dataframe to storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d6d239-fb7e-4ff8-989e-e45a85bb1558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfff3a29-e132-4782-b605-a71b585583a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/02/15 09:03:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('02 reading').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efeb9bf-777a-40c5-826a-07ee8790f69b",
   "metadata": {},
   "source": [
    "The Spark UI is available at http://localhost:4040 when running locally in a PC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba80e1e2-0ffd-48e6-b4d4-18cd4fc35418",
   "metadata": {},
   "source": [
    "# Read nested json into a dataframe\n",
    "\n",
    "HINT: During testing, create a tiny jsonl file so reading is fast. I did `head -n 12 the-file.json > test_12.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b9311-0317-4589-9bac-01a0cef1bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/\n",
    "# https://bigdataprogrammers.com/read-nested-json-in-spark-dataframe/\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-explode-array-and-map-columns-to-rows/\n",
    "from pyspark.sql.types import MapType\n",
    "fname = \"../data/sample.json\"\n",
    "\n",
    "# Note: By default, the schema is inferred from the data.\n",
    "# This is slower and may sometime fail due to bad input files.\n",
    "# A possilbe workaround is to read a short well defined file, extract the schema from it, and then read\n",
    "# the full file using this schema.\n",
    "# inferred = spark.read.json(fname_ref)\n",
    "# inferred.printSchema()\n",
    "#bids = spark.read.schema(inferred.schema).json(fname)\n",
    "df = spark.read.json(fname)\n",
    "df.show()   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74e41e6-b261-46ea-912e-e0d3053d6e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can get a few columns of our choice. note the nesting\n",
    "subset = df.select('address.zip', 'name') \n",
    "subset.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c61960-318e-44f2-ab97-9a110ae0b301",
   "metadata": {},
   "source": [
    "If the notebooks runs inside a Docker container, we need to provide access to the hosted data directory.\n",
    "\n",
    "For example, create a directory in the host and configure in docker-compose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f66a26-8ecf-4f8c-9319-2c02e7e3d402",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reading multiple files at a time\n",
    "https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/\n",
    "\n",
    "Using the read.json() method you can also read multiple JSON files from different paths, just pass all file names with fully qualified paths by separating comma, or a list of files"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b437390e-b674-4a15-935b-0b1fb713368d",
   "metadata": {},
   "source": [
    "prefix=\"wasbs://nc001@dacoursedatastorage.blob.core.windows.net/\"\n",
    "filelist = [ prefix + 'requestLog_D_919539.log.tar_' + str(i) + \".json\" for i in range(6)]\n",
    "test_df = spark.read.schema(inferred.schema).json(filelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d548e70-157a-4c5c-8209-49586a2b9125",
   "metadata": {},
   "source": [
    "# Writing the dataframe to storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88df48c-c2f7-4933-a1b6-76e64431f050",
   "metadata": {},
   "source": [
    "What if you want to persist (save values) of a DF?\n",
    "Can be saved to a database (covered in another lesson), or saved to a file in the file system.\n",
    "Using Parquet format is very efficient as we can see here.\n",
    "\n",
    "\n",
    "For example, In one test read a jsonl file (602MB) into a DF, then wrote it to parquet file (actually it creates a directory with several files).\n",
    "The parquet file is compressed so the total saved storage is 92MB. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6263a89-8999-4fe4-8197-e295fbb1935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Read a CSV into a dataframe, inferring the schema.\n",
    "dataPath = \"../data/Open_Parking_and_Camera_Violations_100.csv\"\n",
    "fines = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\",\"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(dataPath)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f573c-a0e0-4121-af1f-0ce6c0cd30e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# the output file must not exist\n",
    "# Column names must not include spaces (and some other characters\n",
    "fines.select(['Plate','Amount Due']).withColumnRenamed('Amount Due','AmountDue').write.parquet(\"./testing_noam.parquet\")\n",
    "# On my Docker/PC, saving 250K lines DF, took about 9 seconds (CPU times: user 16 Âµs, sys: 3.98 ms, total: 4 ms, Wall time: 8.6 s)\n",
    "#  and reading them took 2.5 Sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a8f15-d481-42e5-a287-374ca0612591",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# read the DF from the parquet file:\n",
    "restored_df = spark.read.parquet(\"./testing_noam.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea20dd7-6d56-4ca4-89db-91ed9432baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b754b78-72ed-448c-86e0-3e58c44eebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "restored_df.select(f.max(\"Plate\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0b1c38-8cb7-4820-9132-959f8323202f",
   "metadata": {},
   "source": [
    "### Repartition before writing to storage\n",
    "\n",
    "Spark DataFrameWriter provides partitionBy method which can be used to partition data on write. It repartition the data into separate files on write using a provided set of columns. [2]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "67c28025-c44b-40fb-b1b3-18ce19daed36",
   "metadata": {},
   "source": [
    "df.write.partitionBy('key').json('/path/to/foo.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
