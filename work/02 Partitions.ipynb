{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6588ab2b-eb4a-4e8c-a1f3-1f9516599f00",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Understanding partitions\n",
    "\n",
    "[**Watch the video**](https://panoptotech.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=1860120d-7b89-47ac-958f-afa80149771a)\n",
    "\n",
    "To allow every executor to perform work in parallel, Spark breaks up the data into chunks called\n",
    "partitions. A partition is a collection of rows that sit on one physical machine in your cluster. A\n",
    "DataFrame’s partitions represent how the data is physically distributed across the cluster of\n",
    "machines during execution. If you have one partition, Spark will have a parallelism of only one,\n",
    "even if you have thousands of executors. If you have many partitions but only one executor,\n",
    "Spark will still have a parallelism of only one because there is only one computation resource. [SDG]\n",
    "\n",
    "\n",
    "Clusters will not be fully utilized unless you set the level of parallelism for each operation high enough. The general recommendation for Spark is to have 4x of partitions to the number of cores in cluster available for application, and for upper bound — the task should take 100ms+ time to execute. If it is taking less time than your partitioned data is too small and your application might be spending more time in distributing the tasks. [2]\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1e73b2-c80a-4869-8ed0-b4a2600e49d4",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0105927-5806-4b19-9288-e012dea44ded",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_1m=spark.range(1*1000*1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca0ae95-2ab9-4c14-9850-dfbd0a403962",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# In standalone mode, numPartitions == num cores in this machine.\n",
    "df_1m.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349f81be-a4fe-4123-b73f-551341fd8e76",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_4p=df_1m.repartition(4)\n",
    "print(f\"this df has {df_4p.rdd.getNumPartitions()} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7915ba96-be6f-4936-9732-3df544513c00",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Does this statemnt has an effect of DF partition? \n",
    "# NO. The command changes from the default SHUFFLE value of 200 to 5,\n",
    "# but that value is used only during shuffling. \n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "\n",
    "spark.range(1*1000*1000).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ddece4-1f50-486f-b251-39b96ac5a83a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Partitioning types\n",
    "\n",
    "### Hash Partitioning\n",
    "Uses Java’s Object.hashCode method to determine the partition as partition = key.hashCode() % numPartitions.\n",
    "\n",
    "### Range Partitioning\n",
    "Uses a range to distribute to the respective partitions the keys that fall within a range. This method is suitable where there’s a natural ordering in the keys and the keys are non negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d95e617-3e18-47ce-a724-3b74d3971c98",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Repartition and Coalesce\n",
    "\n",
    "Another important optimization opportunity is to partition the data according to some frequently\n",
    "filtered columns, which control the physical layout of data across the cluster including the\n",
    "partitioning scheme and the number of partitions.\n",
    "\n",
    "Repartition will **incur a full shuffle of the data**, regardless of whether one is necessary. This\n",
    "means that you should typically only repartition when the future number of partitions is greater\n",
    "than your current number of partitions or when you are looking to partition by a set of columns\n",
    "\n",
    "Another opportunity to repartition is when writing data to disk: Each partition will be written to a separate folder, making reading the data faster. See https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrameWriter.partitionBy.html?highlight=partitionby#pyspark.sql.DataFrameWriter.partitionBy \n",
    "\n",
    "### Coalesce\n",
    "\n",
    "The coalesce reduces the number of partitions in a DataFrame. Coalesce avoids complete shuffle; instead of creating new partitions, it shuffles the data using Hash Partitioner (Default) and adjusts into existing partitions. Which means it can only decrease the number of partitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74218141-10a7-4389-9cdf-3240e46ea9cd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We can repartition a DF:\n",
    "df_10p = df_4p.repartition(10)\n",
    "df_10p.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcd378d-99f3-4eaa-8d91-2d563c8965ba",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Unbalanced partition sizes\n",
    "\n",
    "If a partition is too large and other partitions are small, the processing will not only be slower  but we can also get Out Of Memory error from the executor that processes the large partition.\n",
    "\n",
    "Read [Add SALT to Compensate](https://towardsdatascience.com/skewed-data-in-spark-add-salt-to-compensate-16d44404088b)\n",
    " for a discussion on one way to reduce this problem.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43175f7-ed57-4c0a-b814-169b40c57d17",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# This example *should* demonstrate SQEWED partitions: when the size of partitions is unbalanced.\n",
    "\n",
    "# source: https://luminousmen.com/post/spark-tips-partition-tuning\n",
    "# however, when I ran this code, the 'transactions' has 1 partition after repatition().\n",
    "# the 'df' has 1 partition.\n",
    "# Before calling repartition() it has 8.\n",
    "# Tested on Spark 3.2.0 standalone.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# set smaller number of partitions so they can fit the screen\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 3)\n",
    "# disable broadcast join to see the shuffle\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "length = 1000000\n",
    "names = np.random.choice(['Bob', 'James', 'Marek', 'Johannes', None], length)\n",
    "amounts = np.random.randint(0, 1000000, length)\n",
    "\n",
    "# generate skewed data\n",
    "country = np.random.choice(\n",
    "    ['United Kingdom', 'Poland', 'USA', 'Germany', 'Russia'],\n",
    "    length,\n",
    "    p = [0.05, 0.05, 0.8, 0.05, 0.05]\n",
    ")\n",
    "data = pd.DataFrame({'name': names, 'amount': amounts, 'country': country})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a25d28-f8c6-4857-aa12-e9dbb569ae9c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "transactions = spark.createDataFrame(data).repartition('country') # <<<<<< here we repartition!\n",
    "\n",
    "print(f\"transactions has {transactions.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "countries = spark.createDataFrame(pd.DataFrame({\n",
    "    'id': [11, 12, 13, 14, 15],\n",
    "    'country': ['United Kingdom', 'Poland', 'USA', 'Germany', 'Russia']\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc57555-8d28-4a50-a364-080ec923fda6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = transactions.join(countries, 'country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4543780-644a-44b8-931f-a0819c14f844",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"df has {df.rdd.getNumPartitions()} partitions\")\n",
    "# check the partitions data\n",
    "# I use glom() ONLY for this demonstration. You don't really want get 'per partition' data into python!\n",
    "for i, part in enumerate(df.rdd.glom().collect()):\n",
    "    print({i: part[0:50]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1532b8a3-c2a4-48f7-811d-b9c2c13dafe6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Check yourself\n",
    "* what happens if there are less partitions than executors?\n",
    "* what happens if there are more partitions than executors?\n",
    "* Who is responsible to partition the data?\n",
    "* what is the criterion to decide if a row is in a certain partition?\n",
    "* can an RDD be partitioned by columns only (vertical partitioning)?\n",
    "* Is partitioning automatic? Can I influence it?\n",
    "\n",
    "Answer [here](https://forms.gle/9eu69HhgWRCE8CCMA) and see your results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}