{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44ab91d4-60f9-4b79-b338-e0c2b363aef4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using the Machine Learning library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32141e84-fff5-44b4-9b6e-7885a8f23af2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Spark provides a library with ML functionality. The set of tools is ever expanding -- see the latest at https://spark.apache.org/docs/latest/ml-guide.html\n",
    "\n",
    "The library is implemented in Scala, and has python binding (i.e. calling from python to the API).\n",
    "\n",
    "\n",
    "Using MLFlow ( https://mlflow.org/docs/latest/python_api/mlflow.spark.html?highlight=spark#module-mlflow.spark )is also possible, but not covered here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29449b8e-51c2-4d09-920b-4e5e54eb6efd",
   "metadata": {},
   "source": [
    "**Check the notebook at \"SDG/Advanced_Analytics_and_Machine_Learning-Chapter_25_Preprocessing_and_Feature_Engineering\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0239539-8e23-4526-a01a-6c740ce015a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "If a specific tool is not part of MLlib, maybe someone already implemented it.\n",
    "\n",
    "Always be suspicious of the source: who wrote it? when was the last update? how many stars?\n",
    "\n",
    "See for example https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22 which is a repo without any quality assurance. You can find a great code, a buggy code, or malware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28feace4-9910-4b0c-b691-79daf4a55834",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/02/23 15:05:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession.builder.appName('MLlib').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "181ac6fc-20ff-4f4d-9b04-0b7a47fb6cec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading ../data/sdg/retail-data/by-day/2011-_.parquet from cache Parquet file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.count = 499428\n"
     ]
    }
   ],
   "source": [
    "def load_data(file_name_glob):\n",
    "    \"\"\" load the contents of the input files.\n",
    "        If we already saved them in Parquet file, use it.\n",
    "        >>> load_data('../data/sdg/retail-data/by-day/2010-12*.csv')\n",
    "        :param file_name_glob wildcard value of the files to read. e.g. \"/mnt/dir/data*\"\n",
    "        :return: DataFrame containing all the data\n",
    "    \"\"\"\n",
    "    \n",
    "    def cache_file_name(file_name):\n",
    "        t = file_name.replace('*',\"_\").replace('?',\"_\")\n",
    "        return t[: t.rfind('.')] + \".parquet\"\n",
    "    \n",
    "    import os\n",
    "    dirname = os.path.dirname(file_name_glob)\n",
    "    p = Path(dirname)\n",
    "    fname = Path(file_name_glob)\n",
    "    basename = fname.name\n",
    "    cache_name = cache_file_name(file_name_glob)\n",
    "    if Path(cache_name).exists():\n",
    "        print(f\"reading {cache_name} from cache Parquet file\")\n",
    "        return spark.read.parquet(cache_name)\n",
    "    \n",
    "    #suffix = fname.suffix\n",
    "    if not p.exists():\n",
    "        raise ValueError('Path not found')\n",
    "    file_list = list(p.glob(basename))\n",
    "    x = [ str(f.resolve()) for f in file_list]\n",
    "    df = spark.read \\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(x)\n",
    "    \n",
    "    df.write.parquet(cache_name)\n",
    "    return df\n",
    "    \n",
    "\n",
    "df = load_data('../data/sdg/retail-data/by-day/2011-*.csv')\n",
    "print(f\"df.count = {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2ff27a7-179e-460a-ad4d-6b24f015db0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fa988b-f672-4935-8d66-1dd8cbf99ece",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using Mlib \n",
    "\n",
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f887b51-4422-4660-80fc-650fb8c69b0b",
   "metadata": {},
   "source": [
    "Add a new column: \"day of week\" and split to train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e8f7ec9-c948-43d0-99dc-46b693e60f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:===============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:test ratio: 2.341415438962707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "preppedDataFrame = df\\\n",
    "  .na.fill(0)\\\n",
    "  .withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\"))\n",
    "  #.coalesce(5)\n",
    "\n",
    "# split to train and test:\n",
    "trainDataFrame,testDataFrame  = preppedDataFrame.randomSplit([0.7, 0.3])\n",
    "\n",
    "# we could also split using other criteria:\n",
    "# trainDataFrame = preppedDataFrame\\\n",
    "#   .where(\"InvoiceDate < '2011-07-01'\")\n",
    "# testDataFrame = preppedDataFrame\\\n",
    "#   .where(\"InvoiceDate >= '2011-07-01'\")\n",
    "\n",
    "print(f\"train:test ratio: {trainDataFrame.count()/testDataFrame.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888e3741-a059-49dc-a3af-80be1788457c",
   "metadata": {},
   "source": [
    "convert day of week \"Mon\" -> 2 -> one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19ab6915-328a-47db-b3d5-f2dfa73f5b4d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "day_indexer = StringIndexer()\\\n",
    "  .setInputCol(\"day_of_week\")\\\n",
    "  .setOutputCol(\"day_of_week_index\")\n",
    "\n",
    "country_indexer = StringIndexer()\\\n",
    "  .setInputCol(\"Country\")\\\n",
    "  .setOutputCol(\"country_index\")\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "encoder = OneHotEncoder()\\\n",
    "  .setInputCol(\"day_of_week_index\")\\\n",
    "  .setOutputCol(\"day_of_week_encoded\")\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#  add \"features\" column that contains the input columns as elements in a vector.\n",
    "# Not very exciting, right?\n",
    "vectorAssembler = VectorAssembler()\\\n",
    "  .setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "# Read about pipelines here: https://spark.apache.org/docs/latest/ml-pipeline.html\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transformationPipeline = Pipeline()\\\n",
    "  .setStages([day_indexer, country_indexer, encoder, vectorAssembler])\n",
    "\n",
    "fittedPipeline = transformationPipeline.fit(trainDataFrame)\n",
    "transformedTraining = fittedPipeline.transform(trainDataFrame)\n",
    "tranformedTest = fittedPipeline.transform(testDataFrame)\n",
    "\n",
    "# Let's drop unused columns. \n",
    "# This reduces the amount of needed memory so improving performance.\n",
    "transformedTraining = transformedTraining.drop('day_of_week').drop('day_of_week_encoded').drop('day_of_week_index'). drop('CustomerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e02ac748-737d-4360-8c4e-db3c285679e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, Country: string, country_index: double, features: vector]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caching the transfored DF will save a lot of time when reusing it (e.g. for hyper param tuning)\n",
    "transformedTraining.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d53770-3a86-4532-b450-c6f1b5fe7e5b",
   "metadata": {},
   "source": [
    "In Spark, training machine learning models is a two phase process. First we initialize an untrained model, then we\n",
    "train it. There are always two types for every algorithm in MLlib’s DataFrame API. The algorithm Kmeans and then the\n",
    "trained version which is a KMeansModel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6f1c5f-16cc-40eb-a9e8-ca701c88b7a2",
   "metadata": {},
   "source": [
    "## K means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efd35788-5d6a-4a5d-ace7-988c8f2e68e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans = KMeans()\\\n",
    "  .setK(6)\\\n",
    "  .setSeed(1)\n",
    "\n",
    "kmModel = kmeans.fit(transformedTraining)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "638788da-ef32-4d3e-9d45-d93e52224dbf",
   "metadata": {},
   "source": [
    "TODO:continue here.\n",
    "consult the book, and only then look at the student's submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da489d-56dd-4354-92b3-c520eb26dc28",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Supervised learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a084bc-e420-45c0-9e7d-0bc57ee19072",
   "metadata": {},
   "source": [
    "## Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04038c37-c07e-43a4-8489-fec9e0fe45ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InvoiceNo',\n",
       " 'StockCode',\n",
       " 'Description',\n",
       " 'Quantity',\n",
       " 'InvoiceDate',\n",
       " 'UnitPrice',\n",
       " 'Country',\n",
       " 'country_index',\n",
       " 'features']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformedTraining.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3029b7ab-d114-451c-b476-84b74dadd393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"country_index\",featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea3dec15-baa7-4d7b-9cc9-dba8408d351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see a list of all hyperparams of LogisticRegression\n",
    "# print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e72d953-690e-4946-a84b-10dc02c8c983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/23 15:06:18 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/02/23 15:06:18 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fittedLR = lr.fit(transformedTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec5ba721-71f6-4c3b-8cce-a6c0799c55f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 167:=============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|country_index|     avg(prediction)|\n",
      "+-------------+--------------------+\n",
      "|          8.0|                 0.0|\n",
      "|          0.0|8.581676287434812E-4|\n",
      "|          7.0|                 0.0|\n",
      "|         18.0|                 0.0|\n",
      "|          1.0|                 0.0|\n",
      "|         34.0|                 0.0|\n",
      "|          4.0|                 0.0|\n",
      "|         23.0|                 0.0|\n",
      "|         31.0|                 0.0|\n",
      "|         11.0|                 0.0|\n",
      "|         21.0|                 0.0|\n",
      "|         14.0|                 0.0|\n",
      "|         22.0|0.056179775280898875|\n",
      "|         19.0|                 0.0|\n",
      "|          3.0|                 0.0|\n",
      "|         28.0|                 0.0|\n",
      "|          2.0|                 0.0|\n",
      "|         17.0|                 0.0|\n",
      "|         27.0|                 0.0|\n",
      "|         10.0|                 0.0|\n",
      "|         13.0|                 0.0|\n",
      "|          6.0|                 0.0|\n",
      "|         20.0|                 0.0|\n",
      "|          5.0|0.020718232044198894|\n",
      "|         15.0|                 0.0|\n",
      "|         24.0|                 0.0|\n",
      "|          9.0|                 0.0|\n",
      "|         26.0|                 0.0|\n",
      "|         16.0|                 0.0|\n",
      "|         12.0|                 0.0|\n",
      "|         35.0|                 0.0|\n",
      "|         25.0|                 0.0|\n",
      "|         30.0|                 0.0|\n",
      "|         29.0|                 0.0|\n",
      "|         33.0|                 0.0|\n",
      "|         32.0|                 0.0|\n",
      "|         36.0|                 0.0|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fittedLR.transform(tranformedTest).select(\"country_index\", \"prediction\").\\\n",
    "groupBy(\"country_index\").avg(\"prediction\").show(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f895beb3-66c9-43cc-b4dc-3c8226cb17fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_index</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country_index  prediction\n",
       "0            5.0         5.0\n",
       "1            5.0         5.0\n",
       "2            5.0         5.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fittedLR.transform(tranformedTest).select(\"country_index\", \"prediction\")\\\n",
    ".filter((col('country_index') == col('prediction')) & (col('prediction') != 0)).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eba0864-361d-4f18-bcd8-66b8d359c1a7",
   "metadata": {},
   "source": [
    "## Tuning the hyper-params\n",
    "\n",
    "TODO: reminder why this tuning is needed, and when is it enough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ebb48-5325-47bb-9019-644e6bfde75d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
