{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6588ab2b-eb4a-4e8c-a1f3-1f9516599f00",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Understanding partitions\n",
    "\n",
    "To allow every executor to perform work in parallel, Spark breaks up the data into chunks called\n",
    "partitions. A partition is a collection of rows that sit on one physical machine in your cluster. A\n",
    "DataFrame’s partitions represent how the data is physically distributed across the cluster of\n",
    "machines during execution. If you have one partition, Spark will have a parallelism of only one,\n",
    "even if you have thousands of executors. If you have many partitions but only one executor,\n",
    "Spark will still have a parallelism of only one because there is only one computation resource. [SDG]\n",
    "\n",
    "\n",
    "Clusters will not be fully utilized unless you set the level of parallelism for each operation high enough. The general recommendation for Spark is to have 4x of partitions to the number of cores in cluster available for application, and for upper bound — the task should take 100ms+ time to execute. If it is taking less time than your partitioned data is too small and your application might be spending more time in distributing the tasks. [2]\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee1e73b2-c80a-4869-8ed0-b4a2600e49d4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/02/13 11:09:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/13 11:09:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0105927-5806-4b19-9288-e012dea44ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1m=spark.range(1*1000*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca0ae95-2ab9-4c14-9850-dfbd0a403962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In standalone mode, numPartitions == num cores in this machine.\n",
    "df_1m.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "349f81be-a4fe-4123-b73f-551341fd8e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this df has 4 partitions\n"
     ]
    }
   ],
   "source": [
    "df_4p=df_1m.repartition(4)\n",
    "print(f\"this df has {df_4p.rdd.getNumPartitions()} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7915ba96-be6f-4936-9732-3df544513c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does this statemnt has an effect of DF partition? (no)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "\n",
    "spark.range(1*1000*1000).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d95e617-3e18-47ce-a724-3b74d3971c98",
   "metadata": {},
   "source": [
    "## Repartition and Coalesce\n",
    "\n",
    "Another important optimization opportunity is to partition the data according to some frequently\n",
    "filtered columns, which control the physical layout of data across the cluster including the\n",
    "partitioning scheme and the number of partitions.\n",
    "Repartition will **incur a full shuffle of the data**, regardless of whether one is necessary. This\n",
    "means that you should typically only repartition when the future number of partitions is greater\n",
    "than your current number of partitions or when you are looking to partition by a set of columns\n",
    "\n",
    "Another opportunity to repartition is when writing data to disk: Each partition will be written to a separate folder, making reading the data faster. See https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrameWriter.partitionBy.html?highlight=partitionby#pyspark.sql.DataFrameWriter.partitionBy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74218141-10a7-4389-9cdf-3240e46ea9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can repartition a DF:\n",
    "df_10p = df_4p.repartition(10)\n",
    "df_10p.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcd378d-99f3-4eaa-8d91-2d563c8965ba",
   "metadata": {},
   "source": [
    "### Unbalanced partition sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f43175f7-ed57-4c0a-b814-169b40c57d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 157 ms, sys: 56.3 ms, total: 213 ms\n",
      "Wall time: 212 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This example *should* demonstrate SQEWED partitions: when the size of partitions is unbalanced.\n",
    "\n",
    "# source: https://luminousmen.com/post/spark-tips-partition-tuning\n",
    "# however, when I ran this code, the 'transactions' has 1 partition after repatition().\n",
    "# the 'df' has 1 partition.\n",
    "# Before calling repartition() it has 8.\n",
    "# Tested on Spark 3.2.0 standalone.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# set smaller number of partitions so they can fit the screen\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 3)\n",
    "# disable broadcast join to see the shuffle\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "length = 1000000\n",
    "names = np.random.choice(['Bob', 'James', 'Marek', 'Johannes', None], length)\n",
    "amounts = np.random.randint(0, 1000000, length)\n",
    "\n",
    "# generate skewed data\n",
    "country = np.random.choice(\n",
    "    ['United Kingdom', 'Poland', 'USA', 'Germany', 'Russia'],\n",
    "    length,\n",
    "    p = [0.05, 0.05, 0.8, 0.05, 0.05]\n",
    ")\n",
    "data = pd.DataFrame({'name': names, 'amount': amounts, 'country': country})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a25d28-f8c6-4857-aa12-e9dbb569ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "transactions = spark.createDataFrame(data).repartition('country') # <<<<<< here we repartition!\n",
    "\n",
    "print(f\"transactions has {transactions.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "countries = spark.createDataFrame(pd.DataFrame({\n",
    "    'id': [11, 12, 13, 14, 15],\n",
    "    'country': ['United Kingdom', 'Poland', 'USA', 'Germany', 'Russia']\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc57555-8d28-4a50-a364-080ec923fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = transactions.join(countries, 'country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4543780-644a-44b8-931f-a0819c14f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"df has {df.rdd.getNumPartitions()} partitions\")\n",
    "# check the partitions data\n",
    "# I use glom() ONLY for this demonstration. You don't really want get 'per partition' data into python!\n",
    "for i, part in enumerate(df.rdd.glom().collect()):\n",
    "    print({i: part[0:50]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1532b8a3-c2a4-48f7-811d-b9c2c13dafe6",
   "metadata": {},
   "source": [
    "# Check yourself\n",
    "* what happens if there are less partitions than executors?\n",
    "* what happens if there are more partitions than executors?\n",
    "* Who is repsonsible to partition the data?\n",
    "* what is the criterion to decide if a row is in a certain partition?\n",
    "* can an RDD be partitioned by columns only (vertical partitioning)?\n",
    "* Is partioning automatic? Can I influence it?\n",
    "\n",
    "Answer here [link here] and see your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef67fa5c-5822-4fbd-ab65-2e1e19cc6d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
