{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "241b3477-2842-41f0-a6e0-7b22087f19cf",
   "metadata": {},
   "source": [
    "# Reading from CSV / json file and writing to Parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7532a67b-b82a-4e1b-af78-7ac75163311c",
   "metadata": {},
   "source": [
    "This sample code reads a few fields from nested json, and creates a dataframe,\n",
    "\n",
    "Then write the dataframe to storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d6d239-fb7e-4ff8-989e-e45a85bb1558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff3a29-e132-4782-b605-a71b585583a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('02 reading').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efeb9bf-777a-40c5-826a-07ee8790f69b",
   "metadata": {},
   "source": [
    "The Spark UI is available at http://localhost:4040 when running locally in a PC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba80e1e2-0ffd-48e6-b4d4-18cd4fc35418",
   "metadata": {},
   "source": [
    "# Read nested json into a dataframe\n",
    "\n",
    "HINT: During testing, create a tiny jsonl file so reading is fast. For example `head -n 12 the-file.json > test_12.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b9311-0317-4589-9bac-01a0cef1bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/\n",
    "# https://bigdataprogrammers.com/read-nested-json-in-spark-dataframe/\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-explode-array-and-map-columns-to-rows/\n",
    "from pyspark.sql.types import MapType\n",
    "fname = \"../data/sample.json\"\n",
    "\n",
    "# Note: By default, the schema is inferred from the data.\n",
    "# This is slower and may sometime fail due to bad input files.\n",
    "# A possilbe workaround is to read a short well defined file, extract the schema from it, and then read\n",
    "# the full file using this schema.\n",
    "# inferred = spark.read.json(fname_ref)\n",
    "# inferred.printSchema()\n",
    "#bids = spark.read.schema(inferred.schema).json(fname)\n",
    "df = spark.read.json(fname)\n",
    "df.show()   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df5ce0-fd8c-41ca-9ac1-4fca584bf518",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74e41e6-b261-46ea-912e-e0d3053d6e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can get a few columns of our choice. note the nesting\n",
    "subset = df.select('address.zip', 'name') \n",
    "subset.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c61960-318e-44f2-ab97-9a110ae0b301",
   "metadata": {},
   "source": [
    "If the notebooks runs inside a Docker container, we need to provide access to the hosted data directory.\n",
    "\n",
    "For example, create a directory in the host and configure in docker-compose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f66a26-8ecf-4f8c-9319-2c02e7e3d402",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reading multiple files at a time\n",
    "https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/\n",
    "\n",
    "Using the read.json() method you can also read multiple JSON files from different paths, just pass all file names with fully qualified paths by separating comma, or a list of files"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b437390e-b674-4a15-935b-0b1fb713368d",
   "metadata": {},
   "source": [
    "prefix=\"wasbs://nc001@dacoursedatastorage.blob.core.windows.net/\"\n",
    "filelist = [ prefix + 'requestLog_D_919539.log.tar_' + str(i) + \".json\" for i in range(6)]\n",
    "test_df = spark.read.schema(inferred.schema).json(filelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d548e70-157a-4c5c-8209-49586a2b9125",
   "metadata": {},
   "source": [
    "# Writing the dataframe to storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88df48c-c2f7-4933-a1b6-76e64431f050",
   "metadata": {},
   "source": [
    "What if you want to persist (save values) of a DF?\n",
    "It can be saved to a database (covered in another lesson), or saved to a file in the file system.\n",
    "Using **Parquet** format is very efficient as we can see here.\n",
    "\n",
    "\n",
    "For example, In one test I read a jsonl file (602MB) into a DF, then wrote it to parquet file (actually it creates a directory with several files).\n",
    "The parquet file is compressed so the total saved storage was 92MB. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0679e059-f6fa-4bae-b1da-b0bc952eaecc",
   "metadata": {},
   "source": [
    "## The Parquet format\n",
    "Column based binary file format.\n",
    "- write once, read many → immutable.\n",
    "- optimized, compressed (per column). → write is slow, read is fast.\n",
    "- not indexed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0b73e3-f56b-4e32-a180-1ef9304487cb",
   "metadata": {},
   "source": [
    "## Alternative formats\n",
    "file formats similar to Parquet:\n",
    " - apache Iceberg  https://www.infoworld.com/article/3669848/why-apache-iceberg-will-rule-data-in-the-cloud.html\n",
    " - snappy (which is more of a compression algorithm)\n",
    " - AVRO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6263a89-8999-4fe4-8197-e295fbb1935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Read a CSV into a dataframe, inferring the schema.\n",
    "dataPath = \"../data/Open_Parking_and_Camera_Violations_1M.csv\"\n",
    "fines = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\",\"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(dataPath)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4cea40-88f7-4726-99bf-1b48053abeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fines.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f573c-a0e0-4121-af1f-0ce6c0cd30e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# the output file must NOT exist\n",
    "# Column names must not include spaces (and some other characters)\n",
    "newColNames = [ name.replace(' ','_') for name in fines.columns] # convert to valid names for Parquet\n",
    "fines.toDF(*newColNames).write.parquet(\"./fines1M.parquet\")\n",
    "\n",
    "# You can also drop irrelevant columns:\n",
    "fines.select(['Plate','Amount Due']).withColumnRenamed('Amount Due','AmountDue').write.parquet(\"./OnlyTwoFields.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a8f15-d481-42e5-a287-374ca0612591",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# read the DF from the parquet file:\n",
    "restored_df = spark.read.parquet(\"./fines1M.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaac6a90-34ac-4c88-8594-ddb84cf8b926",
   "metadata": {},
   "source": [
    "Let's run a few actions on the df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea20dd7-6d56-4ca4-89db-91ed9432baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b754b78-72ed-448c-86e0-3e58c44eebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_df.select(f.max(\"Plate\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ecd9bb-c07e-4274-acae-37eda46b280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_df.sort('Plate','County').limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499cf597-c9a2-4d80-9b48-5aa194500321",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_df.sort(f.col('Plate').desc(),'County').limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0b1c38-8cb7-4820-9132-959f8323202f",
   "metadata": {},
   "source": [
    "### Repartition before writing to storage\n",
    "\n",
    "Spark DataFrameWriter provides partitionBy method which can be used to partition data on write. It repartition the data into separate files on write using a provided set of columns. [2]\n",
    "\n",
    "Correctly choosing the key is important for good performance!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "67c28025-c44b-40fb-b1b3-18ce19daed36",
   "metadata": {},
   "source": [
    "df.write.partitionBy('key').json('/path/to/foo.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41692a2-5dce-4df2-94e5-997110200b32",
   "metadata": {},
   "source": [
    "### Bucketizing  \n",
    "(see SDG chapter 9, page 184)\n",
    "\n",
    "To improve search speed we can use bucketing according to a column value.\n",
    "(In spark 3.2.1, we must use `saveAsTable` when using `bucketBy`)\n",
    "\n",
    "How many files are created?<br>\n",
    "Answer: num_partitions * num_buckets == 8*4 in this example.\n",
    "(Actually, for each file there is a CRC file, so about 2\\*8\\*4 files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e030ffb-3663-43dc-a8a1-21b7141b5805",
   "metadata": {},
   "outputs": [],
   "source": [
    "fines.select(['county', 'state', 'Violation'])\\\n",
    ".write.format(\"parquet\").mode(\"overwrite\").bucketBy(4, \"county\")\\\n",
    ".saveAsTable(\"bucketed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cc0cfb-9f58-4962-9b71-ea097ea8cf06",
   "metadata": {},
   "source": [
    "# Check yourself\n",
    "\n",
    "* what will happen if you replace `fines.toDF(*newColNames).show()` with `fines.toDF(*newColNames).toPandas()` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5744cea1-a2fc-489a-8770-74a244558f84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
