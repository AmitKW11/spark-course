{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eccfe551-7af4-45ae-9d89-093e2228e9db",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Structured Streaming + Machine Learning\n",
    "\n",
    "In this notebook we run a more advanced streaming use case.\n",
    "\n",
    "To keep it conscise, I do not repeat explanations from the \"structured streaming\" notebook\n",
    "\n",
    "## The Task\n",
    "You have an input stream of invoice data (Exciting!). You need to predict the unit quantity given [date, unit price, \n",
    "country].\n",
    "You can train on some of the data (e.g. the first N=300K entries) and then predict the rest.\n",
    "Because this is an exercise, you also get the unit quantity, so you can easily compute the quality of the prediction.\n",
    "\n",
    "We don't care here to get a high precision - just demonstrate how to use ML with streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02383544-9d85-41d1-be96-ecef9600a638",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The plan\n",
    "\n",
    "Accumulate N rows into a single dataframe, train a (regression) model.<br>\n",
    "Use the next rows (until end of input stream) as test: perform the prediction and compare to the actual value [unit quantity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187d6160-f85f-4195-85ad-0d7af82ccc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2eb1c2-513f-439b-8cd3-14cec96e7066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCHEMA = \"InvoiceNo INT ,StockCode INT,Description STRING ,Quantity INT,InvoiceDate DATE,UnitPrice FLOAT,CustomerID FLOAT, country STRING\"\n",
    "\n",
    "spark = SparkSession.builder.appName('streamingML')\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"512m\")\\\n",
    "    .config('spark.jars', '/home/jars/*.jar')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526e00fe-ad3f-453b-a611-f0234a93f305",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_server = \"kafka:9092\"\n",
    "topic = \"retail\"\n",
    "\n",
    "batchSize = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded8f102-ae9a-44fd-afaa-bec91d9d7ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = spark.readStream\\\n",
    "                  .format(\"kafka\")\\\n",
    "                  .option(\"kafka.bootstrap.servers\", kafka_server)\\\n",
    "                  .option(\"subscribe\", topic)\\\n",
    "                  .option(\"startingOffsets\", \"earliest\")\\\n",
    "                  .option(\"failOnDataLoss\",False)\\\n",
    "                  .option(\"maxOffsetsPerTrigger\", batchSize )\\\n",
    "                  .load()\\\n",
    "                  .select(f.from_csv(f.decode(\"value\", \"US-ASCII\"), schema=SCHEMA).alias(\"value\")).select(\"value.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92deee-f4fc-4987-bc59-437c24a6c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = None\n",
    "trainedModel = None\n",
    "numTrainRows = max(30000,batchSize)\n",
    "numEpochTrain = numTrainRows / batchSize\n",
    "print(f\"The first {numEpochTrain:.0g} epochs will be used for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e71ec-b27b-498e-b1ae-2e69546db742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "    \"\"\"\n",
    "    Convert the raw input df to a format that can be used by the ML model.\n",
    "    The RandomForestRegressor expects to have 'features' column.\n",
    "    \n",
    "    We drop invalid entries, so may return less rows (sometimes 0)\n",
    "    \"\"\"\n",
    "    assembler = VectorAssembler(inputCols=['StockCode','UnitPrice','CustomerID'], outputCol=\"features\", handleInvalid='skip')\n",
    "    d = assembler.transform(df)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5015cb12-9ec7-4ce2-bcd6-d8a8feae9824",
   "metadata": {},
   "source": [
    "## Setting hyper parameters\n",
    "In this code we focus on the actual stream handling, so just use a single set of parameters.\n",
    "\n",
    "Look at the \"MLlib\" and sdg/advanced\\* notebook to learn how to automatically perform a grid search on a range of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fda79d-38c2-44fd-8c3b-00c902ceca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "numTrees, maxDepth = 30,20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12810a42-1351-45fa-b119-1a9f31dff988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_batch(data, epoch_num):\n",
    "    global train_data\n",
    "    global trainedModel\n",
    "    global numTrainRows\n",
    "    \n",
    "    #raw_count = data.count()\n",
    "    data = transform(data)\n",
    "    trans_count = data.count()\n",
    "    #print(f\"batch size before/after tranform: {raw_count}/{trans_count}\")\n",
    "    if trans_count == 0:\n",
    "        print(f\"NOTHING to process in epoch {epoch_num}\")\n",
    "        return\n",
    "    if epoch_num == 0:\n",
    "        train_data = data\n",
    "    elif epoch_num < numEpochTrain:\n",
    "        train_data = train_data.union(data) # just collect more data (what about garbage collection?)\n",
    "    elif epoch_num == numEpochTrain:\n",
    "        # at last, train the model\n",
    "        print(\"Got all the training data...\")\n",
    "        rf = RandomForestRegressor(numTrees=numTrees, maxDepth=maxDepth, labelCol='Quantity')\n",
    "        # rf.setSeed(42) # just during debug!!! \n",
    "        trainedModel = rf.fit(train_data)\n",
    "        del train_data  # not needed any more, so free some memory\n",
    "        print(\"Finished model training\")\n",
    "    else:\n",
    "        # apply the model\n",
    "        test_predictions = trainedModel.transform(data.select(['Quantity','features']))\n",
    "        evaluator = RegressionEvaluator(labelCol='Quantity', predictionCol='prediction')\n",
    "        pred_error = evaluator.evaluate(test_predictions)\n",
    "        print(f\"{epoch_num}\\t RMSE:{pred_error:4.3g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129abcb9-d750-42c9-b2ab-3d792ca58cda",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Finally! run the next cell\n",
    "Wait a few minutes until data is collected and the model is trained,  and then you will get the prediciton errors for each batch.\n",
    "\n",
    "While you wait, open the UI at http://localhost:4040/StreamingQuery/ and click the **Run ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111e262a-a551-4b8f-a9fd-9b9d371dc439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "streaming_df.writeStream\\\n",
    "    .foreachBatch(handle_batch)\\\n",
    "    .start()\\\n",
    "    .awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15918922-5dee-4b12-b075-b578a019dca9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"https://www.freepnglogos.com/uploads/the-end-png/the-end-photographe-ois-love-life-photography-27.png\" width=\"200\"/>\n",
    "<hr>\n",
    "\n",
    "# Tips for developing your code\n",
    "- start with simple data. Instead of using streaming, apply the code to a 'regular' dataframe\n",
    "- start with a simple mode (for the RandomForest for example, use numTrees=2, maxDepth=2), then increase to get meaningful results\n",
    "- save your source versions with meaningful comments. You will want to get back to a version that worked before. (use git)\n",
    "- save a cache of a dataframe in Parquet file -- see below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51abea3-15a2-49a6-90a1-f6e798b24a85",
   "metadata": {},
   "source": [
    "## Running handle_batch() manually (for developing and debugging)\n",
    "I wanted to control the calls, and keep it fast, so I created a static df, read it from file and used it for training -- just to get the code working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeeee14-da7c-4b29-aea0-e46fe512a1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "def loadData(numLines = 600):\n",
    "    \"\"\"\n",
    "    Load data from cached file if it exsists, and from Kafka otherwise\n",
    "    :return: dataframe\n",
    "    \"\"\"\n",
    "    fname = f\"./retail{numLines}.parquet\"\n",
    "    # Here you see the EAFP pattern in work\n",
    "    # https://realpython.com/python-lbyl-vs-eafp/#the-easier-to-ask-forgiveness-than-permission-eafp-style\n",
    "    try:\n",
    "        df = spark.read.parquet(fname)\n",
    "    except AnalysisException as ex: # catch only relevant exception, so other causes will crash the code asap.\n",
    "        df = spark.read.format(\"kafka\")\\\n",
    "                  .option(\"kafka.bootstrap.servers\", kafka_server).option(\"subscribe\", topic).option(\"startingOffsets\", \"earliest\")\\\n",
    "                  .load().limit(numLines)\n",
    "        df = df.select(f.from_csv(f.decode(\"value\", \"US-ASCII\"), schema=SCHEMA).alias(\"value\")).select(\"value.*\")\n",
    "        df = df.drop('InvoiceNo').drop('Description').drop('InvoiceDate').drop('country')\n",
    "        df.write.parquet(fname)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ebea53-c06d-4a80-a73f-ca7e6a676c22",
   "metadata": {},
   "source": [
    "I am running the next cell dozens of times, changing the functions above, until the code is working as expected.\n",
    "\n",
    "During debugging I set `numTrees, maxDepth = 5,2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e44c05-890e-4a2b-9d97-8e796537dc2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = loadData().randomSplit([1.0,1.0,1.0])\n",
    "\n",
    "numEpochTrain = 1\n",
    "handle_batch(x[0],0) # train\n",
    "handle_batch(x[1],1) # train\n",
    "handle_batch(x[2],2) # test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b040502-8e9a-4183-b8ce-c38b5dccada2",
   "metadata": {},
   "source": [
    "# Check yourself\n",
    "- In handle_batch() we train the model after N records. What is a potential problem with this method if it was a live stream?\n",
    "- why should we remove such statements? `rf.setSeed(42)`\n",
    "- what happens if you set `batchSize = 5000000` ? why?\n",
    "\n",
    "- set `batchSize = 5000`. Find the **Process Rate**, **Batch Duration** in the UI link.\n",
    "  - How long did the tranining take?\n",
    "  - What is the fastest rate your code (on this machine) can handle data in units of [records/sec]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
