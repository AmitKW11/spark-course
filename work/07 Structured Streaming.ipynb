{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eccfe551-7af4-45ae-9d89-093e2228e9db",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Structured Streaming\n",
    "\n",
    "[**Watch the video**](https://panoptotech.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=c42b2eeb-4453-4b05-964e-afb0013d9354)\n",
    " --- This video is for an older version of the notebook\n",
    "\n",
    "For this lab, we will need a data streaming source - A Kafka server.\n",
    "\n",
    "The Kafka server is part of the docker images you already have.\n",
    "\n",
    "We can create one by using Kafka server that simulates a live data stream.\n",
    "\n",
    "Instructions on setting the Kafka server are in `prepare_kafka_server.md` in the root directory of this repo. [**Watch the video**](https://panoptotech.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=f36885e6-8caf-43d9-a6fe-afb00140e63f)\n",
    "\n",
    "\n",
    "refer to [sdg].p44\n",
    "\n",
    "and the root: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "\n",
    "https://docs.databricks.com/spark/latest/structured-streaming/index.html\n",
    "\n",
    "Video from Spark conf 2016: from the developer\n",
    "https://www.youtube.com/watch?v=rl8dIzTpxrI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Basic Concepts\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f044304f-383d-48fe-afbb-b26e378d968e",
   "metadata": {},
   "source": [
    "## Output modes\n",
    "Each time a *trigger* happens we may want to write data to the output (e.g. database, HDFS, datasink).\n",
    "\n",
    "Spark provides these modes:\n",
    "\n",
    "### Append\n",
    "Only the new rows appended to the result table since the last trigger will be written to the external storage. This is applicable only on queries where existing rows in the result table cannot change (e.g., a map on an input stream).\n",
    "\n",
    "### Complete\n",
    "The entire updated result table will be written to external storage.\n",
    "\n",
    "### Update\n",
    "Only the rows that were updated in the result table since the last trigger will be changed in the external storage. This mode works for output sinks that can be updated in place, such as a MySQL table\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02383544-9d85-41d1-be96-ecef9600a638",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The plan\n",
    "You will read data from Kafka data source using the streaming API. Whenever new data is received, you will repeat a calculation (count county instances) and write the new value to the output. \n",
    "\n",
    "Note: in this example we use stdout, but we could also write to a database and update existing records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187d6160-f85f-4195-85ad-0d7af82ccc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "import os,time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7621dad1-88f5-4f7b-80d8-c7abf625b49c",
   "metadata": {},
   "source": [
    "_note:_ the Kafka driver has to be supplied from somewhere - internet or locally.<br>\n",
    "I added the needed driver files to the Docker image so you don't have to wait for download each time you start Docker container.\n",
    "\n",
    "If you need some other driver, the syntax is like: \n",
    "`config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0')`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2eb1c2-513f-439b-8cd3-14cec96e7066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Schema for retail data \n",
    "SCHEMA = \"InvoiceNo INT ,StockCode INT,Description STRING ,Quantity INT,InvoiceDate DATE,UnitPrice FLOAT,CustomerID FLOAT, country STRING\"\n",
    "\n",
    "# The config packages must match the specific Spark version you run!\n",
    "spark = SparkSession.builder.appName('streaming')\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"512m\")\\\n",
    "    .config('spark.jars', '/home/jars/*.jar')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\") # too much noise? replace with \"ERROR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5e0283-ecb4-417a-82b8-b30e77247672",
   "metadata": {},
   "source": [
    "_note:_ \"kafka_server\" is the URL where the Kafka server is running. \n",
    "\n",
    "For example: \n",
    "    myserver.aws.com:29092\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04d0161-40c0-4854-85e6-4dd31eaf5400",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_server = \"kafka:9092\"  # internal name in the Docker network\n",
    "#kafka_server = \"20.169.149.9:29092\"\n",
    "topic = \"retail\"             # the topic name where the data is stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a51d679-2c19-48e8-9e32-567ae94d02ae",
   "metadata": {},
   "source": [
    "## Read the data stream into a regular DataFrame.\n",
    "\n",
    "The dataframe will get bigger and bigger -- so BE CAREFUL!.\\\n",
    "This is ONLY to demonstrate `read()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13e23e9-e19f-4032-a7bf-dfd3c17227a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "static_df = spark.read\\\n",
    "                  .format(\"kafka\")\\\n",
    "                  .option(\"kafka.bootstrap.servers\", kafka_server)\\\n",
    "                  .option(\"subscribe\", topic)\\\n",
    "                  .option(\"startingOffsets\", \"earliest\")\\\n",
    "                  .option(\"failOnDataLoss\",False)\\\n",
    "                  .load()\n",
    "retail_data = static_df.select(f.from_csv(f.decode(\"value\", \"US-ASCII\"), schema=SCHEMA).alias(\"value\")).select(\"value.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ac66d5-3640-4a29-a1b1-3cfec7af00ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# on my pc, there is a fixed 3 sec time for each of count() and show()  ?!\n",
    "# this is probably a spark config: https://stackoverflow.com/questions/59916338/why-is-there-a-delay-in-the-launch-of-spark-executors\n",
    "print(\"%d records in frame\" % retail_data.count())\n",
    "retail_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6c436c-1dd1-412f-a7d7-85efd921aa38",
   "metadata": {},
   "source": [
    "## Read the data stream using the streaming API\n",
    "\n",
    "It does not make sense to read infinite data (or at least unbounded) into a dataframe. We will read from the stream and perform some computation on the data, such as finding commulative count.\n",
    "\n",
    "Let's try to read in streaming mode (a.k.a micro batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec24f86c-84e1-4781-b22c-aab8db77e1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFSETS_PER_TRIGGER = 500\n",
    "streaming_df = spark.readStream\\\n",
    "                  .format(\"kafka\")\\\n",
    "                  .option(\"kafka.bootstrap.servers\", kafka_server)\\\n",
    "                  .option(\"subscribe\", topic)\\\n",
    "                  .option(\"startingOffsets\", \"earliest\")\\\n",
    "                  .option(\"failOnDataLoss\",False)\\\n",
    "                  .option(\"maxOffsetsPerTrigger\", OFFSETS_PER_TRIGGER )\\\n",
    "                  .load()\\\n",
    "                  .select(f.from_csv(f.decode(\"value\", \"US-ASCII\"), schema=SCHEMA).alias(\"value\")).select(\"value.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b153582-a27e-47c8-9e1f-879e77edaff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the structure of the DF\n",
    "streaming_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60de81e-d97b-4993-bd78-b9e5f5227728",
   "metadata": {},
   "source": [
    "_notes:_ \n",
    "1. Monitoring Streaming Queries: check the formal [docs](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#reading-metrics-interactively)\n",
    "1. In the next cell we are writing to **memory sink**. This is for *debugging only*. In real life you would write to a file or some database. (see the next example)\n",
    "1. In JupyterNotebook, writing to the console does not work (hence not using `format(\"console\")`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68967bd8-3ab2-4ed8-a957-5db657b234cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "country_counts = streaming_df.groupBy('country').count()\n",
    "count_countries_query =country_counts.writeStream\\\n",
    ".queryName('num_countries')\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()\n",
    "\n",
    "# https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c033fc52-920d-4079-a393-2bf74f9da1e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wait 20 seconds, letting Spark do its thing.\n",
    "# During this time, Spark will run the query on each incoming microbatch\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "for _ in range(10):\n",
    "    clear_output(wait=True)\n",
    "    print(\"query status:\",count_countries_query.status)\n",
    "    spark.sql('SELECT * FROM num_countries').show()\n",
    "    time.sleep(2)\n",
    "    \n",
    "count_countries_query.stop()\n",
    "# If you don't stop the query, it will run forever, waiting for more data to arrive from the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f462508-757c-4edf-9343-243a1672cf5c",
   "metadata": {},
   "source": [
    "# Using foreach()\n",
    "\n",
    "In the next example, we take it one step forward: get rid of the sleep() and handle batches in our own function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47924a0f-ee51-4cc6-968f-7687193dace8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_batch(df, epoch_id):\n",
    "    \"\"\"\n",
    "    This function is called for each batch. \n",
    "    Do whatever you want with it.\n",
    "    \\param df  Dataframe, containing 'batch size' rows of the input data.\n",
    "    \\param epoch_id  int, 0 based counter \n",
    "    \"\"\"\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{epoch_id}:   {df.count()}\")\n",
    "    df.groupBy('country').count().show()\n",
    "    # TODO: replace with something more inteligent that involves the full data (e.g. aggregate over all data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986ec56-9692-45d3-b240-f413f30c4e73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = streaming_df.writeStream.foreachBatch(process_batch).start()\n",
    "time.sleep(10) # This sleep() is only so you can move on automatically to the next cell.\n",
    "# Remember to stop the query when you had enough.\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f4d3d-5bf4-40ba-981d-e796fbeba000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and then wait for the query to terminate. It can take some time.\n",
    "try:\n",
    "    if query.awaitTermination(timeout=20): # This is a blocking call\n",
    "        print(\"Query terminated\")\n",
    "    else:\n",
    "        print(\"\\nWARNING: the query did NOT terminate!\")\n",
    "except Exception as e:\n",
    "    print(f\"got  {str(type(e))}. This is caused by interrupting the 'process_batch' and can be ignored\")\n",
    "    \n",
    "query.status['isTriggerActive']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a33673-ccc1-4354-87bd-4e3d3ce1c433",
   "metadata": {},
   "source": [
    "> **Tip:** Some important information is printed to the log output.<br>\n",
    "Use `docker logs spark-lab -f` in another terminal to get a live stream of the log messages.\n",
    " \n",
    "> **Tip:** If you get `IllegalArgumentException: Cannot start query with name num_countries2 as a query with that name is already active in this SparkSession` it is because I ran the cell twice. Restart the Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1fb38-6da4-4ffa-b9cd-d210af5d6569",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_countries_query2 =country_counts.writeStream\\\n",
    ".queryName('num_countries3')\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()\n",
    "\n",
    "for _ in range(10):\n",
    "    clear_output(wait=True)\n",
    "    display(count_countries_query2.status)\n",
    "    spark.sql('SELECT * FROM num_countries').show()\n",
    "    time.sleep(2)\n",
    "    \n",
    "count_countries_query2.stop()\n",
    "if count_countries_query2.awaitTermination(timeout=2):\n",
    "    print(\"Query terminated\")\n",
    "else:\n",
    "    print(\"\\nWARNING: the query did NOT terminate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad30d2d4-757c-48b2-b55d-d93cc6e52386",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Let's use something more realistic: Read a data stream, process it, write output to a database sink.\n",
    "\n",
    "Make sure you completed the \"Working with databases\" lesson, and that the database is up and running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11200314-12ef-48e1-bae0-57acb896f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc69dc0-6f3e-4a30-8e67-5b563ffbd416",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Reading text from a network connection\n",
    "\n",
    "Copied verbatim (מִלָה בְּמִלָה) from https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#quick-example\n",
    "\n",
    "*FIRST* run the data source in another window, and then run the cell below. (If you first run the cell, it will complain on \"Connection refused\" which means there is no input).\n",
    "\n",
    "When you had enough, close the data source, and the cell will finish automatically (because it will identify the connection is terminated)\n",
    "\n",
    "NOTE: Reading from socket is only for debug/prototyping.<br>\n",
    "Also **writing to console, socket, memory is for debug only.** (see [SDG]p 357)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5058703c384ba23",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Installing netcat (nc) in the Spark server (in the running container)\n",
    "In the video, I used preinstalled 'nc' application.  Here are the instructions to install it:\n",
    "\n",
    "```\n",
    "docker exec -it spark-lab sh\n",
    "# now, in the container's shell:\n",
    "wget https://busybox.net/downloads/binaries/1.35.0-x86_64-linux-musl/busybox_NC -O nc\n",
    "chmod +x nc\n",
    "\n",
    "# run it!\n",
    "./nc localhost -l -p 9999\n",
    "\n",
    "# to exit, press ^C\n",
    "```\n",
    "\n",
    "Remember that each time a container is killed, all its content is removed, so you will have to install again.\n",
    "\n",
    "> **Note1:** when running in Jupyter notebook, writing to the console is not visible in the cell output. \n",
    "Istead, it is written to the log output of the spark node, so run the next line of code in a terminal. It will look for these prints and print the first 10 lines: `2>&1 docker logs spark-lab -f | grep -A 10 \"^Batch:\"`\n",
    "\n",
    " > **Note2:** After the `nc` server is stopped, the spark client may try to reconnect so the cell will run indefinitely until your stop the jupyter kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8749a56c-dba6-48fa-b382-8d281b181e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select(\n",
    "   explode(\n",
    "       split(lines.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "\n",
    " # Start running the query that prints the running counts to the console(note1)\n",
    "query = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "\n",
    "if query.awaitTermination():\n",
    "    print(\"Query terminated\")\n",
    "else:\n",
    "    print(\"\\nWARNING: the query did NOT terminate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e332e5-3d12-4919-93a3-989884f9f9e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Challenges in distributed data streaming\n",
    "\n",
    "## Consistency\n",
    "Some data is already processed in one node, but stale values in another node - can cause errors\n",
    "\n",
    "## Fault tolerance\n",
    "How to handle failed reads?\n",
    "\n",
    "## Out of order data\n",
    "e.g. node 1 received and processed \"close event\" and node 2 then processes the \"start event\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6be5311-1b45-48b4-b1e6-c2fbaa135d3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# What we did not cover here\n",
    "\n",
    "This was just a taste of the streaming API. \n",
    "\n",
    "New features are added from time to time, so checking the docs is always advised.\n",
    "\n",
    "Some interesting topics to follow:\n",
    "* selection, Projection\n",
    "* Handling errors (duplication, recovery ...)\n",
    "* Window operations (see 'window functions' notebook)\n",
    "* Join operations\n",
    "* ForeachBatch -- call a user function https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.streaming.DataStreamWriter.foreachBatch.html\n",
    "  * see also https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#using-foreach-and-foreachbatch\n",
    "* Foreach sink -- call a user function for each record, giving more control  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec16b88-418f-4437-8481-f9f729f76788",
   "metadata": {},
   "source": [
    "## Debugging and testing\n",
    "\n",
    "You can use **memory, console** and **socket** for debugging. They should not be used for production due to performance and lack of fault tolerance!\n",
    "\n",
    "If you do want to output data to a table for interactive SQL queries in production, the authors\n",
    "recommend using the Parquet file sink on a distributed file system (e.g., S3). You can then query\n",
    "the data from any Spark application. [SDG]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d877ec54-83cf-4a21-a8eb-e9582b22ee06",
   "metadata": {},
   "source": [
    "# Check yourself\n",
    "\n",
    "* What will happen if you run 'country_counts.show()'? Why?\n",
    "* change OFFSETS_PER_TRIGGER to 100. How does it affect the processing?\n",
    "* replace `outputMode(\"complete\")` with \"append\" and \"update\" and run the kafka code again. Is this what you expected?\n",
    "* When using *memory* sink, (the memory of) which node is used? \n",
    "* what happens if you subscribe to nonexistent topic in Kafka?\n",
    "* What happens if the microbatch processing time is longer than the input data rate?\n",
    "\n",
    "Answer [here](https://forms.gle/cUPe5xeTwoGwbH4PA) and see your results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2e68d-48ea-4834-b360-8fc8b45c06f6",
   "metadata": {},
   "source": [
    "# Finished? <p style=\"color:red;\">Remember to stop/kill the Docker container to avoid consuming CPU.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
