{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b17acd7c-48e4-4480-a6a8-8dc8a7295336",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Structured Streaming\n",
    "\n",
    "For this lab, we will need a data streaming source\n",
    "\n",
    "refer to [sdg].p44\n",
    "\n",
    "and the root: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "\n",
    "\n",
    "We can create one by using Kafka server that simulates a live data stream.\n",
    "\n",
    "Instructions on setting the Kafka server are in `prepare_kafka_server.md` in the root directory of this repo.\n",
    "\n",
    "\n",
    "## Basic Concepts\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png\">\n",
    "\n",
    "\n",
    "## The plan\n",
    "You will read data from Kafka data source using the streaming API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "187d6160-f85f-4195-85ad-0d7af82ccc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "import os,time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d2eb1c2-513f-439b-8cd3-14cec96e7066",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1a574f2b-8893-40fa-8b22-435f9aa036fd;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 609ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1a574f2b-8893-40fa-8b22-435f9aa036fd\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/9ms)\n",
      "23/02/21 11:19:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Schema for retail data \n",
    "SCHEMA = \"InvoiceNo INT ,StockCode INT,Description STRING ,Quantity INT,InvoiceDate DATE,UnitPrice FLOAT,CustomerID FLOAT, country STRING\"\n",
    "\n",
    "# The config packages will try to download the needed packages from maven.org --> you need internet connection\n",
    "# It must match the specific Spark version you run!\n",
    "spark = SparkSession.builder.appName('streaming')\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"512m\")\\\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0')\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f04d0161-40c0-4854-85e6-4dd31eaf5400",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_server = \"kafka:9092\"  # internal name in the Docker network\n",
    "topic = \"retail\"             # the topic name where the data is stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a51d679-2c19-48e8-9e32-567ae94d02ae",
   "metadata": {},
   "source": [
    "## Read the data stream into a regular DataFrame.\n",
    "\n",
    "The dataframe will get bigger and bigger -- so BE CAREFUL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b13e23e9-e19f-4032-a7bf-dfd3c17227a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "static_df = spark.read\\\n",
    "                  .format(\"kafka\")\\\n",
    "                  .option(\"kafka.bootstrap.servers\", kafka_server)\\\n",
    "                  .option(\"subscribe\", topic)\\\n",
    "                  .option(\"startingOffsets\", \"earliest\")\\\n",
    "                  .option(\"failOnDataLoss\",False)\\\n",
    "                  .load()\n",
    "retail_data = static_df.select(f.from_csv(f.decode(\"value\", \"US-ASCII\"), schema=SCHEMA).alias(\"value\")).select(\"value.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43ac66d5-3640-4a29-a1b1-3cfec7af00ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "542214 records in frame\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|CustomerID|       country|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|     null|     null|         Description|    null|       null|     null|      null|       Country|\n",
      "|   536365|     null|WHITE HANGING HEA...|       6| 2010-12-01|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6| 2010-12-01|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|     null|CREAM CUPID HEART...|       8| 2010-12-01|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|     null|KNITTED UNION FLA...|       6| 2010-12-01|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 4.59 ms, sys: 2.32 ms, total: 6.9 ms\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# on my pc, there is a fixed 3 sec time for each of count() and show()  ?!\n",
    "# this is probably a spark config: https://stackoverflow.com/questions/59916338/why-is-there-a-delay-in-the-launch-of-spark-executors\n",
    "print(\"%d records in frame\" % retail_data.count())\n",
    "retail_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6c436c-1dd1-412f-a7d7-85efd921aa38",
   "metadata": {},
   "source": [
    "## Read the data stream using the streaming API\n",
    "\n",
    "It does not make sense to read infinite data (or at least unbounded) into a dataframe. \n",
    "\n",
    "Let's try to read in streaming mode (a.k.a micro batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec24f86c-84e1-4781-b22c-aab8db77e1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFSETS_PER_TRIGGER = 5000\n",
    "streaming_df = spark.readStream\\\n",
    "                  .format(\"kafka\")\\\n",
    "                  .option(\"kafka.bootstrap.servers\", kafka_server)\\\n",
    "                  .option(\"subscribe\", topic)\\\n",
    "                  .option(\"startingOffsets\", \"earliest\")\\\n",
    "                  .option(\"failOnDataLoss\",False)\\\n",
    "                  .option(\"maxOffsetsPerTrigger\", OFFSETS_PER_TRIGGER )\\\n",
    "                  .load()\\\n",
    "                  .select(f.from_csv(f.decode(\"value\", \"US-ASCII\"), schema=SCHEMA).alias(\"value\")).select(\"value.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b153582-a27e-47c8-9e1f-879e77edaff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: int, StockCode: int, Description: string, Quantity: int, InvoiceDate: date, UnitPrice: float, CustomerID: float, country: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the structure of the DF\n",
    "streaming_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68967bd8-3ab2-4ed8-a957-5db657b234cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/21 11:23:37 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-37b1bef1-b86f-4954-818c-f11f3fd33f95. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/02/21 11:23:37 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "country_counts = streaming_df.groupBy('country').count()\n",
    "count_countries_query =country_counts.writeStream\\\n",
    ".queryName('num_countries')\\\n",
    ".format(\"console\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()\n",
    "\n",
    "# https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00a193fe-89c7-40e2-94c0-5b881d55bf98",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------+-----+\n",
      "|       country|count|\n",
      "+--------------+-----+\n",
      "|       Germany|   30|\n",
      "|        France|   20|\n",
      "|          EIRE|   24|\n",
      "|        Norway|   73|\n",
      "|       Country|    2|\n",
      "|     Australia|   14|\n",
      "|United Kingdom| 4835|\n",
      "|   Netherlands|    2|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------------+-----+\n",
      "|       country|count|\n",
      "+--------------+-----+\n",
      "|       Germany|  181|\n",
      "|        France|  106|\n",
      "|       Belgium|   12|\n",
      "|         Italy|   24|\n",
      "|          EIRE|  109|\n",
      "|     Lithuania|   34|\n",
      "|        Norway|   73|\n",
      "|         Spain|    5|\n",
      "|   Switzerland|    6|\n",
      "|         Japan|   16|\n",
      "|       Country|    4|\n",
      "|        Poland|    8|\n",
      "|      Portugal|    7|\n",
      "|     Australia|   14|\n",
      "|United Kingdom| 9399|\n",
      "|   Netherlands|    2|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------------+-----+\n",
      "|       country|count|\n",
      "+--------------+-----+\n",
      "|       Germany|  213|\n",
      "|        France|  167|\n",
      "|       Belgium|   12|\n",
      "|         Italy|   25|\n",
      "|          EIRE|  145|\n",
      "|     Lithuania|   34|\n",
      "|        Norway|   73|\n",
      "|         Spain|    5|\n",
      "|       Iceland|   31|\n",
      "|   Switzerland|    6|\n",
      "|         Japan|   16|\n",
      "|       Country|    6|\n",
      "|        Poland|    8|\n",
      "|      Portugal|   14|\n",
      "|     Australia|   14|\n",
      "|United Kingdom|14229|\n",
      "|   Netherlands|    2|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------------+-----+\n",
      "|       country|count|\n",
      "+--------------+-----+\n",
      "|       Germany|  254|\n",
      "|        France|  197|\n",
      "|       Belgium|   12|\n",
      "|         Italy|   25|\n",
      "|          EIRE|  145|\n",
      "|     Lithuania|   35|\n",
      "|        Norway|  147|\n",
      "|         Spain|    5|\n",
      "|       Iceland|   31|\n",
      "|   Switzerland|    6|\n",
      "|         Japan|   17|\n",
      "|       Country|    8|\n",
      "|        Poland|    8|\n",
      "|      Portugal|   80|\n",
      "|     Australia|   22|\n",
      "|United Kingdom|19006|\n",
      "|   Netherlands|    2|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/21 11:23:58 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@6ee6191f is aborting.\n",
      "23/02/21 11:23:58 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@6ee6191f aborted.\n",
      "23/02/21 11:23:58 WARN Shell: Interrupted while joining on: Thread[Thread-8606,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/02/21 11:23:58 WARN Shell: Interrupted while joining on: Thread[Thread-8604,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/02/21 11:23:58 WARN Shell: Interrupted while joining on: Thread[Thread-8605,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:751)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/02/21 11:23:58 WARN Shell: Interrupted while joining on: Thread[Thread-8607,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/02/21 11:23:58 WARN Shell: Interrupted while joining on: Thread[Thread-8608,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1590)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/02/21 11:23:58 WARN Shell: Interrupted while joining on: Thread[Thread-8610,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/02/21 11:23:58 WARN Shell: Interrupted while joining on: Thread[Thread-8609,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:211)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:346)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:142)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:351)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:127)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/02/21 11:23:58 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 17 (task 825, attempt 0, stage 13.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/02/21 11:23:58 ERROR DataWritingSparkTask: Aborting commit for partition 17 (task 825, attempt 0, stage 13.0)\n",
      "23/02/21 11:23:58 ERROR DataWritingSparkTask: Aborted commit for partition 17 (task 825, attempt 0, stage 13.0)\n",
      "23/02/21 11:23:58 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 18 (task 826, attempt 0, stage 13.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/02/21 11:23:58 ERROR DataWritingSparkTask: Aborting commit for partition 18 (task 826, attempt 0, stage 13.0)\n",
      "23/02/21 11:23:58 ERROR DataWritingSparkTask: Aborted commit for partition 18 (task 826, attempt 0, stage 13.0)\n",
      "23/02/21 11:23:58 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 21 (task 829, attempt 0, stage 13.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/02/21 11:23:58 ERROR DataWritingSparkTask: Aborting commit for partition 21 (task 829, attempt 0, stage 13.0)\n",
      "23/02/21 11:23:58 ERROR DataWritingSparkTask: Aborted commit for partition 21 (task 829, attempt 0, stage 13.0)\n",
      "23/02/21 11:23:58 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 19 (task 827, attempt 0, stage 13.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/02/21 11:23:58 ERROR DataWritingSparkTask: Aborting commit for partition 19 (task 827, attempt 0, stage 13.0)\n",
      "23/02/21 11:23:58 ERROR DataWritingSparkTask: Aborted commit for partition 19 (task 827, attempt 0, stage 13.0)\n",
      "23/02/21 11:23:58 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 22 (task 830, attempt 0, stage 13.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/02/21 11:23:58 ERROR DataWritingSparkTask: Aborting commit for partition 22 (task 830, attempt 0, stage 13.0)\n",
      "23/02/21 11:23:58 ERROR DataWritingSparkTask: Aborted commit for partition 22 (task 830, attempt 0, stage 13.0)\n",
      "23/02/21 11:23:58 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 23 (task 831, attempt 0, stage 13.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/02/21 11:23:58 ERROR DataWritingSparkTask: Aborting commit for partition 23 (task 831, attempt 0, stage 13.0)\n",
      "23/02/21 11:23:58 ERROR DataWritingSparkTask: Aborted commit for partition 23 (task 831, attempt 0, stage 13.0)\n",
      "23/02/21 11:23:58 WARN TaskSetManager: Lost task 19.0 in stage 13.0 (TID 827) (eac212d573d5 executor driver): TaskKilled (Stage cancelled)\n",
      "23/02/21 11:23:58 WARN TaskSetManager: Lost task 18.0 in stage 13.0 (TID 826) (eac212d573d5 executor driver): TaskKilled (Stage cancelled)\n",
      "23/02/21 11:23:58 WARN TaskSetManager: Lost task 21.0 in stage 13.0 (TID 829) (eac212d573d5 executor driver): TaskKilled (Stage cancelled)\n",
      "23/02/21 11:23:58 WARN TaskSetManager: Lost task 22.0 in stage 13.0 (TID 830) (eac212d573d5 executor driver): TaskKilled (Stage cancelled)\n",
      "23/02/21 11:23:58 WARN TaskSetManager: Lost task 17.0 in stage 13.0 (TID 825) (eac212d573d5 executor driver): TaskKilled (Stage cancelled)\n",
      "23/02/21 11:23:58 WARN TaskSetManager: Lost task 23.0 in stage 13.0 (TID 831) (eac212d573d5 executor driver): TaskKilled (Stage cancelled)\n",
      "23/02/21 11:23:58 ERROR Utils: Aborting task\n",
      "org.apache.spark.executor.CommitDeniedException: Commit denied for partition 20 (task 828, attempt 0, stage 13.0)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:620)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/02/21 11:23:58 ERROR DataWritingSparkTask: Aborting commit for partition 20 (task 828, attempt 0, stage 13.0)\n",
      "23/02/21 11:23:58 ERROR DataWritingSparkTask: Aborted commit for partition 20 (task 828, attempt 0, stage 13.0)\n",
      "23/02/21 11:23:58 WARN TaskSetManager: Lost task 20.0 in stage 13.0 (TID 828) (eac212d573d5 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "# wait some time like we have something better to do.\n",
    "# During this time the Spark will run the query on each incoming microbatch\n",
    "time.sleep(20)\n",
    "count_countries_query.stop()\n",
    "# If you don't stop the query, it will run forever, waiting for more data to arrive from the input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc69dc0-6f3e-4a30-8e67-5b563ffbd416",
   "metadata": {},
   "source": [
    "# Another example - reading text from a network connection\n",
    "\n",
    "Copied verbatim (מִלָה בְּמִלָה) from https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#quick-example\n",
    "\n",
    "*FIRST* run the data source in another window, and then run the cell below. (If you first run the cell, it will complain on \"Connection refused\" which means there is no input.\n",
    "\n",
    "When you had enough, close the datasource, and the cell will finish automatically (because it will identiy the connection is terminated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8749a56c-dba6-48fa-b382-8d281b181e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/21 11:27:37 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n",
      "23/02/21 11:27:37 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-bf22d1ca-7c4a-404e-8c59-2262a2065570. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/02/21 11:27:37 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "|  is|    1|\n",
      "|noam|    1|\n",
      "|here|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "|some|    1|\n",
      "| not|    1|\n",
      "|  is|    2|\n",
      "|noam|    1|\n",
      "|here|    2|\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "| some|    1|\n",
      "|  not|    2|\n",
      "|nice,|    1|\n",
      "|  it?|    1|\n",
      "|   is|    4|\n",
      "|spark|    1|\n",
      "| noam|    1|\n",
      "| here|    2|\n",
      "|   or|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/21 11:28:38 WARN TextSocketMicroBatchStream: Stream closed by localhost:9999\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select(\n",
    "   explode(\n",
    "       split(lines.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "\n",
    " # Start running the query that prints the running counts to the console\n",
    "query = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6be5311-1b45-48b4-b1e6-c2fbaa135d3d",
   "metadata": {},
   "source": [
    "# What we did not cover here\n",
    "\n",
    "This was just a taste of the streaming API. \n",
    "\n",
    "New features are added from time to time, so checking the docs is always advised.\n",
    "\n",
    "Some interesting topics to follow:\n",
    "* selection, Projection\n",
    "* Handling errors (duplication, recovery ...)\n",
    "* Window operations\n",
    "* Join operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d877ec54-83cf-4a21-a8eb-e9582b22ee06",
   "metadata": {},
   "source": [
    "# Check yourself\n",
    "\n",
    "* What will happen if you run 'country_counts.show()'? Why?\n",
    "* change OFFSETS_PER_TRIGGER to 100. How does it affect the processing?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
