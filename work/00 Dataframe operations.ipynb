{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "914ed86a-9422-40f6-aa26-72858d66075d",
   "metadata": {},
   "source": [
    "# DataFrame Operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780e03c3-6d21-462a-bcbc-0ce4d2779b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import*\n",
    "\n",
    "spark = SparkSession.builder.appName('DataFrame Operations').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e1c67-1e44-4942-817b-ebc725baba57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read a CSV into a dataframe, inferring the schema.\n",
    "dataPath = \"../data/Open_Parking_and_Camera_Violations_100.csv\"\n",
    "fines = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\",\"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(dataPath)\n",
    "  \n",
    "# inferSchema means we will automatically figure out column types \n",
    "# at a cost of reading the data more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff5e9dc-9173-437c-a97e-6bb618458ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fines.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9141ed-35a5-403e-81b6-adcf2472eaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A schema is a StructType made up of a number of fields, StructFields, that have a name, type, and a Boolean flag which specifies whether that column can contain missing or null values.\n",
    "fines.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628fb79a-3c3d-4b83-98cf-8428414b8a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "fines.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4c4ba5-4b33-4188-9ebc-ff9b50fc2d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fines.select(['Plate', 'Violation']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a720e61-5da4-49d9-9266-eee5123591bd",
   "metadata": {},
   "source": [
    "Let's see how to manually specify a known schema for a data file, so we can skip the costly \"Infer Schema\":\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4127c6-c3e7-4906-ab45-4e8716b2c667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, IntegerType\n",
    "\n",
    "schema = StructType([StructField('Plate',StringType(), True),\n",
    "                     StructField('State',StringType(),True),\n",
    "                     StructField('License Type',StringType()),\n",
    "                     StructField('Summons Number',IntegerType()),\n",
    "                     StructField('Issue Date',StringType()),\n",
    "                     StructField('Violation Time',StringType()),\n",
    "                     StructField('Violation',StringType()),\n",
    "                     StructField('Judgment Entry Date',StringType()),\n",
    "                     StructField('Fine Amount',DoubleType(),True),\n",
    "                     StructField('Penalty Amount',DoubleType(),True),\n",
    "                     StructField('Interest Amount',DoubleType(),True),\n",
    "                     StructField('Reduction Amount',DoubleType(),True),\n",
    "                     StructField('Payment Amount',DoubleType(),True),\n",
    "                     StructField('Amount Due',DoubleType(),True),\n",
    "                     StructField('Precinct',IntegerType(),True),\n",
    "                     StructField('County',StringType(),True),\n",
    "                     StructField('Issuing Agency',StringType(),True),\n",
    "                     StructField('Violation Status',StringType(),True),\n",
    "                     StructField('Summons Image',StringType(),True)])\n",
    "\n",
    "\n",
    "fine2 = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\",\"true\")\\\n",
    "  .schema(schema)\\\n",
    "  .load(dataPath)\n",
    "\n",
    "fine2.printSchema()\n",
    "#fine2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581d6bb9-923d-42c7-90f7-1a69a7d7fbad",
   "metadata": {},
   "source": [
    "\n",
    "Now that we've explored the data, let's return to understanding\n",
    "**transformations** and **actions**.  \n",
    "Let's create several transformations and then an action. After that we\n",
    "will inspect exactly what's happening under the hood.\n",
    "\n",
    "These transformations are simple, first we group by two variables and then compute the average fine.\n",
    "Then we're going to inner join that to the original dataset on the column State.\n",
    "Then we'll select the Violation Status from that new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e957bb4b-c199-4866-bba2-15a77b864271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple grouping\n",
    "df1 = fines.groupBy([\"State\",\"Violation\"]).avg(\"Fine Amount\") \n",
    "\n",
    "# a simple join and selecting some columns\n",
    "df2 = df1 .join(fines, on=[\"Violation\"], how='inner').select([\"Plate\",\"Violation\",\"Violation Status\"])\n",
    "\n",
    "df1.show(10)\n",
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e815c47b-1bb4-40d5-9057-de46ba5a29f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.explain()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea6f5ea-005d-48dd-b91d-a529f237393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will execute the plan that Apache Spark built up previously. \n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df5682-69cd-495e-934d-b913dbc42972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can convert to Pandas IF the data can fit into one node\n",
    "df2.to_pandas_on_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15f5a5-0e91-4aa8-9899-c9a75d13f17e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
