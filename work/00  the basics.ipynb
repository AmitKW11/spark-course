{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1dd32e-423a-4f74-bd2e-ffff7af9d201",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 00 The basics of using Spark and Jupyter notebook\n",
    "\n",
    "\n",
    "## Definitions\n",
    "### Application\n",
    "A user program built on Spark using its APIs. It consists of a driver program and executors on the cluster.\n",
    "\n",
    "### SparkSession\n",
    "An object that provides a point of entry to interact with underlying Spark functionality and allows programming Spark with its APIs. In an interactive Spark shell, the Spark driver instantiates a SparkSession for you, while in a Spark application, you create a SparkSession object yourself.\n",
    "\n",
    "### Job\n",
    "A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g., save(), collect()).\n",
    "\n",
    "### Stage\n",
    "Each job gets divided into smaller sets of tasks called stages that *depend* on each other.\n",
    "This means that a stage is executed serially\n",
    "\n",
    "### Task\n",
    "A single unit of work or execution that will be sent to a Spark executor. Tasks can be executed in parallel\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef29270-b24e-4eac-a260-70cec0a7a56b",
   "metadata": {},
   "source": [
    "Depending on the configuration, when starting the notebook that is connected to Spark, a SparkSession and SparkContext are already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149aee98-53ee-40e7-8775-71dc1374c454",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'spark' in dir():\n",
    "    print(\"spark context is already created for you!\")\n",
    "else: print(\"You need to create your own SparkSession object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a2f340-6ebe-4b9a-b401-6081d96a41b1",
   "metadata": {},
   "source": [
    "In any case, we can ask for a spark session, and we will get the existing or a new one, *maybe* with the configuration modification we specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9ab46-b9bf-4630-b323-d992fdc65643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.random import RandomRDDs\n",
    "from pyspark.sql.types import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bdffb1-2078-4ebb-88d9-d743b8f2225b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Before getting/creating the Session, we can try to modify parameters. \n",
    "spark = SparkSession.builder.appName('00 the basics')\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "# keep only important logs\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8fe4b0-cc95-4557-8342-3b465988e2fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ONLY when running in jupyter:\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2184c3dc-7361-46c5-9fc0-7dba9cf851ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# see what version of Spark we are running.\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6597c48c-7ca4-4dcb-a8cd-8730d6ece2bf",
   "metadata": {},
   "source": [
    "You should get something like\n",
    "```\n",
    "SparkSession - in-memory\n",
    "\n",
    "SparkContext\n",
    "\n",
    "Spark UI\n",
    "\n",
    "Version           v3.2.0  << should be at least 3.2.0\n",
    "Master            local[*] << local means Spark is running on one machine, '*' means it uses all the cores in this machine\n",
    "AppName           00 the basics\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e70db73-6546-414e-a057-5083e36db0f6",
   "metadata": {},
   "source": [
    "The Spark UI is available once the session object is created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b20acd-c32b-4bb2-ac50-05f521f5ff14",
   "metadata": {},
   "source": [
    "Now open this link to see the Spark UI: \n",
    "http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64098fa8-0211-4147-b643-85cd8fb6426f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# What is Spark?\n",
    "Apache Spark is an open-source cluster computing framework.\n",
    "\n",
    "Built on top of Hadoop MapReduce.\n",
    "\n",
    "Utilizes In-memory computing.\n",
    "\n",
    "Originally developed at UC Berkeley (2009).\n",
    "\n",
    "# Basic Dataframe operations\n",
    "\n",
    "## The RDD  - Resilient Distributed Datasets\n",
    "- Spark's primary data abstraction.\n",
    "- A fault-tolerant collection of elements (any type), partitioned across the nodes of the cluster\n",
    "and capable of accepting parallel operations.\n",
    "- Sharing data across multiple stages of an iterative computation.<br>\n",
    "Efficency is accomplished in two ways:\n",
    "  - Ensures that the partitions that are assigned to each worker node are maintained between iterations to avoid shuffling data.\n",
    "  - Avoids writing and reading from HDFS in between iteration jobs by keeping the RDDs in memory, since the assignment to workers is maintained from one iteration to the next, this is feasible.\n",
    "- Immutable.\n",
    "- RDD Operations: **Transformations & Actions**\n",
    "\n",
    "### Transformations\n",
    "\n",
    "Transformations are operations that will not be completed at the time\n",
    "you write and execute the code in a cell - they will only get executed\n",
    "once you have called a action. An example of a transformation might be\n",
    "to convert an integer into a float or to filter a set of values.\n",
    "\n",
    "### Actions\n",
    "\n",
    "Actions are commands that are computed by Spark right at the time of\n",
    "their execution. They consist of running all of the previous\n",
    "transformations in order to get back an actual result. An action is\n",
    "composed of one or more jobs which consists of tasks that will be\n",
    "executed by the workers in parallel where possible.\n",
    "\n",
    "\n",
    "Here are some simple examples of transformations and actions. Remember,\n",
    "these are not all the transformations and actions - this is just a short\n",
    "sample of them. We'll get to why Apache Spark is designed this way\n",
    "shortly!\n",
    "\n",
    "| Transformations(*lazy*) | Actions |\n",
    "|-------------------------|---------|\n",
    "| select                  | show    |\n",
    "| distinct                | count   |\n",
    "| groupBy                 | collect |\n",
    "| sum                     | save    |\n",
    "| orderBy                 |         |\n",
    "| filter                  |         |\n",
    "| limit                   |         |\n",
    "\n",
    "\n",
    "## Directed Acyclic Graph (DAG)\n",
    "* Vertices are RDD, edges are Transformations\n",
    "\n",
    "* Generalization of MapReduce\n",
    "\n",
    "* Action divides DAG to Stages\n",
    "\n",
    "* This model lets Spark decide which calculations should be recomputed and which can be reused (shown as \"skipped\" in the user interface)\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/yQf7L.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd985f1f-2855-4155-aa4d-801d4acbbe1b",
   "metadata": {},
   "source": [
    "## Spark​ Structured​ API\n",
    "The Structured APIs are a tool for manipulating all sorts of data, from unstructured log files to semi-structured CSV files and highly structured Parquet files. These APIs refer to three core types of distributed collection APIs:\n",
    "\n",
    "* Datasets (Java and Scala API only).\n",
    "* DataFrames.\n",
    "* SQL tables and views\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c626471-9829-45db-9027-7f3c2ad70542",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Dataframe\n",
    "\n",
    "A DataFrame is the most common Structured API and simply represents a table of data with\n",
    "rows and columns. The list that defines the columns and the types within those columns is called\n",
    "the schema. You can think of a DataFrame as a spreadsheet with named columns.\n",
    "\n",
    "Each column has a name and a type (e.g. StringType) .\n",
    "\n",
    "Like RDD, the DataFrame is \n",
    "Immutable, in-memory, resilient, distributed collection of data.\n",
    "\n",
    "It allows better optimizations (memory management and optimized execution plan), and was added to spark in version 2.\n",
    "\n",
    "\n",
    "see [Data types](https://spark.apache.org/docs/latest/sql-ref-datatypes.html)\n",
    "\n",
    "\n",
    "### DataFrame vs. RDD: which is better?\n",
    "Usually, DataFrame. Consider RDD if:\n",
    "\n",
    "* Unstructured data (text, media).\n",
    "* Specific execution control needed.\n",
    "* Data manipulation with functional programming concepts.\n",
    "\n",
    "In practice Spark DF rides on RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc2cc7-5f37-48bb-8f78-c684f98e8cd9",
   "metadata": {},
   "source": [
    "## Working with RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac82e43-c645-4ba7-9db5-200359d19bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parallelize() will copy the python object to the JVM, then cut it into partitions according to some rule, \n",
    "# and then send the partitions to the worker nodes for processing.\n",
    "nums = sc.parallelize([1, 2, 3, 4, 555])\n",
    "print('Type:', type(nums))\n",
    "print('Count:', nums.count())\n",
    "\n",
    "# each node runs the map() on the partitions it has.\n",
    "# The collect() collects all the results (partitions of the result RDD) to the driver node,\n",
    "# Then copies the data from the JVM to the python process.\n",
    "print('Squared:', nums.map(lambda x: x**2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad395698-3519-4707-9b5c-bef9f9bd7d48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(f\"SparkContext default Number of partitions: {sc.defaultParallelism}\")\n",
    "print(f\"Number of CPUs in the system: {os.cpu_count()}\")\n",
    "print(f\"Number of partitions in nums RDD: {nums.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc312850-3ad2-4f88-bdd5-93ea90bce425",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Generate a random double RDD that contains 1 million i.i.d. values drawn from the\n",
    "# standard normal distribution `N(0, 1)`, evenly distributed in 10 partitions.\n",
    "u = RandomRDDs.normalRDD(spark, 1000000, 10)\n",
    "u = u.map(lambda x: (x,)) # convert to tuple so we can transorm into DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b99cfd-c0a7-4325-b328-1ca1c07d74b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Working with Dataframe\n",
    "\n",
    "The RDD is the basic building block, and usually we will want to use a higher level object: The Dataframe wraps the RDD and exposes a convenient API.\n",
    "\n",
    "DataFrame always has a schema\n",
    "\n",
    "### The Schema\n",
    " A schema is a StructType made up of a number of fields, StructFields, that have a name, type, and a Boolean flag which specifies whether that column can contain missing or null values.\n",
    " \n",
    "When reading data from a file, the schema can be inferred automatically at a cost of reading the data more than once.\n",
    "\n",
    "### The Row\n",
    "\n",
    "In Spark, each row in a DataFrame is a single record. Spark represents\n",
    "this record as an object of type Row. Spark manipulates Row objects\n",
    "using column expressions in order to produce usable values. Row objects\n",
    "internally represent arrays of bytes. The byte array interface is never\n",
    "shown to users because we only use column expressions to manipulate\n",
    "them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c81c887-697a-4dfa-b8e8-9a9f7755d85f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = StructType([  StructField('c1', FloatType(), False)])\n",
    "# we can move from RDD to Dataframe and back. \n",
    "df = spark.createDataFrame(u, schema) #something is wrong here with the schema\n",
    "\n",
    "# each DF has a schema:\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca4883-f2ca-4d77-9c7e-968d679c73ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tip: To see the table in nicer format, convert it to Pandas:\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ff91d1-7f28-459a-82d6-cd6069f35ff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the RDD from the Dataframe\n",
    "r = df.rdd\n",
    "type(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d71802-76f5-4b19-9c58-d13e3daff399",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a simple dataframe\n",
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "rdd = spark.sparkContext.parallelize(dept)\n",
    "\n",
    "df = rdd.toDF()\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7de19af-3e3d-4ff0-9f2a-d855e2fc5081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Transformation:\n",
    "# create an array of M numbers\n",
    "# This is fast since it is a TRANSFORMATION. \n",
    "# It is just an execution plan, so if allocating M numbers\n",
    "# will use all the memory on this machine, we will not see it now.\n",
    "M = 100*1000 *1000\n",
    "myRange = spark.range(M).toDF(\"number\")\n",
    "nums_doubled_df = myRange.selectExpr(\"(number * 2) as value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e2805-3167-4d0c-9000-2985f646bfd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Actions:\n",
    "# Collect the dataframe from all worker nodes (the executors) to the driver program.\n",
    "# if this is too large a \"Java heap space exception\" will happen, and then you have to restart your kernel.\n",
    "# Since we use Pyspark, this data is then *copied* from the JVM to the python runtime.\n",
    "\n",
    "the_big_list = myRange.collect()\n",
    "print(nums_doubled_df.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c7e27-14a6-44da-87e5-687bb71f5d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(the_big_list), len(the_big_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a817d09-771e-406f-b4df-6837e9c87b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "divisBy2 = myRange.where(\"number % 2 = 0\")\n",
    "print(\"Count: \",divisBy2.count())\n",
    "divisBy2.sort('number').show(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f824e6f-daeb-4b64-8dda-1ba0c5111646",
   "metadata": {},
   "source": [
    "## Example: text processing using RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dc2172-e9e9-4b0b-b4d1-c6e3b4ea0a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_rdd = sc.textFile('../data/toxic.txt')\n",
    "print('Type:', type(toxic_rdd))\n",
    "print('Count (rows):', toxic_rdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa2a0c-3cc7-4f51-a4cd-59cd489f9690",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef9499c-ff0b-4dcc-9c91-b83e3fed4205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split rows to words:\n",
    "toxic_words = toxic_rdd.flatMap(lambda row: row.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004f161-7c77-4af1-9820-c443cf733d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 most frequent words:\n",
    "toxic_words.map(lambda word: (word.casefold(), 1)) \\\n",
    "        .reduceByKey(lambda a, b: a + b) \\\n",
    "            .sortBy(lambda t: t[1], ascending=False) \\\n",
    "                .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f59c0e-88e4-4a7d-a5b1-5bffeed43d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting 'baby':\n",
    "toxic_words.filter(lambda word: word.lower() == 'baby').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cb44d7-773d-447a-8ece-025d4034d8e0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Check yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a21f592-c932-4304-9d2d-3780185ac92b",
   "metadata": {},
   "source": [
    "Try to increase M (in the range() above ) by 1000, and run the code again. What do you expect? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
