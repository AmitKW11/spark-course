{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44ab91d4-60f9-4b79-b338-e0c2b363aef4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using the Machine Learning library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32141e84-fff5-44b4-9b6e-7885a8f23af2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Spark provides a library with ML functionality. The set of tools is ever expanding -- see the latest at https://spark.apache.org/docs/latest/ml-guide.html\n",
    "\n",
    "The library is implemented in Scala, and has python binding (i.e. calling from python to the API).\n",
    "\n",
    "\n",
    "Using MLFlow ( https://mlflow.org/docs/latest/python_api/mlflow.spark.html?highlight=spark#module-mlflow.spark )is also possible, but not covered here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29449b8e-51c2-4d09-920b-4e5e54eb6efd",
   "metadata": {},
   "source": [
    "**Check the notebook at \"sdg/Advanced_Analytics_and_Machine_Learning-Chapter_25_Preprocessing_and_Feature_Engineering\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0239539-8e23-4526-a01a-6c740ce015a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "If a specific tool is not part of MLlib, maybe someone already implemented it.\n",
    "\n",
    "Always be suspicious of the source: who wrote it? when was the last update? how many stars?\n",
    "\n",
    "See for example https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22 which is a repo without any quality assurance. You can find a great code, a buggy code, or malware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28feace4-9910-4b0c-b691-79daf4a55834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession.builder.appName('MLlib').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ac6fc-20ff-4f4d-9b04-0b7a47fb6cec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(file_name_glob):\n",
    "    \"\"\" load the contents of the input files.\n",
    "        If we already saved them in Parquet file, use it.\n",
    "        >>> load_data('../data/sdg/retail-data/by-day/2010-12*.csv')\n",
    "        :param file_name_glob wildcard value of the files to read. e.g. \"/mnt/dir/data*\"\n",
    "        :return: DataFrame containing all the data\n",
    "    \"\"\"\n",
    "    \n",
    "    def cache_file_name(file_name):\n",
    "        t = file_name.replace('*',\"_\").replace('?',\"_\")\n",
    "        return t[: t.rfind('.')] + \".parquet\"\n",
    "    \n",
    "    import os\n",
    "    dirname = os.path.dirname(file_name_glob)\n",
    "    p = Path(dirname)\n",
    "    fname = Path(file_name_glob)\n",
    "    basename = fname.name\n",
    "    cache_name = cache_file_name(file_name_glob)\n",
    "    if Path(cache_name).exists():\n",
    "        print(f\"reading {cache_name} from cache Parquet file\")\n",
    "        return spark.read.parquet(cache_name)\n",
    "    \n",
    "    #suffix = fname.suffix\n",
    "    if not p.exists():\n",
    "        raise ValueError('Path not found')\n",
    "    file_list = list(p.glob(basename))\n",
    "    x = [ str(f.resolve()) for f in file_list]\n",
    "    df = spark.read \\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(x)\n",
    "    \n",
    "    df.write.parquet(cache_name)\n",
    "    return df\n",
    "    \n",
    "\n",
    "df = load_data('../data/sdg/retail-data/by-day/2011-*.csv')\n",
    "print(f\"df.count = {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff27a7-179e-460a-ad4d-6b24f015db0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fa988b-f672-4935-8d66-1dd8cbf99ece",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using Mlib \n",
    "\n",
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f887b51-4422-4660-80fc-650fb8c69b0b",
   "metadata": {},
   "source": [
    "Add a new column: \"day of week\" and split to train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f7ec9-c948-43d0-99dc-46b693e60f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "preppedDataFrame = df\\\n",
    "  .na.fill(0)\\\n",
    "  .withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\"))\n",
    "  #.coalesce(5)\n",
    "\n",
    "# split to train and test:\n",
    "trainDataFrame,testDataFrame  = preppedDataFrame.randomSplit([0.7, 0.3])\n",
    "\n",
    "# we could also split using other criteria:\n",
    "# trainDataFrame = preppedDataFrame\\\n",
    "#   .where(\"InvoiceDate < '2011-07-01'\")\n",
    "# testDataFrame = preppedDataFrame\\\n",
    "#   .where(\"InvoiceDate >= '2011-07-01'\")\n",
    "\n",
    "print(f\"train:test ratio: {trainDataFrame.count()/testDataFrame.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888e3741-a059-49dc-a3af-80be1788457c",
   "metadata": {},
   "source": [
    "convert day of week \"Mon\" -> 2 -> one hot encoding\n",
    "\n",
    "**What is the downside of using one hot encoding?**\n",
    "- wasted space -> solved by using sparse vectors\n",
    "- increased dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab6915-328a-47db-b3d5-f2dfa73f5b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "day_indexer = StringIndexer()\\\n",
    "  .setInputCol(\"day_of_week\")\\\n",
    "  .setOutputCol(\"day_of_week_index\")\n",
    "\n",
    "country_indexer = StringIndexer()\\\n",
    "  .setInputCol(\"Country\")\\\n",
    "  .setOutputCol(\"country_index\")\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "encoder = OneHotEncoder()\\\n",
    "  .setInputCol(\"day_of_week_index\")\\\n",
    "  .setOutputCol(\"day_of_week_encoded\")\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#  add \"features\" column that contains the input columns as elements in a vector.\n",
    "# Not very exciting, right?\n",
    "vectorAssembler = VectorAssembler()\\\n",
    "  .setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "# Read about pipelines here: https://spark.apache.org/docs/latest/ml-pipeline.html\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transformationPipeline = Pipeline()\\\n",
    "  .setStages([day_indexer, country_indexer, encoder, vectorAssembler])\n",
    "\n",
    "fittedPipeline = transformationPipeline.fit(trainDataFrame)\n",
    "transformedTraining = fittedPipeline.transform(trainDataFrame)\n",
    "tranformedTest = fittedPipeline.transform(testDataFrame)\n",
    "\n",
    "# Let's drop unused columns. \n",
    "# This reduces the amount of needed memory so improving performance.\n",
    "transformedTraining = transformedTraining.drop('day_of_week').drop('day_of_week_encoded').drop('day_of_week_index'). drop('CustomerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02ac748-737d-4360-8c4e-db3c285679e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Caching the transfored DF will save a lot of time when reusing it (e.g. for hyper param tuning)\n",
    "transformedTraining.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d53770-3a86-4532-b450-c6f1b5fe7e5b",
   "metadata": {},
   "source": [
    "In Spark, training machine learning models is a two phase process. First we initialize an untrained model, then we\n",
    "train it. There are always two types for every algorithm in MLlibâ€™s DataFrame API. The algorithm Kmeans and then the\n",
    "trained version which is a KMeansModel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6f1c5f-16cc-40eb-a9e8-ca701c88b7a2",
   "metadata": {},
   "source": [
    "## K means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd35788-5d6a-4a5d-ace7-988c8f2e68e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans = KMeans()\\\n",
    "  .setK(6)\\\n",
    "  .setSeed(1)\n",
    "\n",
    "kmModel = kmeans.fit(transformedTraining)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "638788da-ef32-4d3e-9d45-d93e52224dbf",
   "metadata": {},
   "source": [
    "TODO:continue here.\n",
    "consult the book, and only then look at the student's submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da489d-56dd-4354-92b3-c520eb26dc28",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Supervised learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a084bc-e420-45c0-9e7d-0bc57ee19072",
   "metadata": {},
   "source": [
    "## Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04038c37-c07e-43a4-8489-fec9e0fe45ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTraining.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3029b7ab-d114-451c-b476-84b74dadd393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"country_index\",featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3dec15-baa7-4d7b-9cc9-dba8408d351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see a list of all hyperparams of LogisticRegression\n",
    "# print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e72d953-690e-4946-a84b-10dc02c8c983",
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedLR = lr.fit(transformedTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ba721-71f6-4c3b-8cce-a6c0799c55f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fittedLR.transform(tranformedTest).select(\"country_index\", \"prediction\").\\\n",
    "groupBy(\"country_index\").avg(\"prediction\").show(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f895beb3-66c9-43cc-b4dc-3c8226cb17fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedLR.transform(tranformedTest).select(\"country_index\", \"prediction\")\\\n",
    ".filter((col('country_index') == col('prediction')) & (col('prediction') != 0)).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eba0864-361d-4f18-bcd8-66b8d359c1a7",
   "metadata": {},
   "source": [
    "## Tuning the hyper-params\n",
    "\n",
    "TODO: reminder why this tuning is needed, and when is it enough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ebb48-5325-47bb-9019-644e6bfde75d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
