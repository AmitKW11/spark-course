{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44ab91d4-60f9-4b79-b338-e0c2b363aef4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Machine Learning Via Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c1a92a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "In this notebook, we will explore how Spark can assist us in machine learning applications and provide us with additional tools to address ML challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee85197",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Using the Machine Learning library (MLlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f3398",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Spark provides a library with ML functionality. The set of tools is ever expanding - see the latest at https://spark.apache.org/docs/latest/ml-guide.html\n",
    "\n",
    "The library is implemented in Scala, and has python binding (i.e. calling from python to the API).\n",
    "\n",
    "\n",
    "Using MLFlow (https://mlflow.org/docs/latest/python_api/mlflow.spark.html?highlight=spark#module-mlflow.spark)is also possible, but not covered here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ee2335",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**For more information and explanations, check chapters 24 & 25 in** : Chambers, B., & Zaharia, M. (2018). \"Spark: The definitive guide: Big data processing made simple\" [SDG].\n",
    "\n",
    "SDG serves as a comprehensive guide to Apache Spark, a powerful open-source distributed computing system used for big data processing and analytics.\n",
    "\n",
    "You can find this book here:\n",
    "https://github.com/VolodymyrGavrysh/My_RoadMap_Data_Science/blob/master/kds/books/Spark-The%20Definitive%20Guide.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bef1b6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0239539-8e23-4526-a01a-6c740ce015a3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "If a specific tool is not part of MLlib, maybe someone already implemented it.\n",
    "\n",
    "Always be suspicious of the code source: who wrote it? when was the last update? how many stars?\n",
    "\n",
    "See for example https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22 which is a repo without any quality assurance. You can find a great code, a buggy code, or malware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c4b962",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We deal with a classic data-set example of retail-data, capturing transactions and sales activities. Each record represents a unique transaction within a retail environment during the year 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28feace4-9910-4b0c-b691-79daf4a55834",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('MLlib').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ac6fc-20ff-4f4d-9b04-0b7a47fb6cec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(file_name_glob):\n",
    "    \"\"\" load the contents of the input files.\n",
    "        If we already saved them in Parquet file, use it.\n",
    "        #  >>> load_data('../data/sdg/retail-data/by-day/2010-12*.csv')\n",
    "        :param file_name_glob wildcard value of the files to read. e.g. \"/mnt/dir/data*\"\n",
    "        :return: DataFrame containing all the data\n",
    "    \"\"\"\n",
    "    \n",
    "    def cache_file_name(file_name):\n",
    "        t = file_name.replace('*',\"_\").replace('?',\"_\")\n",
    "        return t[: t.rfind('.')] + \".parquet\"\n",
    "    \n",
    "    import os\n",
    "    dirname = os.path.dirname(file_name_glob)\n",
    "    p = Path(dirname)\n",
    "    fname = Path(file_name_glob)\n",
    "    basename = fname.name\n",
    "    cache_name = cache_file_name(file_name_glob)\n",
    "    if Path(cache_name).exists():\n",
    "        return spark.read.parquet(cache_name)\n",
    "    \n",
    "    #suffix = fname.suffix\n",
    "    if not p.exists():\n",
    "        raise ValueError('Path not found')\n",
    "    file_list = list(p.glob(basename))\n",
    "    x = [ str(f.resolve()) for f in file_list]\n",
    "    df = spark.read \\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(x)\n",
    "\n",
    "    # df.write.parquet(cache_name) - for optimization\n",
    "    return df\n",
    "    \n",
    "\n",
    "df = load_data('../data/sdg/retail-data/by-day/2011-*.csv')\n",
    "print(f\"df.count = {df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d0f409",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff27a7-179e-460a-ad4d-6b24f015db0c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f887b51-4422-4660-80fc-650fb8c69b0b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Add a new column: \"day of week\" and split to train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f7ec9-c948-43d0-99dc-46b693e60f05",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "preppedDataFrame = df\\\n",
    "  .na.fill(0)\\\n",
    "  .withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\"))\n",
    "  #.coalesce(5)\n",
    "\n",
    "# split to train and test:\n",
    "trainDataFrame,testDataFrame  = preppedDataFrame.randomSplit([0.7, 0.3])\n",
    "\n",
    "# we could also split using other criteria:\n",
    "# trainDataFrame = preppedDataFrame\\\n",
    "#   .where(\"InvoiceDate < '2011-07-01'\")\n",
    "# testDataFrame = preppedDataFrame\\\n",
    "#   .where(\"InvoiceDate >= '2011-07-01'\")\n",
    "\n",
    "print(f\"train:test ratio: {trainDataFrame.count()/testDataFrame.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888e3741-a059-49dc-a3af-80be1788457c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Transformations for the data\n",
    "Convert \"day of week\" into one hot vector. E.g \"Mon\" -> 2 -> one hot encoding\n",
    "Convert \"Country\" -> \"country_index\"\n",
    "Convert \"UnitPrice\", \"Quantity\", \"day_of_week_encoded\" columns into a single feature vector -> \"features\"\n",
    "\n",
    "**What is the downside of using one hot encoding?**\n",
    "- wasted space -> solved by using sparse vectors\n",
    "- increased dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab6915-328a-47db-b3d5-f2dfa73f5b4d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "day_indexer = StringIndexer()\\\n",
    "  .setInputCol(\"day_of_week\")\\\n",
    "  .setOutputCol(\"day_of_week_index\")\n",
    "\n",
    "country_indexer = StringIndexer()\\\n",
    "  .setInputCol(\"Country\")\\\n",
    "  .setOutputCol(\"country_index\")\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "encoder = OneHotEncoder()\\\n",
    "  .setInputCol(\"day_of_week_index\")\\\n",
    "  .setOutputCol(\"day_of_week_encoded\")\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#  add \"features\" column that contains the input columns as elements in a vector.\n",
    "# Not very exciting, right?\n",
    "vectorAssembler = VectorAssembler()\\\n",
    "  .setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "# Read about pipelines here: https://spark.apache.org/docs/latest/ml-pipeline.html\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transformationPipeline = Pipeline()\\\n",
    "  .setStages([day_indexer, country_indexer, encoder, vectorAssembler])\n",
    "\n",
    "fittedPipeline = transformationPipeline.fit(trainDataFrame)\n",
    "transformedTraining = fittedPipeline.transform(trainDataFrame)\n",
    "tranformedTest = fittedPipeline.transform(testDataFrame)\n",
    "\n",
    "# Let's drop unused columns. \n",
    "# This reduces the amount of needed memory so improving performance.\n",
    "transformedTraining = transformedTraining.drop('day_of_week').drop('day_of_week_encoded').drop('day_of_week_index'). drop('CustomerID')\n",
    "\n",
    "# Apply the fitted pipeline to the test data\n",
    "transformedTest = fittedPipeline.transform(testDataFrame)\n",
    "\n",
    "# Drop unused columns from transformedTest\n",
    "transformedTest = transformedTest.drop('day_of_week').drop('day_of_week_encoded').drop('day_of_week_index').drop('CustomerID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc89223",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The column \"features\" in the data frames was created through feature engineering technique - vectorization using Spark's VectorAssembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02ac748-737d-4360-8c4e-db3c285679e0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Caching the transfored DF will save a lot of time when reusing it (e.g. for hyper param tuning)\n",
    "transformedTraining.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e6a814",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transformedTraining.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d4f0fb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Classification using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131784cc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "**Reminder**: Logistic regression is a fundamental statistical method used for binary classification tasks.\n",
    "\n",
    "Let's examine the performance of the logistic regression model by predicting the country indices, with only the features in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b826f1a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model\n",
    "lr = LogisticRegression(labelCol=\"country_index\", featuresCol=\"features\")\n",
    "\n",
    "# Fit the logistic regression model to the training data\n",
    "fittedLR = lr.fit(transformedTraining)\n",
    "\n",
    "# Make predictions on the test data and select relevant columns\n",
    "predictions = fittedLR.transform(tranformedTest).select(\"country_index\", \"prediction\")\n",
    "\n",
    "# Group predictions by country index and calculate average prediction\n",
    "grouped_predictions = predictions.groupBy(\"country_index\").avg(\"prediction\")\n",
    "\n",
    "# Show the results for the top 20 country indices\n",
    "grouped_predictions.show(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6501dd1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can evaluate the model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7ed3be",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"country_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6f1c5f-16cc-40eb-a9e8-ca701c88b7a2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Clustering using K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64698c29",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "**Reminder**: K-means is an unsupervised machine learning algorithm used for clustering data into distinct groups, by iteratively assigning data points to the nearest cluster centroid, and updating the centroids to minimize the within-cluster sum of squares.\n",
    "\n",
    "In Spark, training machine learning models is a two phase process. First we initialize an untrained model, then we\n",
    "train it. There are always two types for every algorithm in MLlib’s DataFrame API. The algorithm Kmeans and then the\n",
    "trained version which is a KMeansModel.\n",
    "\n",
    "The column \"features\" contains the transformed features that will be used by the KMeans algorithm for clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b328d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Define the KMeans model with k=6 clusters and a seed value for reproducibility\n",
    "kmeans = KMeans().setK(6).setSeed(1)\n",
    "\n",
    "# Fit the KMeans model to the transformed training data\n",
    "kmModel = kmeans.fit(transformedTraining)\n",
    "\n",
    "# Make predictions on the transformed test data\n",
    "predictions = kmModel.transform(transformedTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893d41b2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When we call the fit() method with transformedTraining, KMeans utilizes the \"features\" column to cluster the data points based on the features contained within it. The algorithm finds the optimal cluster centroids based on these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2adb8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Shows the result.\n",
    "centers = kmModel.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for i, center in enumerate(centers):\n",
    "    print(f\"{i+1}: {center}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948eb588",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's evaluate the model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b90f72c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "# Compute the Silhouette score\n",
    "silhouette_score = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette Score:\", silhouette_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d694710",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Repeating the Runs\n",
    "\n",
    "In the context of machine learning models like K-means clustering, ensuring repeatability and reproducibility of results is crucial for reliability and trustworthiness of the model.\n",
    "\n",
    "**Repeatability** refers to the ability to obtain the same results by running the model multiple times with the same initial conditions, including the random seed. It is important for ensuring consistency in model performance and behavior.\n",
    "\n",
    "**Reproducibility**, on the other hand, is the ability to obtain similar results even when different random numbers are used. In other words, if the model is run with different random seeds, but under the same conditions, one should still expect to get comparable outcomes. Reproducibility is essential for confirming the robustness of the model and its generalizability across different random initializations.\n",
    "\n",
    "In the context of K-means clustering, where initial centroids are initialized randomly, ensuring both repeatability and reproducibility is vital. While setting a random seed (e.g., using `setSeed()` function) helps achieve repeatability, validating the model's robustness against different initializations is equally important for assessing its reproducibility.\n",
    "\n",
    "By repeating runs with varying seeds and observing consistency or variance in results, we can gain insights into the stability and reliability of the K-means clustering algorithm. This process not only validates the model but also helps identify potential biases or sensitivities to initial conditions, contributing to the overall trustworthiness of the clustering results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f3ae9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define a list of seed values for testing repeatability and reproducibility\n",
    "seed_values = [1, 42, 123]\n",
    "\n",
    "for seed in seed_values:\n",
    "    print(f\"Running with seed: {seed}\")\n",
    "    # Define the KMeans model with k=6 clusters and a seed value for reproducibility\n",
    "    kmeans = KMeans().setK(6).setSeed(seed)\n",
    "\n",
    "    # Fit the KMeans model to the transformed training data\n",
    "    kmModel = kmeans.fit(transformedTraining)\n",
    "\n",
    "    # Make predictions on the transformed test data\n",
    "    predictions = kmModel.transform(transformedTest)\n",
    "\n",
    "    # Evaluate clustering by computing Silhouette score\n",
    "    evaluator = ClusteringEvaluator()\n",
    "\n",
    "    # Compute the Silhouette score\n",
    "    silhouette_score = evaluator.evaluate(predictions)\n",
    "    print(\"Silhouette Score:\", silhouette_score)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd4681b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The consistent high Silhouette scores obtained across runs with different seeds provide evidence for both repeatability and reproducibility of the KMeans clustering model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc67f2d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To explore further opportunities in classifications & regressions, please look at:\n",
    "https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a7446",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Selection and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5edb61",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Reminder**: Hyperparameters are settings that govern the learning process and affect the model's behavior and performance.\n",
    "\n",
    "Hyperparameter tuning is essential because it helps optimize machine learning models' performance by finding the most effective combination of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cb85e8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Using grid (cartesian product) is inefficient due to a lot of wasted runs. There are more advanced methods to reduce the tested parameters combinations.\n",
    "\n",
    "**ParamGridBuilder**: The ParamGridBuilder is used to define a grid of hyperparameters for the model. In this example, we define a grid for the maximum number of iterations (maxIter) and regularization parameter (regParam) for the logistic regression model.\n",
    "\n",
    "**CrossValidator**: CrossValidator is used to perform model selection and hyperparameter tuning. It takes an estimator (in this case, logistic regression), a set of hyperparameters, an evaluator, and the number of folds for cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df703809",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Exploring a Grid Search Technique to Construct a Logistic Regression Model and Investigate Diverse Hyperparameter Configurations Methodically:\n",
    "\n",
    "*Note*: Beware of resource shortage! the following code will run 4 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7844887",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator , MulticlassClassificationEvaluator\n",
    "\n",
    "# Define the Logistic Regression model\n",
    "lr = LogisticRegression(labelCol=\"country_index\", featuresCol=\"features\")\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.maxIter, [50, 150]) \\\n",
    "    .addGrid(lr.regParam, [0.05, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"country_index\", metricName=\"accuracy\")\n",
    "\n",
    "# Define the cross-validator\n",
    "cross_validator = CrossValidator(estimator=lr,\n",
    "                                 estimatorParamMaps=param_grid,\n",
    "                                 evaluator=evaluator,\n",
    "                                 numFolds=3)\n",
    "\n",
    "# Fit the model using cross-validation\n",
    "cv_model = cross_validator.fit(transformedTraining)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = cv_model.transform(tranformedTest)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed57ea7a-aa9f-4593-b73b-a48be141c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(cv_model.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195039ef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To explore further opportunities in tuning, please look at:\n",
    "https://spark.apache.org/docs/latest/ml-tuning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7abf5c9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dimensionality Reduction using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876c95a4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Reminder**: Principal Component Analysis (PCA) is a widely used dimensionality reduction technique that aims to capture the most important information in a dataset by transforming the original features into a new set of orthogonal variables called principal components.\n",
    "\n",
    "Let's perform PCA on the \"features\" column in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e1a9c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# Assuming you have assembled features into a vector column named \"features\"\n",
    "# You may need to adjust the inputCol based on your actual data\n",
    "assembled_features = transformedTraining.select(\"features\")\n",
    "\n",
    "# Define the PCA model with desired number of principal components\n",
    "num_principal_components = 3  # Don't use 2 components automatically. We live in a 3D world.\n",
    "pca = PCA(k=num_principal_components, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "\n",
    "# Fit the PCA model to the data\n",
    "pca_model = pca.fit(assembled_features)\n",
    "\n",
    "# Transform the data to the lower-dimensional space\n",
    "transformed_data = pca_model.transform(assembled_features)\n",
    "\n",
    "# Show the transformed data with PCA features\n",
    "transformed_data.select(\"pca_features\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820da736",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135009f8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming transformed_data is your DataFrame with PCA features\n",
    "pca_features = transformed_data.select(\"pca_features\").collect()\n",
    "\n",
    "# Extracting PCA feature values\n",
    "pca_values = [row.pca_features.toArray() for row in pca_features]\n",
    "\n",
    "# Creating a 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Extracting PCA components\n",
    "x = [val[0] for val in pca_values]\n",
    "y = [val[1] for val in pca_values]\n",
    "z = [val[2] for val in pca_values]\n",
    "\n",
    "# Plotting data points\n",
    "ax.scatter(x, y, z)\n",
    "\n",
    "# Setting labels\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7422d3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To explore further opportunities in dimensionality reduction, please look at:\n",
    "https://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f28f0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Basic Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1863111",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's demonstrate how to perform correlation analysis.\n",
    "\n",
    "**Reminder**: Correlation analysis is a statistical technique used to quantify the degree to which two or more variables are related to each other. It helps identify patterns, relationships, and dependencies between different variables in a dataset. Correlation analysis is essential for understanding the interdependencies between features and for feature selection in machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90279ca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# Compute the correlation matrix for the \"features\" column\n",
    "corr_matrix = Correlation.corr(transformedTraining, \"features\").head()\n",
    "\n",
    "# Extract the correlation matrix as a DenseMatrix\n",
    "corr_matrix_values = corr_matrix[0].toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747e2cae",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a heatmap using Seaborn\n",
    "plt.figure(figsize=(7, 7))\n",
    "sns.heatmap(corr_matrix_values, annot=True, cmap=\"coolwarm\", fmt=\".2f\", square=True,\n",
    "            xticklabels=transformedTraining.columns, yticklabels=transformedTraining.columns)\n",
    "\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d3e2de",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To explore further opportunities in statistics, please look at:\n",
    " https://spark.apache.org/docs/latest/ml-statistics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013a2451",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Do not forget to release the resources held by Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
