{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98c79d3a-ba9b-4907-b9b7-52c81c017a1e",
   "metadata": {},
   "source": [
    "# User Defined Functions (UDF)\n",
    "\n",
    "What if the transformation we need is not supplied by Spark?\n",
    "\n",
    "We can add our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aacaa6a-64fe-4b3d-b266-80ad9018d304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/19 15:40:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ef1c4f-b2bd-4cd8-8d5d-c3e159821c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(col):\n",
    "    if col % 2:\n",
    "        return col/2\n",
    "    return col * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "454b2c8a-39a2-4fe6-8c24-c896b4fedecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_func = f.udf(func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30175842-dc01-41cd-b824-ce6a5ef879cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(500)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ea8132d-eaf0-4245-bb9c-0bf52cb68d94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|computed|\n",
      "+---+--------+\n",
      "|  0|       0|\n",
      "|  1|     0.5|\n",
      "|  2|      -2|\n",
      "|  3|     1.5|\n",
      "|  4|      -4|\n",
      "|  5|     2.5|\n",
      "+---+--------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Add a new column, computed from the \"id\" column\n",
    "df2 = df.withColumn('computed', sp_func('id'))\n",
    "df2.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44aa133-9a34-45e9-8702-6cf612d83267",
   "metadata": {},
   "source": [
    "## what about performance?\n",
    "\n",
    "Summary: If you can use use regular Spark functions, use them. Use UDF only if no choice.\n",
    "\n",
    "See :\n",
    "\n",
    "https://medium.com/quantumblack/spark-udf-deep-insights-in-performance-f0a95a4d8c62#:~:text=In%20these%20circumstances%2C%20PySpark%20UDF,two%20types%20of%20PySpark%20UDFs.\n",
    "\n",
    "https://stackoverflow.com/questions/38296609/spark-functions-vs-udf-performance\n",
    "\n",
    "https://www.databricks.com/session_eu20/optimizing-apache-spark-udfs\n",
    "\n",
    "\n",
    "\n",
    "When calling a UDF (either scala or python), the data has to be serialized (from the internal representation in the JVM), deserialized to pass to the function and then back.\n",
    "Even strings might be serialized due to change in representation (utf-8 / utf-16)\n",
    "\n",
    "With Python, there is another stage: the data is copied from the JVM process to the python process (in the same executor)\n",
    "[ NOTE: possibly even worse: If only the driver has python process, then all the data will be sent from the executors in the workers to the driver node, and then back ]\n",
    "\n",
    "Some [?] python UDF functions are called on a vector (instead of a single row), so the performance is much better. In any case, it will be worse (by as much as 10x) then the scala UDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83d1370b-4bd8-456c-b0f2-b0278d4a4742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|(id * 2)|\n",
      "+--------+\n",
      "|       0|\n",
      "|       2|\n",
      "|       4|\n",
      "|       6|\n",
      "|       8|\n",
      "|      10|\n",
      "+--------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(f.expr('id*2')).show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd72314-75b3-4120-ac59-ac048b11006b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
