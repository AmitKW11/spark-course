{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd8ad32",
   "metadata": {},
   "source": [
    "\n",
    "# Learning Spark: Broadcast, Persist, and Data Distribution\n",
    "\n",
    "## Introduction\n",
    "This notebook focuses on an advanced view of Broadcast Variables, Persisting Data, and Data Distribution, with detailed comparisons to Partitioning. It complements a previous notebook that covers the fundamentals of Broadcast in Spark.\n",
    "---\n",
    "By Meital Abadi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4b9dee",
   "metadata": {},
   "source": [
    "# **PART 1:**\n",
    "## 1. Understanding Broadcast Variables in Spark\n",
    "\n",
    "### What are Broadcast Variables?\n",
    "- **Read-Only Sharing**: Broadcast variables are read-only once distributed.\n",
    "- **Efficient Distribution**: Data is sent to each node only once.\n",
    "- **Local Caching**: Each node caches the broadcast data for faster access.\n",
    "\n",
    "### Why use Broadcast?\n",
    "- **Avoids redundant data transmission**: Instead of sending the same data multiple times, it is broadcasted once.\n",
    "- **Optimizes performance**: Spark ensures that each worker node can access the broadcasted data without excessive network traffic.\n",
    "\n",
    "#### 🔹 Example: Using Broadcast for Efficient Joins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b141a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast, col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"BroadcastComparison\").getOrCreate()\n",
    "\n",
    "large_df = spark.range(1, 1000001).toDF(\"id\")\n",
    "lookup_data = [(i, f\"Category {i % 10}\") for i in range(1, 51)]\n",
    "small_df = spark.createDataFrame(lookup_data, [\"id\", \"category\"])\n",
    "\n",
    "start_time = time.time()\n",
    "regular_join_df = large_df.join(small_df, \"id\", \"left_outer\")\n",
    "regular_join_df.count()\n",
    "regular_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "broadcast_join_df = large_df.join(broadcast(small_df), \"id\", \"left_outer\")\n",
    "broadcast_join_df.count()\n",
    "broadcast_time = time.time() - start_time\n",
    "\n",
    "print(f\"Regular Join Time: {regular_time:.4f} sec\")\n",
    "print(f\"Broadcast Join Time: {broadcast_time:.4f} sec\")\n",
    "\n",
    "total_rows = broadcast_join_df.count()\n",
    "matched_rows = broadcast_join_df.filter(col(\"category\").isNotNull()).count()\n",
    "unmatched_rows = total_rows - matched_rows\n",
    "\n",
    "print(f\"Total rows: {total_rows}, Matched: {matched_rows}, Unmatched: {unmatched_rows}\")\n",
    "\n",
    "broadcast_join_df.orderBy(\"id\").show(20, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623fe0ad",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "### Understanding the Output\n",
    "\n",
    "Broadcast join ran about **25% faster** than the regular join since it reduces shuffling.  \n",
    "Only **50 rows matched**, while **999,950 had no match** because their IDs weren’t in the lookup table.  \n",
    "This method works best when joining a **large dataset with a small lookup table**, avoiding unnecessary data transfers.  \n",
    "\n",
    "We tested both approaches to compare performance and see when broadcasting is useful.  \n",
    "It helps speed up joins by keeping small tables in memory instead of repeatedly transferring them. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b67529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "big_lookup = {str(i): f\"value_{i}\" for i in range(1000)}\n",
    "broadcast_big = sc.broadcast(big_lookup)\n",
    "\n",
    "test_keys = [str(random.randint(0, 999)) for _ in range(10000)]\n",
    "test_rdd = sc.parallelize(test_keys)\n",
    "\n",
    "def lookup_no_broadcast(key):\n",
    "    return big_lookup.get(key, \"not_found\")\n",
    "\n",
    "def lookup_with_broadcast(key):\n",
    "    return broadcast_big.value.get(key, \"not_found\")\n",
    "\n",
    "print(\"Running performance tests...\")\n",
    "\n",
    "start = time.time()\n",
    "test_rdd.map(lookup_no_broadcast).count()\n",
    "time_without = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "test_rdd.map(lookup_with_broadcast).count()\n",
    "time_with = time.time() - start\n",
    "\n",
    "print(f\"Time without broadcast: {time_without:.2f} seconds\")\n",
    "print(f\"Time with broadcast: {time_with:.2f} seconds\")\n",
    "print(f\"Performance improvement: {(time_without/time_with):.2f}x faster\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb67fada",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Performance Test Results  \n",
    "\n",
    "The test compares **direct lookups** vs. **broadcasted lookups** in an RDD.  \n",
    "- **Without broadcast**, each worker accesses the dictionary separately, causing repeated data transfer.  \n",
    "- **With broadcast**, the dictionary is stored locally on each worker, avoiding unnecessary communication.  \n",
    "- The final output shows **broadcast significantly speeds up lookups**, reducing execution time by **X times**.  \n",
    "\n",
    "This proves that **broadcasting is useful when multiple tasks need to access the same static data**, minimizing network overhead and improving performance.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f04a26",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Persisting Data in Spark\n",
    "\n",
    "### 🔹 What is `persist()`?\n",
    "Persisting (or caching) allows Spark to store data in memory (or disk) to speed up repeated computations. It is useful in iterative algorithms or when multiple actions use the same dataset.we doing it to improve computation efficiency.\n",
    "\n",
    "### 🔹 When to use `persist()`?\n",
    "- When the same dataset is used multiple times within a job.\n",
    "- To **reduce recomputation overhead** in iterative algorithms (e.g., Machine Learning, Graph Processing).\n",
    "\n",
    "#### ✨ Example: Comparing Performance with and without `persist()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1a51b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Measure execution time without persist\n",
    "start_time = time.time()\n",
    "broadcasted_df.count()\n",
    "print(\"Without persist:\", time.time() - start_time, \"seconds\")\n",
    "\n",
    "# Persist the DataFrame in memory\n",
    "broadcasted_df.persist()\n",
    "\n",
    "# Measure execution time with persist\n",
    "start_time = time.time()\n",
    "broadcasted_df.count()\n",
    "print(\"With persist:\", time.time() - start_time, \"seconds\")\n",
    "\n",
    "broadcasted_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a08ca66",
   "metadata": {},
   "source": [
    "### Understanding the Results  \n",
    "\n",
    "The output shows the execution time without `persist()` (0.168s) and with `persist()` (0.151s). The difference is small because Spark is running on a single machine, meaning there is no real overhead from reloading the data. However, in a distributed environment, `persist()` significantly reduces recomputation time by storing the DataFrame in memory.  \n",
    "\n",
    "\n",
    "\n",
    "* The execution time difference is small because Spark is running on a single machine, where all data is already in local memory. In a distributed environment, persist() significantly reduces recomputation time by avoiding repeated data transfers between nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2185e82",
   "metadata": {},
   "source": [
    "## 3. RDD vs. DataFrame: When to Use RDDs?\n",
    "RDDs (Resilient Distributed Datasets) provide **low-level control** over distributed data, whereas DataFrames are optimized for SQL-like operations.\n",
    "\n",
    "| Feature  | RDD | DataFrame |\n",
    "|----------|----|-----------|\n",
    "| Flexibility | High (custom logic) | Moderate (SQL-based) |\n",
    "| Performance | Lower (no optimizations) | High (query optimization) |\n",
    "| Schema | Not enforced | Enforced |\n",
    "| API Usability | Verbose | Concise |\n",
    "\n",
    "#### 🔹 Example: When RDDs are Necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7eb30b",
   "metadata": {},
   "source": [
    "Unlike DataFrames, RDDs allow full control over data partitioning, transformations, and distributed execution. They are useful when applying complex transformations, handling unstructured data, or working with custom partitioning logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ed1129",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([(\"apple\", 10), (\"banana\", 20), (\"orange\", 30)])\n",
    "\n",
    "# Applying custom transformations using RDD API\n",
    "rdd_filtered = rdd.filter(lambda x: x[1] > 15)\n",
    "print(rdd_filtered.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1f0e66",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Why Use RDD Instead of DataFrame?\n",
    "RDDs allow for custom transformations that DataFrames do not support, such as complex filtering, custom partitioning, and transformations that require full control over the data distribution.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f239b412",
   "metadata": {},
   "source": [
    "# **PART 2:** \n",
    "\n",
    "### Partitioning in Spark\n",
    "Partitioning in Spark determines how data is distributed across nodes in a cluster, impacting performance and efficiency. Proper partitioning reduces data shuffling and improves parallelism.\n",
    "\n",
    "🔹 Why is partitioning important?\n",
    "\n",
    "- Ensures balanced workload distribution across cluster nodes.\n",
    "- Reduces data movement (shuffling) in operations like join and groupBy().\n",
    "- Helps optimize parallel processing by utilizing all available resources efficiently.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 📌 Basic Partitioning Example in Spark:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002986bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PartitioningExample\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Creating an RDD with 6 partitions\n",
    "rdd = sc.parallelize(range(1, 101), numSlices=6)\n",
    "\n",
    "# Checking the number of partitions\n",
    "print(f\"Number of partitions: {rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartitioning to increase partitions\n",
    "repartitioned_rdd = rdd.repartition(12)\n",
    "print(f\"After repartitioning: {repartitioned_rdd.getNumPartitions()}\")\n",
    "\n",
    "# Coalescing to decrease partitions\n",
    "coalesced_rdd = repartitioned_rdd.coalesce(4)\n",
    "print(f\"After coalescing: {coalesced_rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e715a6e",
   "metadata": {},
   "source": [
    "### 📌 When to Use Partitioning vs. Broadcast  \n",
    "Partitioning is useful when **handling large datasets** that need efficient processing across a distributed system. However, **Broadcasting** is an alternative that works better when **joining a large dataset with a small lookup table** to minimize shuffling.  \n",
    "\n",
    "#### 💡 Partitioning vs. Broadcast - Key Differences  \n",
    "\n",
    "| **Feature**       | **Partitioning**                         | **Broadcast**                       |\n",
    "|-------------------|-----------------------------------------|-------------------------------------|\n",
    "| **Use Case**      | Large datasets, distributed processing  | Small lookup tables, joins         |\n",
    "| **Performance**   | Optimized for large-scale computation  | Reduces network transfer in joins  |\n",
    "| **Shuffling**     | Some shuffling when data is moved      | Avoids shuffling altogether        |\n",
    "| **Scalability**   | Works for massive data                 | Best when lookup tables fit in memory |\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Demonstrating Broadcast vs. Partitioning  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450fda2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Creating a large DataFrame\n",
    "large_df = spark.range(1, 1000001).toDF(\"id\")\n",
    "\n",
    "# Small lookup table\n",
    "lookup_data = [(i, f\"Category {i % 10}\") for i in range(1, 101)]\n",
    "small_df = spark.createDataFrame(lookup_data, [\"id\", \"category\"])\n",
    "\n",
    "# Regular Join (Without Broadcast)\n",
    "regular_join = large_df.join(small_df, \"id\", \"left_outer\")\n",
    "print(f\"Regular Join partitions: {regular_join.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Broadcast Join (Optimized for Small Tables)\n",
    "broadcast_join = large_df.join(broadcast(small_df), \"id\", \"left_outer\")\n",
    "print(f\"Broadcast Join partitions: {broadcast_join.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c36fbd3",
   "metadata": {},
   "source": [
    "### 📌 Explaining the Results  \n",
    "\n",
    "I tested **regular join vs. broadcast join** to see how partitioning behaves in each case.  \n",
    "\n",
    "- **Regular Join: 5 partitions** → Spark shuffled data between partitions, which can slow performance.  \n",
    "- **Broadcast Join: 8 partitions** → The lookup table was sent to all nodes, reducing shuffling but increasing partitions.  \n",
    "\n",
    "### 🔹 Key Differences  \n",
    "- **Regular join** moves data across partitions, causing overhead.  \n",
    "- **Broadcast join** avoids shuffling, making it faster for small lookup tables.  \n",
    "- **Partitioning changes dynamically** based on the join strategy.  \n",
    "\n",
    "Broadcasting works best for **small tables**, while partitioning is needed for **large datasets**. Next, I’ll explore how both methods can be combined for better optimization. 🚀  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab7631",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "\n",
    "# עד כאן\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461c769d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01f37fc3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "197170b2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91e3e438",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
