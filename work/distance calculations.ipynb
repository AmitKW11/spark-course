{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27df52c7-45e6-493b-b815-eddaffa6051b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Computing distance\n",
    "2023-12-11\n",
    "\n",
    "**NOTE:** Depending on the values used in this notebook (num_rows and vec_dimension), some cells may cause spark jobs to fail due to OOM.<br>\n",
    "In some tests, the OOM even caused the Spark Driver to crash.\n",
    "\n",
    "\n",
    "In this notebook you will try several methods of computing Eucalidean distance between vectors.<br>\n",
    "Depending on the specific way you implement, it can cause your code to explode, give wrong answers, or take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "222f7118-85f4-4fa1-8cb9-1af8e35a7cfd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import*\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col, row_number\n",
    "random.seed(4)\n",
    "spark = SparkSession.builder.appName('efficient distance').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e314891c-e1ec-434b-a7ad-75b21cf92e3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "num_rows = 1000\n",
    "vec_dimension = 100\n",
    "\n",
    "print(f\"The all_pairs matrix will be {num_rows*num_rows} rows, of {vec_dimension} doubles\")\n",
    "print(f\"The all_pairs matrix memory size will be at least (without python/java overheads:\\n\\\n",
    "{num_rows*num_rows*vec_dimension*8/(1<<32)} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "174a7cfa-d15f-4809-affd-13565eee3e59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def random_floats(N: int, M: int) -> pyspark.sql.DataFrame:\n",
    "    \"\"\" Generate a DataFrame with random float numbers\n",
    "    N = 5  # Number of rows\n",
    "    M = 3  # Number of columns\n",
    "    >>> random_floats(6,2)\n",
    "    \"\"\"\n",
    "    data = [[float(random.uniform(0, 100)) for _ in range(M)]for _ in range(N)]\n",
    "    rdd = spark.sparkContext.parallelize(data)\n",
    "    df = rdd.toDF()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0a42796-6423-44eb-b108-a91a3af52c07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use this cell to save or load our df .\n",
    "create = True\n",
    "if create:\n",
    "    df = random_floats(num_rows,vec_dimension)\n",
    "    #df.write.mode(\"overwrite\").parquet(f\"rand{num_rows}_{vec_dimension}\")\n",
    "else:\n",
    "    df = spark.read.parquet(f\"rand{num_rows}_{vec_dimension}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "931a56fc-9b40-4478-a1b2-c81727c804ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df.sample(0.001).limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27248606-b3a8-4e95-8948-30065cb67c88",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Given 2 vectors (of float), compute their Eucalean distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3773cdd7-f1e5-4926-8251-1853bf34782c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Different ways to compute the distance (2 vectors) - from the worst to a little better\n",
    "\n",
    "To achieve the result in reasonable time, the computations have to be done within the JVM (running the Scala code) and not pass data to and from the python inteperter.\n",
    "\n",
    "Also, as usual, we **must not** cause collect() to be called! it will load the full DF to the RAM of this node, probably killing it for large enough row count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "922f44bb-b7f1-4887-9aff-f32880fc5292",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### The worst -- do it entirely in python - will break for big data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2c6cffd-6e8c-4d5a-8b80-64eb9a37678f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# This method should only be used if the resulting NumPy ndarray is expected to be small, as all the data is loaded into the driverâ€™s memory.\n",
    "# To make this abomination even worse, we genereate the column by running the lambda, then collect to a python list in the driver node, then copy to ndarray \n",
    "col1 = df.rdd.map(lambda x: x[0])\n",
    "col2 = df.rdd.map(lambda x: x[1])\n",
    "c1 = np.asarray(col1.collect())\n",
    "c2 = np.asarray(col2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a1c9a7-c0b0-4681-b73d-36683a0e9141",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# using python only\n",
    "delta = [ a-b for a,b in zip(c1,c2)]\n",
    "deltaSqr = [d*d for d in delta]\n",
    "sum(deltaSqr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55eed352-607b-4098-be04-d5f7731148ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the cell above, were you aware of possible numerical stability problems? See https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.hypot.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d6199c9-fcde-4848-a5c4-b87b1b337409",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "source": [
    "### Some of it in python - using numpy (good for local workload only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d86740b8-13c5-4669-ae1b-67a5e8d5ab42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Using numpy\n",
    "d = np.square(c1-c2)\n",
    "d.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "166b7abf-def3-4453-abb1-913b81bf74aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### All in Scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ae2c89d-e50e-41d8-8704-25e86b339c6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "from pyspark.ml.linalg import Vectors\n",
    "v1 = Vectors.dense((col1.collect())) # <<<<<<<<< must fix this!!!! The whole idea is that data never loaded to the Driver memory!\n",
    "v2 = Vectors.dense((col2.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e166323a-6cb6-4c2c-bea9-cc310567eaf4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%time Vectors.squared_distance(v1,v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d8d3e13-8ae2-4f97-8e14-86866c318d13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Did you see that in the three methods above, each of them returned a different value? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d88da02a-7102-4ff1-b75c-90e23aab6102",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "# Given N vectors, compute the distance between every pair (i.e. N\\**2 comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd8b9928-e0b8-4646-89c8-ff5dd8d5542b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We have num_rows vectors. Each vector is of length vec_dimension.\n",
    "\n",
    "Compute distance for all the pairs\n",
    "\n",
    "\n",
    "vec_dimension = 100\n",
    "\n",
    "config: DBR 14ML\n",
    "\n",
    "standard DS3 v2 14GB RAM. <br>\n",
    "Driver = 1 node<br>\n",
    "Workers = 2 nodes\n",
    "\n",
    "NOTE: If you run a cell more than once, the second run might skip some stages (optimizations), so you need to take the FIRST run.\n",
    "```\n",
    "run time [second] to compute distance of ALL the pairs\n",
    "   rows       map()        udf()\n",
    "  10000**2        \n",
    "  30000**2       \n",
    " 100000**2        \n",
    "1000000**2        \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ead5cb1-a95c-4413-a8d9-705a9b605447",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# convert to vector, put in new columns 'features'\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(outputCol=\"features\")\n",
    "assembler.setInputCols(df.columns)\n",
    "M = assembler.transform(df).select(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d198ac2-b8d8-4584-8a70-e1bff0bfd786",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "# I think v1,v2 have to be {Spark/pandas}Series. \n",
    "# This will activate the function pairwise(?)\n",
    "def dist(v1,v2):\n",
    "    # the squared_distance() returns numpy.float64.\n",
    "    # The Dataframe structType expects float/double, so complains about the type.\n",
    "    # we must cast it\n",
    "    return float( Vectors.squared_distance(v1,v2))\n",
    "\n",
    "dist_udf = f.udf(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc89b327-5967-4173-8091-0a0d5cb2221b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_pairs = M.crossJoin(M).toDF(*['left','right']) # after the join(), both columns have the same name, so we create a NEW DF with meaningful names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec313d95-2c57-4feb-aa01-3cee57fe9a1a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "How is it possible that creating all the pairs took so little time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "856ac5b5-d4f5-4c6d-b961-0f9254a96a5f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## How many rows are in all_pairs? How many do we need?\n",
    "\n",
    "all_pairs has num_rows**2. We need the distance(v[i],v[j]) for all i <> j , but distance (i,j) == distance(j,i) so need HALF the pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a0d9fe2-d52e-4853-b66a-b2b94a4599a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If you want to round or just display shorter numbers, you can:\n",
    "from pyspark.sql.functions import format_number\n",
    "df.select( [format_number(c,2).alias(c) for c in df.columns] ).show(2)\n",
    "\"\"\"\n",
    "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
    "|   _1|   _2|   _3|   _4|   _5|   _6|   _7|   _8|   _9|  _10|\n",
    "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
    "|23.60|10.32|39.61|15.50| 6.65|40.16|91.80|80.05|76.52|22.19|\n",
    "|53.67|27.67|17.27|10.62|21.44|92.75|82.89|80.67|80.04|19.34|\n",
    "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9130729-16fb-4766-8369-8bb1ceba154a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Apply the function using map() for each row\n",
    "This can be done on RDD only so we move there and back\n",
    "\n",
    "NOTE: When running on local setup, with \n",
    "num_rows = 10000, vec_dimension = 100\n",
    "the following cell hangs. CPU is 0%, and memory consumption is fixed. I suspect the memory allocation to the JVM is exhausted.\n",
    "\n",
    "**Need to find stderr of this stage.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60319897-765d-4ebd-b8ae-0b72857cd29b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# RDD have no column names (the internet is full of lies), so we\n",
    "# must use int indexes\n",
    "result_rdd = all_pairs.rdd.map(lambda row: dist(row[0], row[1]))\n",
    "# for some reason, the first row in the RDD is empty, so the schema cannot be inferred. \n",
    "# We must supply it explicitly.\n",
    "spark.createDataFrame(result_rdd,schema=\"Double\").show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0697a21f-5085-4c1d-a598-4685d33060ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Use UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b4263e-ff97-479e-aa0e-28b3bf174b29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf,sum\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Register the function as a UDF\n",
    "your_udf = udf(dist, returnType=DoubleType())\n",
    "\n",
    "# Apply the function to each row\n",
    "result_df = all_pairs.withColumn(\"result\",your_udf(all_pairs['left'], all_pairs['right']))\n",
    "result_df.cache()\n",
    "result_df.select('result').show(4)\n",
    "\n",
    "total = result_df.agg(sum('result')).collect()\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "685aedfc-6fc7-4166-83fd-3c56b71248a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We should get num_rows**2 rows. For num_rows = 1000000, the Driver node crashed: \"java.lang.RuntimeException: abort: DriverClient destroyed\n",
    "#    Internal error. Attach your notebook to a different compute or restart the current compute.\"\n",
    "# This is because count() can (but not always due to optimizations) load data into memory. In this case I saw in the driver logs \"OOM\"\n",
    "# result_df.count()\n",
    "\n",
    "distances = result_df.select('result')\n",
    "sorted_dist = distances.orderBy(col(\"result\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59f710d2-d40e-425a-a4de-09e4c74394b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this code ran on 2 [???] executors for 11 minutes\n",
    "top10 = sorted_dist.take(10)\n",
    "# only now the execution plan will evaluate. \n",
    "# for num_rows = 10K, it ran for 13.45minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8081fd1-6418-4c9e-be6d-1612755d1199",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Why does the cell above hangs for so long? (when using num_rows = 10K)?\n",
    "In DBR, click the **View** link of the job, choose Executors, choose stdout of one of the workers.\n",
    "\n",
    "Here is one line that repeats forever:\n",
    "`2023-12-10T17:19:58.806+0000: 301.202: [GC (Allocation Failure) [PSYoungGen: 2478672K->4033K(2480640K)] 3593339K->1118724K(4011008K), 0.0086329 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] `\n",
    "\n",
    "Which simply means: **I am out of memory, asking for more, not getting it fast enough**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "567b9cba-6e56-4ff7-8d1a-59662a6bdfd2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Can we get the K smallest values without sorting the full DF?\n",
    "\n",
    "Below are two attempts. They improved by 10..20% in my test.    Remember that the runtime includes the whoe DAG execution -- not just sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e649b87-b9d5-44bb-ad80-d65e69d52e1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this code ran on 5 executors for 10 minutes\n",
    "from pyspark.sql.functions import col, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "k = 20\n",
    "window_spec = Window.orderBy(col(\"result\").asc()) # This does not realy help, since the window is all the DF!\n",
    "dfK = distances.withColumn(\"rank\", dense_rank().over(window_spec)).filter(col(\"rank\") <= k).drop(\"rank\")\n",
    "dfK.limit(k).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f127bd60-a4a1-4257-952c-37275cbfbc53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this code ran on 6 executors for 8 minutes\n",
    "top_k_elements = distances.select(\"result\").rdd.flatMap(lambda x: x).takeOrdered(k)\n",
    "top_k_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71794c32-a0ea-4f21-a71d-7dba9f2f618c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Before comparing results: <br>\n",
    "How many executors (worker nodes) worked on each job? The cluster may resize due to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59d7793e-3534-4e7b-8a0d-6b8b141ee92c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "distance calculations",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
