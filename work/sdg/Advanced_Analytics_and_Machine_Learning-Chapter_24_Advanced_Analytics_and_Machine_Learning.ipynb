{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDG Chapter 24 - Advanced Analytics and Machine Learning\n",
    "\n",
    "Converted from the py file in the repo (using p2j) and then manualy fixed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#import pyspark.sql.functions as f\n",
    "#from pyspark.sql.types import *\n",
    "spark = SparkSession.builder.appName('MLlib').getOrCreate()\n",
    "datapath = \"../../data/sdg/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors can be dense or sparse. Both can be used by the MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "denseVec = Vectors.dense(1.0, 2.0, 3.0)\n",
    "size = 3\n",
    "idx = [1, 2] # locations of non-zero elements in vector\n",
    "values = [2.0, 3.0]\n",
    "sparseVec = Vectors.sparse(size, idx, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(datapath + \"/simple-ml\")\n",
    "df.orderBy(\"value2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature engineering with Transformers\n",
    "The RFormula is a method to select fields. It borrows from R language.\n",
    "\n",
    "The same processing can be done using other API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "supervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fittedRF = supervised.fit(df)   # apply the formula to the df\n",
    "preparedDF = fittedRF.transform(df) # ...and actually perform the addition of 'features'\n",
    "preparedDF.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have preprocessed data.\n",
    "\n",
    "Let's train a model on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = preparedDF.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\") # Use the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list of all the LogisticRegression class hyper parameters.\n",
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. This transformation is 'eager', done immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedLR = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the mode to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fittedLR.transform(train).select(\"label\", \"prediction\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "### Estimators\n",
    "The MLlib has several classes to estimate model performance.\n",
    "\n",
    "We now **repeat** the code above, this time performing grid search on the hyper params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rForm = RFormula()\n",
    "lr = LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of manually\n",
    "using our transformations and then tuning our model we just make them stages in the overall\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "stages = [rForm, lr]\n",
    "pipeline = Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you arranged the logical pipeline, the next step is training. In our case, we won’t train\n",
    "just one model (like we did previously); we will train several variations of the model by\n",
    "specifying different combinations of hyperparameters that we would like Spark to test. We will\n",
    "then select the best model using an Evaluator that compares their predictions on our validation\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "params = ParamGridBuilder()\\\n",
    "  .addGrid(rForm.formula, [\n",
    "    \"lab ~ . + color:value1\",\n",
    "    \"lab ~ . + color:value1 + color:value2\"])\\\n",
    "  .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "  .addGrid(lr.regParam, [0.1, 2.0])\\\n",
    "  .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built a grid with 2 Rformula, 3 ElasticNet, 2 Regularization params.\n",
    "\n",
    "Overall 2 * 3 * 2 = 12 combinations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the grid is built, it’s time to specify our evaluation process. The evaluator allows us to\n",
    "automatically and objectively compare multiple models to the same evaluation metric. There are\n",
    "evaluators for classification and regression, covered in later chapters, but in this case we will use\n",
    "the BinaryClassificationEvaluator\n",
    "\n",
    "The possible metrics are areaUnderPR and areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\\\n",
    "  .setMetricName(\"areaUnderROC\")\\\n",
    "  .setRawPredictionCol(\"prediction\")\\\n",
    "  .setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a pipeline that specifies how our data should be transformed, we will perform\n",
    "model selection to try out different hyperparameters in our logistic regression model and\n",
    "measure success by comparing their performance using the areaUnderROC metric.\n",
    "\n",
    "We could also use CrossValidator instead of TrainValidationSplit.\n",
    "\n",
    "Remember that we MUST NOT use the test data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "tvs = TrainValidationSplit()\\\n",
    "  .setTrainRatio(0.75)\\\n",
    "  .setEstimatorParamMaps(params)\\\n",
    "  .setEstimator(pipeline)\\\n",
    "  .setEvaluator(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvsFitted = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did it perform on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(tvsFitted.transform(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a summary for some of the (12) models we tries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " trainedPipeline = tvsFitted.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " TrainedLR = trainedPipeline.stages[1] # stages[0] has the RFormulamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the loss (== objective function) in each iteration\n",
    "# We can see that the loss converges to 0.43 . Is it good enough?\n",
    "TrainedLR.summary.objectiveHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what we can get from the summary (or just look at the doc)\n",
    "[ x for x in dir(TrainedLR.summary) if x[0] != '_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainedLR.summary.precisionByLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainedLR.summary.recallByThreshold.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainedLR.getRegParam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can save the model, to use it later for predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvsFitted.write().overwrite().save('lr_model_very_nice')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
