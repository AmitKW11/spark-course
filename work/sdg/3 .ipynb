{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4842140-3ca1-4729-a125-7eb9feeb7fb4",
   "metadata": {},
   "source": [
    "# structured streaming\n",
    "\n",
    "\n",
    "work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aa2c11-87c7-4b7c-a22a-c3da2288bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "datapath = \"../../data/sdg/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b2dc1-f3be-4c2c-bcfe-230633405ad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "staticDataFrame = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(datapath + \"/retail-data/by-day/*.csv\")\n",
    "staticDataFrame.createOrReplaceTempView(\"retail_data\")\n",
    "staticSchema = staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23886ab3-16dd-4cd9-b852-ffb13426ae85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, column, desc, col\n",
    "staticDataFrame\\\n",
    ".selectExpr(\n",
    "\"CustomerId\",\n",
    "\"(UnitPrice * Quantity) as total_cost\",\n",
    "\"InvoiceDate\")\\\n",
    ".groupBy(\n",
    "col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    ".sum(\"total_cost\")\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250a7c79-20d7-41de-9407-b87069a6abff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "streamingDataFrame = spark.readStream\\\n",
    ".schema(staticSchema)\\\n",
    ".option(\"maxFilesPerTrigger\", 1)\\\n",
    ".format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".load(datapath + \"/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4bc69c-6804-4d02-b064-483958211984",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingDataFrame.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd3d3e9-c957-42f9-b872-826a08dc8108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "purchaseByCustomerPerHour = streamingDataFrame\\\n",
    ".selectExpr(\n",
    "\"CustomerId\",\n",
    "\"(UnitPrice * Quantity) as total_cost\",\n",
    "\"InvoiceDate\")\\\n",
    ".groupBy(\n",
    "col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    ".sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57894178-6f3b-49da-a64f-5b2f98ce6685",
   "metadata": {},
   "source": [
    "\n",
    "**NOTE:**<br>You shouldn’t use either of these streaming methods in production, but they do make for\n",
    "convenient demonstration of Structured Streaming’s power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde34774-6d46-46dc-a660-786b7a9dd1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchaseByCustomerPerHour.writeStream\\\n",
    ".format(\"memory\")\\\n",
    ".queryName(\"customer_purchases\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec13b75-00d6-4668-bc28-778c89ea2ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM customer_purchases\n",
    "ORDER BY `sum(total_cost)` DESC\n",
    "\"\"\")\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2d355ac-53e9-40dd-8c42-3e60b7d44db4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# write to the console. \n",
    "# use it for debug ONLY -- it will write a lot of data!\n",
    "purchaseByCustomerPerHour.writeStream\\\n",
    ".format(\"console\")\\\n",
    ".queryName(\"customer_purchases_2\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3779cf54-3c02-4310-8f05-1eefeeeadac1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using Machine Learning (MLlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cd00f8-e4f1-4aa2-9831-d6bf8693af67",
   "metadata": {},
   "source": [
    "## eample: using K-MEANS clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a53a65-3c45-4700-9d95-889e95f9bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "preppedDataFrame = staticDataFrame\\\n",
    ".na.fill(0)\\\n",
    ".withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\"))\\\n",
    ".coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0390511a-c704-438d-b75d-9f19d4950606",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataFrame = preppedDataFrame\\\n",
    ".where(\"InvoiceDate < '2011-07-01'\")\n",
    "testDataFrame = preppedDataFrame\\\n",
    ".where(\"InvoiceDate >= '2011-07-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c2b606-89e7-4506-bb2b-1fd1f1645c43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainDataFrame.count(), testDataFrame.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5019362f-f557-45bf-a120-22e999d31405",
   "metadata": {},
   "source": [
    "### Convert types to numeric\n",
    "\n",
    "String indexer and One Hot Encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6245c5ae-6436-434d-a3ba-7e113e474f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer()\\\n",
    ".setInputCol(\"day_of_week\")\\\n",
    ".setOutputCol(\"day_of_week_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6973cbd-eb7b-4494-9b20-9f8917dee565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "encoder = OneHotEncoder()\\\n",
    ".setInputCol(\"day_of_week_index\")\\\n",
    ".setOutputCol(\"day_of_week_encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac002ef-d1a6-4161-92dc-407ee9bbd7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler()\\\n",
    ".setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n",
    ".setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727bc076-0010-4466-97c9-91adb7230cfe",
   "metadata": {},
   "source": [
    "Create a pipeline so any new data will be processed.\n",
    "\n",
    "In a real scenario we will need to scale the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ea215-e209-4bd8-a8f6-83742123f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "transformationPipeline = Pipeline()\\\n",
    ".setStages([indexer, encoder, vectorAssembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43952f94-3b16-41f4-a9a8-db9e2ea74109",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fittedPipeline = transformationPipeline.fit(trainDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1451a420-c4f7-410f-8672-b4c01a3ca42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a86acec-e53a-4396-9fd3-0ae50e6b99e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTraining = fittedPipeline.transform(trainDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ba247-18e5-4511-867e-53fa74062671",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTraining.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e272265-ed63-4a90-aa11-735cc28cde45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans = KMeans()\\\n",
    ".setK(20)\\\n",
    ".setSeed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066dc422-7cc4-4a69-b8be-154853ff9145",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmModel = kmeans.fit(transformedTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca03d3b-1e5b-41ed-ac4c-b2a0beb435de",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmModel.summary.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94160ee-c4c0-4c67-bdf8-68c00f980e36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: the [SDG] book is written with spark 2.x. \n",
    "# In spark 3.0 the cost computation has changed.\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "evaluator.setPredictionCol('pred')\n",
    "\n",
    "transformedTest = fittedPipeline.transform(testDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe13e48-b16c-4234-8a15-9eb85d672596",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39f7ec2-1808-4fae-8a1f-ef77a5885bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8760250-caa7-4b53-b634-ec06bbeaacc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d8ed1d-10db-4e4f-962a-ce3aa10d2e55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "silhouette = evaluator.evaluate(transformedTraining.select('features'))\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00b805e-9ef9-4487-8f85-433c0dfb9f29",
   "metadata": {},
   "source": [
    "# Check yourself\n",
    "* disable 'transformedTraining.cache()' and repeat the run. How long is the run now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9343521d-106e-45a7-9858-8df59e587c49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
