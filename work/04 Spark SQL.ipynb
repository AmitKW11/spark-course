{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd7fdcaf-a575-49c5-b831-2106e545dc0d",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "(No video yet)\n",
    "\n",
    "Remember that Spark is a data processsing engine, not a database.\n",
    "\n",
    "See https://spark.apache.org/docs/latest/sql-programming-guide.html \n",
    "\n",
    "Most of the text here is taken from  [SDG] chapter 10 \"Spark SQL\" .\n",
    "\n",
    "Spark SQL is a Spark module for structured data processing.\n",
    "\n",
    "Do not confuse with reading/writing to an RDBMS. \n",
    "You can run SQL query on a dataframe that you created from any data source.\n",
    "\n",
    "In a nutshell, with Spark SQL you can run SQL queries against views or tables organized into\n",
    "databases. You also can use system functions or define user functions and analyze query plans in\n",
    "order to optimize their workloads. This integrates directly into the DataFrame and Dataset API,\n",
    "and as we saw in previous chapters, you can choose to express some of your data manipulations\n",
    "in SQL and others in DataFrames and they will **compile to the same underlying code**. [SDG]\n",
    "\n",
    "## What is Apache Hive?\n",
    "Before Sparkâ€™s rise, Hive was the de facto big data SQL access layer. Originally developed at Facebook, Hive became an incredibly popular tool across industry for *performing SQL operations on big data*. In many ways it helped propel Hadoop into different industries because analysts could run SQL queries[SDG]\n",
    "\n",
    "\n",
    "## NOTE\n",
    "Spark SQL is intended to operate as an online **analytic** processing (OLAP) database, not an online transaction processing (OLTP) database. [SDG]\n",
    "\n",
    "\n",
    "You can completely interoperate between SQL and DataFrames, as you see\n",
    "fit. For instance, you can create a DataFrame, manipulate it with SQL, and then manipulate it\n",
    "again as a DataFrame:\n",
    "\n",
    "```\n",
    "spark.read.json(\"/data/flight-data/json/2015-summary.json\")\\\n",
    ".createOrReplaceTempView(\"some_sql_view\") # DF => SQL\n",
    "spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count)\n",
    "FROM some_sql_view GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\\\n",
    ".where(\"DEST_COUNTRY_NAME like 'S%'\").where(\"`sum(count)` > 10\")\\\n",
    ".count() # SQL => DF\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f3456-c8fe-4bd1-9944-83b0e4e631ec",
   "metadata": {},
   "source": [
    "# Views\n",
    "\n",
    "To an end user, views are displayed as tables, except rather than rewriting all of the data to a new\n",
    "location, they simply perform a transformation on the source data at query time.\n",
    "\n",
    "Views are created in the `default` database"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e2c37d5-1bb5-4a51-8c73-129966b73a88",
   "metadata": {},
   "source": [
    "CREATE VIEW just_usa_view AS\n",
    "    SELECT * FROM flights WHERE dest_country_name = 'United States'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6241f734-06d4-461f-8407-b78f7356d2dc",
   "metadata": {},
   "source": [
    "A view is effectively a **transformation** and Spark will perform it only at query time. This means\n",
    "that it will only apply that filter after you actually go to query the table (and not earlier).\n",
    "Effectively, views are equivalent to creating a new DataFrame from an existing DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e3d010b-8a1a-4b60-b707-9e91ae7a9718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "datapath = \"../data/sdg/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d176d-1a1f-43c9-80eb-ae3eeca20ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(datapath + \"/retail-data/by-day/2010*.csv\")\n",
    "\n",
    "df.createOrReplaceTempView(\"retail_data\")\n",
    "schema = df.schema\n",
    "df.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e6ff3-f916-43e1-b9dd-871a853ee55c",
   "metadata": {},
   "source": [
    "The `retail_data` is a temporary view. It will live as long as the current SparkSession. <br>\n",
    "This view cannot be shared with other Spark applications or databases. There are ways to do this, but not covered here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3a7490-0313-4dfe-b4e1-dd028b74ab73",
   "metadata": {},
   "source": [
    "# Let's run some code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e1f54b-4364-4169-ad4b-735023674105",
   "metadata": {},
   "source": [
    "Dataframe operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a560d3b6-95ae-441f-a0c5-fcb43884bcc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, column, desc, col\n",
    "df.selectExpr(\n",
    "\"CustomerId\",\n",
    "\"(UnitPrice * Quantity) as total_cost\",\n",
    "\"InvoiceDate\").groupBy(\n",
    "col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\")).sum(\"total_cost\").sort(desc(\"CustomerId\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250a0de5-ee27-4cd9-be3d-a8d26bd71a08",
   "metadata": {},
   "source": [
    "Spark SQL operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7e541-597a-4f5f-b27a-e816db738f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT  CustomerId , date(InvoiceDate) as Created,  SUM(UnitPrice * Quantity) AS total_cost FROM retail_data\n",
    "             GROUP BY CustomerId, date(InvoiceDate)\n",
    "\"\"\").sort(desc(\"CustomerId\"))\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49a4d8-f3dc-429d-b441-5dddad254514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a4afaa4-07a9-45f0-95af-74cb7366e902",
   "metadata": {},
   "source": [
    "# Complex Types\n",
    "Complex types are a departure from standard SQL and are an incredibly powerful feature that\n",
    "does not exist in standard SQL. Understanding how to manipulate them appropriately in SQL is\n",
    "essential. There are three core complex types in Spark SQL: **structs, lists, and maps**.\n",
    "\n",
    "This is an advanced topic.<br>\n",
    "For examples, check the book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2652b8d9-e1e1-4d38-b20a-edf0de38b92e",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "\n",
    "When performing queries such as `groupby(\"column\").sum()`, all the data has to be scanned, using sequential read.\n",
    "\n",
    "What if we have `select a,b where b=\"wine\"` and there are few matching rows? \n",
    "\n",
    "Spark does not support indexing of the data (not to be confused with indexing of the database that we read to create the dataframe!).\n",
    "\n",
    "Instead, you should rely on *partitioning* by the columns you plan to query. This should provide the needed speed.\n",
    "\n",
    "* can a partition be read in parallel by several threads?\n",
    "\n",
    "See interesting disucssion in StackOverflow:\n",
    "https://stackoverflow.com/questions/36938976/why-spark-sql-considers-the-support-of-indexes-unimportant .\n",
    "\n",
    "Microsoft implemented a prototype indexer, but I don't know if it was integrated into Spark: https://www.databricks.com/session_na20/hyperspace-an-indexing-subsystem-for-apache-spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd02b98-cc97-46ab-a9db-1aaf779c5e54",
   "metadata": {},
   "source": [
    "# Check youreself\n",
    "* Where are database files stored?\n",
    "* Can Spark do UPDATE TABLE? Why?\n",
    "* What is the role of View?\n",
    "* is indexing used? if not, what should be used instead?\n",
    "\n",
    "Answer [here](https://forms.gle/KKVZRgboNBDFKE7K6) and see your results\n",
    "\n",
    "## Additional Exercises \n",
    "\n",
    "* Rename the \"window\" column in the DataFrame operation to 'Created' within the same 'selectExpr' query, aligning it with the SQL operation.\n",
    "\n",
    "* Create two separate operations, one using DataFrame operations and the other using SQL, to generate a table showing the count of invoices per customer, remove null values, and arrange the results in ascending order.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc41635c-5efb-458c-a2ce-ab733510bcd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
