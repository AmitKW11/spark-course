{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e437465a-8c86-4bf0-b123-10b0a14b44ea",
   "metadata": {},
   "source": [
    "# SPARK UI TUTORIAL\n",
    "\n",
    "What are we going to talk about?\n",
    "\n",
    "This notebook will help you understand Spark's Web UI - basically your dashboard for seeing what's happening in your Spark applications.\n",
    "All information on this notebook is on the basis of Apache Spark 3.5 UI Guide, available at:\n",
    "https://spark.apache.org/docs/latest/web-ui.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9223fce7-1bf4-413f-9e0d-d9f7ea4c932c",
   "metadata": {},
   "source": [
    "## Table of contents:\n",
    "\n",
    "1. What is Spark - a reminder\n",
    "2. Jobs Tab\n",
    "3. DAG Visualizations & Event Timeline\n",
    "4. Stages Tab\n",
    "5. Storage Tab \n",
    "6. SQL Tab \n",
    "7. Environment Tab\n",
    "8. Executor Tab\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6672b264-955c-4929-b9e3-897ac9d764dc",
   "metadata": {},
   "source": [
    "## 1. What's Apache Spark? - reminder\n",
    "Think of Spark as a super-powered data processing engine. You know how your laptop struggles when you try to work with huge Excel files? \n",
    "Spark is designed to handle data that's WAY bigger than that by spreading the work across multiple computers.\n",
    "\n",
    "Cool things about Spark:\n",
    "- It's super fast (uses computer memory instead of just hard drive)\n",
    "- Works with Python and other languages\n",
    "- Can handle both regular data processing and other like machine learning\n",
    "- Won't lose your data if something crashes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d127532-8150-4746-9ded-3e1239784161",
   "metadata": {},
   "source": [
    "## The UI part \n",
    "\n",
    "The Spark UI is like the dashboard of your car - it shows you what's happening under the hood.\n",
    "- See if your code is actually running or stuck\n",
    "- Find out why your program is taking forever\n",
    "- Figure out if you're about to crash your computer's memory ðŸ˜…\n",
    "  \n",
    "It's a webpage with the following tabs:\n",
    "Jobs Tab,\n",
    "Stages Tab,\n",
    "Storage Tab, \n",
    "SQL Tab,\n",
    "Environment Tab,\n",
    "and Executor Tab.\n",
    "\n",
    "In addition to that, there are DAG visualizations (more on that later) and Event timelines, which summarize metrics for tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e64679-2192-4382-a483-e9e333b2121a",
   "metadata": {},
   "source": [
    "Let's create our first Spark program and check out the UI:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "809ebd4d-e8c1-4d5b-9cb6-8fa93f983188",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\ak_we\\anaconda3\\lib\\site-packages (3.5.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\ak_we\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "765cafbe-45a6-49ed-bf0d-c1fefd214a26",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey! Your Spark UI should be available at: http://host.docker.internal:4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start up Spark (like turning on your car's engine)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyFirstSparkApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Hey! Your Spark UI should be available at:\", spark.sparkContext.uiWebUrl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae1eeae-e5da-4248-ad68-d9fac13cc8c8",
   "metadata": {},
   "source": [
    "\n",
    "# 2. The Spark Jobs Tab:\r\n",
    "The Jobs tab in Apache Spark's UI is one of the most important interfaces for understanding what's happening in your Spark application.\r\n",
    "\r\n",
    "## What is a Spark Job?\r\n",
    "A \"job\" in Spark is created whenever you call an **action** on your data. Actions are operations that trigger actual computation and produce results, unlike transformations which just build up a plan. Common actions include:\r\n",
    "- `collect()` - Brings data back to the driver program\r\n",
    "- `count()` - Counts the number of elements\r\n",
    "- `save()` - Writes data to storage\r\n",
    "- `show()` - Displays data\r\n",
    "- `take(n)` - Returns the first n elements\r\n",
    "\r\n",
    "## What the Jobs Tab Shows You\r\n",
    "\r\n",
    "### Job Listing and Status\r\n",
    "- **Job ID**: A unique identifier for each job (starting at 0)\r\n",
    "- **Status indicators**: \r\n",
    "  - RUNNING - Currently executing\r\n",
    "  - SUCCEEDED - Completed successfully\r\n",
    "  - FAILED - Terminated with errors\r\n",
    "  - PENDING - Waiting to be scheduled\r\n",
    "\r\n",
    "### Detailed Metrics\r\n",
    "- **Submission Time**: When the job was sent to the Spark engine\r\n",
    "- **Duration**: How long the job took/is taking to complete\r\n",
    "- **Stages**: The job broken down into its component processing steps\r\n",
    "- **Tasks**: Individual units of work within each stage\r\n",
    "- **Input/Output**: Data processed and generated\r\n",
    "\r\n",
    "### Visual Elements\r\n",
    "- **Progress bars**: Show percentage completion of each job\r\n",
    "- **Stage visualization**: See which parts of your job are complete or in progress\r\n",
    "- **Timeline view**: A chronological representation of when jobs executed\r\n",
    "- **Dependency charts* reads clearly as part of your Spark tutorial. applications run more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eca71410-69dd-4786-be4e-4f7f082582b5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o52.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 0.0 failed 1 times, most recent failure: Lost task 5.0 in stage 0.0 (TID 5) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(students, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrade\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate average grade by name\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m avg_grade \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mavg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrade\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1263\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m \n\u001b[0;32m   1245\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[1;32m-> 1263\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcollectToPython()\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o52.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 0.0 failed 1 times, most recent failure: Lost task 5.0 in stage 0.0 (TID 5) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create sample data\n",
    "students = [(1, \"Alice\", 85), (2, \"Bob\", 92), (3, \"Charlie\", 78)]\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(students, [\"id\", \"name\", \"grade\"])\n",
    "\n",
    "# Calculate average grade by name\n",
    "avg_grade = df.groupBy(\"name\").avg(\"grade\").collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06d2d25-3505-47e6-8f83-6bf67cc7b7a9",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Your First Exercise: Making Spark Work\n",
    " Let's create something bigger to actually see what's happening\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d4fd51-ba97-4364-9f5b-1bd1ece2596e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Create pretend student data\n",
    "student_data = []\n",
    "for i in range(10000):  # 10,000 students\n",
    "    student_data.append((\n",
    "        i,  # student ID\n",
    "        random.randint(60, 100),  # random grade\n",
    "        random.choice(['Math', 'Science', 'History', 'English'])  # random subject\n",
    "    ))\n",
    "\n",
    "# Turn it into a Spark DataFrame\n",
    "students_df = spark.createDataFrame(student_data, [\"id\", \"grade\", \"subject\"])\n",
    "\n",
    "# Find average grades by subject\n",
    "result = students_df.groupBy(\"subject\") \\\n",
    "    .agg({\"grade\": \"avg\"}) \\\n",
    "    .collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d8b46e-8bef-4ef8-a7cc-1602cee9643b",
   "metadata": {},
   "source": [
    "## While this runs, quickly:\n",
    " 1. Open the Spark UI (http://localhost:4040)\n",
    " 2. Click on the Jobs tab\n",
    " 3. Look at what's happening\n",
    "\n",
    "## Things to spot in the UI:\n",
    " - How many stages did your job have?\n",
    " - How long did it take?\n",
    " - Are there any cool graphs showing up?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb37f57e-2b9f-436a-9e67-71b0676f2585",
   "metadata": {},
   "source": [
    "# 3. Understanding Spark Event Timeline and DAG Visualization\n",
    "\n",
    "# Event Timeline in Spark\n",
    "\n",
    "The Event Timeline in Apache Spark provides a chronological view of your application's execution. It's an essential diagnostic tool that helps beginners understand the flow and timing of operations.\n",
    "\n",
    "### What the Event Timeline Shows\n",
    "\n",
    "1. **Time-based visualization**: A horizontal chart showing when each component of your Spark application was active\n",
    "   \n",
    "2. **Component types displayed**:\n",
    "   - **Jobs**: The highest-level units of work (triggered by actions)\n",
    "   - **Stages**: Sub-divisions of jobs that represent distinct processing steps\n",
    "   - **Tasks**: The smallest units of work, distributed across executors\n",
    "   - **Executors**: The processes actually running your code\n",
    "\n",
    "3. **Color coding**:\n",
    "   - Different colors typically represent different states (running, completed, failed)\n",
    "   - The length of each bar represents duration\n",
    "   - Gaps may indicate idle time or scheduling delays\n",
    "\n",
    "### How to Use the Event Timeline\n",
    "\n",
    "- **Identify bottlenecks**: Look for unusually long stages or tasks\n",
    "- **Spot parallelism issues**: See if your tasks are running concurrently or serially\n",
    "- **Detect skew**: Find tasks that take much longer than others in the same stage\n",
    "- **Understand dependencies**: See which stages must complete before others can begin\n",
    "- **Track resource usage**: Monitor how many executors are active at any time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a52627d-a977-41f2-8c2b-41591a2bf78d",
   "metadata": {},
   "source": [
    "# DAG Visualization in Spark\n",
    "\n",
    "The DAG (Directed Acyclic Graph) Visualization shows the logical and physical plan of your Spark operations.\n",
    "\n",
    "### Key Elements of the DAG Visualization\n",
    "\n",
    "1. **Nodes**: \n",
    "   - Each box represents an operation (like `map`, `filter`, `join`, etc.)\n",
    "   - Labels show the specific transformation being applied\n",
    "   - May include additional details about partitioning, size, or format\n",
    "\n",
    "2. **Edges (arrows)**:\n",
    "   - Show the flow of data between operations\n",
    "   - Indicate dependencies between steps\n",
    "   - Direction always flows from input to output (hence \"directed\")\n",
    "\n",
    "3. **Stages**:\n",
    "   - Groups of operations that can be executed together without shuffling data\n",
    "   - Usually separated by shuffle boundaries (operations that require data redistribution)\n",
    "\n",
    "### What Makes it \"Acyclic\"\n",
    "\n",
    "The \"acyclic\" part means there are no loops or cycles in the graph - data always flows forward, never back to an earlier operation. This is fundamental to how Spark processes data in a deterministic way.\n",
    "\n",
    "### How Beginners Can Use the DAG Visualization\n",
    "\n",
    "1. **Understanding transformations**: See how operations like `map`, `filter`, and `join` connect together\n",
    "   \n",
    "2. **Identifying shuffles**: Spot where data is being redistributed across the cluster (often a performance bottleneck)\n",
    "   \n",
    "3. **Recognizing optimizations**: See how Spark has optimized your code (some operations might be combined or reordered)\n",
    "   \n",
    "4. **Debugging**: Trace the flow of data to understand where problems might occur\n",
    "   \n",
    "5. **Learning Spark's execution model**: Visualize how Spark translates high-level code into concrete execution steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78786aeb-5cde-4f1c-9010-a0e185591f14",
   "metadata": {},
   "source": [
    "\n",
    "## How Timeline and DAG Visualizations Work Together\n",
    "\n",
    "These two visualizations complement each other:\n",
    "- **Timeline**: Shows *when* things happened and how long they took\n",
    "- **DAG**: Shows *what* happened and how operations connected\n",
    "\n",
    "Together, they give beginners a complete picture of both the logical structure and temporal execution of their Spark applications, which is invaluable for learning, optimization, and debugging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15231ec-0958-464e-9761-74c55c0e275c",
   "metadata": {},
   "source": [
    "# 4. The Stages Tab \r\n",
    "\r\n",
    "## What Are Stages in Spark?\r\n",
    "\r\n",
    "In Apache Spark, a \"stage\" is a set of tasks that can be executed in parallel without requiring data to be shuffled across the cluster. Stages are created when:\r\n",
    "\r\n",
    "1. A **shuffle operation** is required (like `groupByKey`, `reduceByKey`, `join`, etc.)\r\n",
    "2. An **action** is called that triggers computation (like `collect`, `count`, `save`)\r\n",
    "\r\n",
    "Understanding stages is crucial because they represent the actual chunks of work that Spark performs.\r\n",
    "\r\n",
    "## The Stages Tab Interface\r\n",
    "\r\n",
    "The Stages tab in Spark's UI provides detailed information about each stage in your application:\r\n",
    "\r\n",
    "### Key Components of the Stages Tab\r\n",
    "\r\n",
    "1. **Stages List**:\r\n",
    "   - **Stage ID**: A unique identifier for each stage\r\n",
    "   - **Description**: What operations this stage performs\r\n",
    "   - **Submitted Time**: When this stage was submitted to the scheduler\r\n",
    "   - **Duration**: How long the stage took to execute\r\n",
    "   - **Tasks Progress**: Visual representation of task completion\r\n",
    "   - **Input/Output**: Data sizes processed and produced\r\n",
    "\r\n",
    "2. **Stage Details** (when clicking on a specific stage):\r\n",
    "   - **Task metrics**: Statistics about task execution\r\n",
    "   - **Executor information**: Which executors ran tasks\r\n",
    "   - **Shuffle read/write**: Data movement between stages\r\n",
    "   - **Task distribution**: How evenly work was distributed\r\n",
    "\r\n",
    "3. **Aggregated Metrics**:\r\n",
    "   - **Summary statistics**: Averages, minimums, maximums\r\n",
    "   - **Percentile information**: Distribution of task durations\r\n",
    "   - **Skew detection**: Identifying imbalanced work distribution\r\n",
    "\r\n",
    "## How to Interpret the Stages Tab\r\n",
    "\r\n",
    "### Stage Status Indicators\r\n",
    "\r\n",
    "- **Active**: Currently running stages\r\n",
    "- **Pending**: Stages waiting to be scheduled\r\n",
    "- **Completed**: Successfully finished stages\r\n",
    "- **Failed**: Stages that encountered errors\r\n",
    "- **Skipped**: Stages thatages\r\n",
    "4. **Compare similar jobs**: See how changes to your code affec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f45be-faa3-4ca4-93ab-441ab7755b79",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Let's do something that creates multiple stages\n",
    "# First, create two sets of student data\n",
    "\n",
    "class_a = spark.createDataFrame([\n",
    "    (i, f\"Student_{i}\", random.randint(60, 100))\n",
    "    for i in range(1000)\n",
    "], [\"id\", \"name\", \"grade\"])\n",
    "\n",
    "class_b = spark.createDataFrame([\n",
    "    (i, random.choice(['A', 'B', 'C', 'D']))\n",
    "    for i in range(1000)\n",
    "], [\"id\", \"letter_grade\"])\n",
    "\n",
    "# Now let's combine them!\n",
    "combined_classes = class_a.join(class_b, \"id\") \\\n",
    "    .where(\"grade >= 70\") \\\n",
    "    .groupBy(\"letter_grade\") \\\n",
    "    .count() \\\n",
    "    .collect()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab58a2d5-bba4-445f-85e3-008192b81030",
   "metadata": {},
   "source": [
    "### Check the Stages tab - you should see multiple stages because\n",
    " Spark had to:\n",
    " 1. Process class_a data\n",
    " 2. Process class_b data\n",
    " 3. Join them together\n",
    " 4. Group and count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4932cbb-bbe9-4555-8923-a8fbb5b19869",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Exercise\n",
    " Your mission: Figure out which operation is slowest!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b302c7fe-b21a-4e48-b3af-708234878bdd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def investigate_performance():\n",
    "    # Create some data\n",
    "    data1 = spark.createDataFrame([\n",
    "        (i, random.randint(1, 50), random.choice(['X', 'Y', 'Z']))\n",
    "        for i in range(10000)\n",
    "    ], [\"id\", \"score\", \"group\"])\n",
    "    \n",
    "    data2 = spark.createDataFrame([\n",
    "        (i, random.choice(['Red', 'Blue', 'Green']))\n",
    "        for i in range(10000)\n",
    "    ], [\"id\", \"team\"])\n",
    "    \n",
    "    # Do a bunch of operations\n",
    "    result = data1.join(data2, \"id\") \\\n",
    "        .groupBy(\"team\", \"group\") \\\n",
    "        .avg(\"score\") \\\n",
    "        .orderBy(\"team\") \\\n",
    "        .collect()\n",
    "    \n",
    "    return result\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d858cc87-03c3-4f33-9288-60cbc5caf2d0",
   "metadata": {},
   "source": [
    "\n",
    "### Run it and check the UI!\n",
    " Things to look for in the Stages tab:\n",
    " - Which stage took the longest?\n",
    " - How much data was shuffled around?\n",
    " - Did all tasks take about the same time?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d390597d-5b16-44fc-901a-9a4c5f21c7d9",
   "metadata": {},
   "source": [
    "# 5. The Storage Tab\r\n",
    "\r\n",
    "### What is the Storage Tab in Spark?\r\n",
    "\r\n",
    "The Storage tab in Apache Spark's UI provides visibility into your application's data caching behavior. Caching (or \"persisting\") data is one of the most powerful optimization techniques in Spark, allowing frequently used datasets to be stored in memory or disk for faster access.\r\n",
    "\r\n",
    "## Key Components of the Storage Tab\r\n",
    "\r\n",
    "### RDD/DataFrame/Dataset Listing\r\n",
    "\r\n",
    "1. **Cached Data Entries**:\r\n",
    "   - **Name**: Identifier for each cached dataset\r\n",
    "   - **Storage Level**: How and where data is stored\r\n",
    "   - **Cache Size**: Memory/disk space utilized\r\n",
    "   - **Fraction Cached**: Percentage of dataset actually stored\r\n",
    "   - **Number of Partitions**: How the data is divided\r\n",
    "\r\n",
    "2. **Storage Level Details**:\r\n",
    "   - **MEMORY_ONLY**: Data stored in memory as deserialized Java objects\r\n",
    "   - **MEMORY_AND_DISK**: Data stored in memory first, overflow goes to disk\r\n",
    "   - **MEMORY_ONLY_SER**: Data stored in memory in serialized format (more compact)\r\n",
    "   - **DISK_ONLY**: Data stored only on disk\r\n",
    "   - **OFF_HEAP**: Data stored in off-heap memory (outside JVM)\r\n",
    "\r\n",
    "3. **Additional Information**:\r\n",
    "   - **Creation Time**: When the data was cached\r\n",
    "   - **Data Distribution**: How data is spread across executors\r\n",
    "   - **Last Access Time**: When cached data was last used\r\n",
    "\r\n",
    "## How Caching Works in Spark\r\n",
    "\r\n",
    "When you call `.cache()` or `.persist()` on an RDD, DataFrame, or Dataset:\r\n",
    "\r\n",
    "1. The first time an action is called on the dataset, Spark computes it and stores the result\r\n",
    "2. Subsequent operations using this data will read from cache instead of recomputing\r\n",
    "3. Cached data remains until explicitly unpersistet or u1.\n",
    "## Why the Storage Tab Mat\n",
    "ers\r\n",
    "\r\n",
    "### Verifying Caching\r\n",
    "\r\n",
    "- Check if datasets are actually cached and how much was cached (vs. spilled to disk).\r\n",
    "- Monitor memory usage to 2.P exceeding limits or wasting space.\r\n",
    "\r\n",
    "### Performance Optimization\r\n",
    "\r\n",
    "- Identify which cached datasets are used frequently.\r\n",
    "- Check partition distribution for skew or imbalance.\r\n",
    "- Evaluate storage level (e.g., memory vsve your application's performance and resource utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e97612cb-d02e-4e40-8631-c7304ebeb244",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o73.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 1.0 failed 1 times, most recent failure: Lost task 6.0 in stage 1.0 (TID 14) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:381)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 43 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:381)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 43 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m big_df\u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Force it to actually load into memory\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m big_df\u001b[38;5;241m.\u001b[39mcount()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1240\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m \n\u001b[0;32m   1220\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcount())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o73.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 1.0 failed 1 times, most recent failure: Lost task 6.0 in stage 1.0 (TID 14) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:381)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 43 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:381)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 43 more\r\n"
     ]
    }
   ],
   "source": [
    "# Let's make Spark remember something\n",
    "big_df = spark.createDataFrame([\n",
    "    (i, f\"Data_{i}\")\n",
    "    for i in range(10000)\n",
    "], [\"id\", \"data\"])\n",
    "\n",
    "# Tell Spark to keep this in memory\n",
    "big_df.cache()\n",
    "\n",
    "# Force it to actually load into memory\n",
    "big_df.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6777449b-605c-4069-a442-3a2f5e26ff2b",
   "metadata": {},
   "source": [
    "### Now check the Storage tab!\n",
    "How much memory is it using?\n",
    "\n",
    "Is everything in memory or did some spill to disk?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a922ac-20ce-4444-b80f-98a6bf094baf",
   "metadata": {},
   "source": [
    "# 6. The SQL Tab:\r\n",
    "\r\n",
    "## What it Shows\r\n",
    "The SQL Tab displays all Spark SQL queries and DataFrame operations executed in your application, helping you understand query performance and execution det. ailThe SQL Tab is your window into how Spark transforms your high-level code into execution steps, critical for optimizing query performance.s.\r\n",
    "\r\n",
    "## Key Components\r\n",
    "\r\n",
    "### Query List View\r\n",
    "- **Query ID**: Unique identifier for each query\r\n",
    "- **Description**: The SQL statement or DataFrame operation\r\n",
    "- **Duration**: Execution time (most important metric!)\r\n",
    "- **Status**: Running, completed, or failed\r\n",
    "\r\n",
    "### Query Details View (when clicking a query)\r\n",
    "- **Logical Plan**: The abstract representation of what you want to compute\r\n",
    "- **Physical Plan**: The actual execution strategy Spark uses\r\n",
    "  - Shows operations like joins, filters, aggregations\r\n",
    "  - Includes important execution details (join types, estimated sizes)\r\n",
    "- **Visualization**: Interactive graph showing query execution flow\r\n",
    "\r\n",
    "##\n",
    "- erformancthe e Analysis\r\n",
    "- Identify slowest queries by sorting by duration\r\n",
    "- Look for expensive operations (large shuffles, cartesian products)\r\n",
    "- Check if filters are being pushed down early in execution\r\n",
    "- Spot inries (shuffles)\r\n",
    "\r\n",
    "### Debugging\r\n",
    "- Find failed qrace complex query execution paths\r\n",
    "- Connecfective Spark development and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "038c06c2-3acc-4de7-82ab-6c1dc09304c0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o80.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 2.0 failed 1 times, most recent failure: Lost task 6.0 in stage 2.0 (TID 22) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:381)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 43 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:381)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 43 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 11\u001b[0m\n\u001b[0;32m      2\u001b[0m big_df\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Now run a SQL query\u001b[39;00m\n\u001b[0;32m      5\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m    SELECT \u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m        SUBSTRING(data, 1, 5) as short_data,\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124m        COUNT(*) as count\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124m    FROM my_table\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m    GROUP BY SUBSTRING(data, 1, 5)\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_string(n, truncate, vertical))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical)\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o80.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 2.0 failed 1 times, most recent failure: Lost task 6.0 in stage 2.0 (TID 22) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:381)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 43 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:381)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$getOrElseUpdate$1(BlockManager.scala:1372)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 43 more\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# First, let's make our DataFrame available for SQL\n",
    "big_df.createOrReplaceTempView(\"my_table\")\n",
    "\n",
    "# Now run a SQL query\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        SUBSTRING(data, 1, 5) as short_data,\n",
    "        COUNT(*) as count\n",
    "    FROM my_table\n",
    "    GROUP BY SUBSTRING(data, 1, 5)\n",
    "\"\"\").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e40d1-e3ec-49ce-8ca4-150cefb9a639",
   "metadata": {},
   "source": [
    "### Check out the SQL tab to see:\n",
    " What steps Spark took to run your query?\n",
    " \n",
    " How long each part took?\n",
    " \n",
    " If there are any parts that could be faster?\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94919c01-fb61-4d5c-b87b-7467ae8c5f7d",
   "metadata": {},
   "source": [
    "# 7.The Environment Tab\r\n",
    "\r\n",
    "## What it Shows\r\n",
    "The Environment Tab provides a comprehensive view of the configuration settings and runtime environment of your Spark applicon.ati\n",
    "The Environment Tab is your reference for understanding exactly how your Spark application is configured, critical for debugging configuration issues and ensuring optimal performance settin\n",
    "nation issues and ensuring optimal performance settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074abb28-6b72-434c-aa77-ece77bb62b11",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# We can change some settings\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")  # Default is 200!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378cd66f-790c-4208-b3b6-6232b688e73a",
   "metadata": {},
   "source": [
    "### Check the Environment tab to see:\n",
    " What settings are active\n",
    " \n",
    " How much memory Spark can use\n",
    " \n",
    " What version of Spark you're running\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bb4a3d-8c2c-4d85-8ddf-285ad5a516e4",
   "metadata": {},
   "source": [
    "# 8. The Executors Tab\r\n",
    "\r\n",
    "## What it Shows\r\n",
    "The Executors Tab provides information about the worker processes (executors) running your Spark application tasks across the clur.steThe Executors Tab gives you visibility into the physical resources processing your data, essential for diagnosing performance issues and optimizing resource allocatior.\r\n",
    "\r\n",
    "## Key Compoents\r\n",
    "\r\n",
    "### Summary Statistics\r\n",
    "- **Active Executors**: Total number of running executors\r\n",
    "- **Total Cores**: Available CPU cores across all executors\r\n",
    "- **Total Memory**: Aggregate memory allocation\r\n",
    "\r\n",
    "### Per-Executor Information\r\n",
    "- **Executor ID**: Unique identifier for each executor\r\n",
    "- **Storage Memory**: Memory used for caching data\r\n",
    "- **Task Metrics**:\r\n",
    "  - **Completed/Failed Tasks**: Task execution status\r\n",
    "  - **Task Time**: Processing duration\r\n",
    "  - **GC Time**: Time spent on garbage collection\r\n",
    "  - **Shuffle Read/Write**: Data transferred between executors\r\n",
    "\r\n",
    "#\n",
    "### Resource Monitoring\r\n",
    "- Verify executor count matches your configuration\r\n",
    "- Monitor memor distribution is balanced\r\n",
    "- Track GC time relative to task time (high ratio sure)\r\n",
    "\r\n",
    "### Troubleshooting\r",
    "s\r\n",
    "- Look for skewed workloads\r\n",
    "- Detect memory-related issues\r\n",
    "- Findetrics\r\n",
    "\r\n",
    "### Performance Analysis\r\n",
    "- - pck for data skew in shuffle metrics\r",
    "oerformance issues and optimizing resource allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4118769d-3065-43a3-bbeb-645e2f0986fa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Let's create some work for our executors\n",
    "# (This will be more interesting on a real cluster, but we can still see it in local mode)\n",
    "\n",
    "# Create a bigger dataset to process\n",
    "big_data = spark.createDataFrame([\n",
    "    (i, random.randint(1, 100), random.choice(['A', 'B', 'C', 'D', 'E']))\n",
    "    for i in range(50000)  # 50,000 rows should be enough to see some action\n",
    "], [\"id\", \"value\", \"category\"])\n",
    "\n",
    "# Do some processing that will use our executors\n",
    "processed = big_data.repartition(8) \\\n",
    "    .groupBy(\"category\") \\\n",
    "    .agg({\"value\": \"sum\", \"value\": \"avg\", \"value\": \"max\"}) \\\n",
    "    .cache() \\\n",
    "    .collect()\n",
    "\n",
    "# Processing another dataset to give executors more work\n",
    "another_data = spark.createDataFrame([\n",
    "    (i, f\"item_{i}\", random.randint(1, 1000))\n",
    "    for i in range(20000)\n",
    "], [\"id\", \"name\", \"price\"])\n",
    "\n",
    "another_result = another_data.join(big_data, \"id\", \"left\") \\\n",
    "    .where(\"price > value\") \\\n",
    "    .groupBy(\"category\") \\\n",
    "    .count() \\\n",
    "    .collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a93ccca-abfe-4665-bd69-6e79d3c58841",
   "metadata": {},
   "source": [
    "#### ðŸŽ¯ Exercise: Executor Detective Work!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80869b38-1436-49d7-bfa7-2862687019dd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Let's make our executors work harder and see what happens\n",
    "def investigate_executors():\n",
    "    # Create 3 datasets to process\n",
    "    data1 = spark.createDataFrame([\n",
    "        (i, random.randint(1, 100), f\"product_{i}\")\n",
    "        for i in range(20000)\n",
    "    ], [\"id\", \"quantity\", \"product\"])\n",
    "    \n",
    "    data2 = spark.createDataFrame([\n",
    "        (i, random.choice(['Region1', 'Region2', 'Region3', 'Region4']))\n",
    "        for i in range(20000)\n",
    "    ], [\"id\", \"region\"])\n",
    "    \n",
    "    data3 = spark.createDataFrame([\n",
    "        (i, random.randint(10, 1000), random.choice([True, False]))\n",
    "        for i in range(20000)\n",
    "    ], [\"id\", \"price\", \"in_stock\"])\n",
    "    \n",
    "    # Cache the first dataset\n",
    "    data1.cache()\n",
    "    data1.count()  # Materialize the cache\n",
    "    \n",
    "    # Run a complex job\n",
    "    result = data1.join(data2, \"id\") \\\n",
    "        .join(data3, \"id\") \\\n",
    "        .where(\"in_stock = true\") \\\n",
    "        .groupBy(\"region\") \\\n",
    "        .agg({\"price\": \"sum\", \"quantity\": \"sum\"}) \\\n",
    "        .orderBy(\"region\") \\\n",
    "        .collect()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the investigation\n",
    "result = investigate_executors()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e677a1c0-a782-4b0c-baa4-ee99c67b80e3",
   "metadata": {},
   "source": [
    "### Now check the Executors tab and answer these questions:\n",
    " 1. How many executors do you see?\n",
    " 2. How much memory is each executor using?\n",
    " 3. How many tasks has each executor completed?\n",
    " 4. Is there a big difference in task runtime between executors?\n",
    " 5. Do you see any \"spill\" (data written to disk instead of kept in memory)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923bed07-8493-4eec-9604-749403fd7ebd",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Final Exercise: Put It All Together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2703468a-2aed-443e-aab7-6e155476edbe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a mini school database and analyze it\n",
    "def analyze_school_data():\n",
    "    # First, let's set some configuration to make things interesting\n",
    "    # This will show up in the Environment tab\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")  # Default is 200\n",
    "    spark.conf.set(\"spark.executor.memory\", \"1g\")        # Just for demo purposes\n",
    "    \n",
    "    # Create students table\n",
    "    students = spark.createDataFrame([\n",
    "        (i, f\"Student_{i}\", random.randint(14, 18))\n",
    "        for i in range(1000)\n",
    "    ], [\"id\", \"name\", \"age\"])\n",
    "    \n",
    "    # Create grades table\n",
    "    grades = spark.createDataFrame([\n",
    "        (random.randint(1, 1000), \n",
    "         random.choice(['Math', 'Science', 'History']),\n",
    "         random.randint(60, 100))\n",
    "        for _ in range(5000)\n",
    "    ], [\"student_id\", \"subject\", \"grade\"])\n",
    "    \n",
    "    # Create a third table for more complex joins\n",
    "    attendance = spark.createDataFrame([\n",
    "        (random.randint(1, 1000),\n",
    "         random.choice(['Math', 'Science', 'History']),\n",
    "         random.randint(70, 100))\n",
    "        for _ in range(7000)\n",
    "    ], [\"student_id\", \"subject\", \"attendance_pct\"])\n",
    "    \n",
    "    # Cache one of our tables to see memory usage in the Storage and Executors tabs\n",
    "    grades.cache()\n",
    "    grades.count()  # Materialize the cache\n",
    "    \n",
    "    # Make them available for SQL\n",
    "    students.createOrReplaceTempView(\"students\")\n",
    "    grades.createOrReplaceTempView(\"grades\")\n",
    "    attendance.createOrReplaceTempView(\"attendance\")\n",
    "    \n",
    "    # Run a more complex query with multiple joins\n",
    "    result = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            s.name,\n",
    "            s.age,\n",
    "            AVG(g.grade) as avg_grade,\n",
    "            AVG(a.attendance_pct) as avg_attendance,\n",
    "            COUNT(DISTINCT g.subject) as subjects_taken\n",
    "        FROM students s\n",
    "        JOIN grades g ON s.id = g.student_id\n",
    "        JOIN attendance a ON s.id = a.student_id AND g.subject = a.subject\n",
    "        GROUP BY s.name, s.age\n",
    "        HAVING AVG(g.grade) > 80 AND AVG(a.attendance_pct) > 85\n",
    "        ORDER BY avg_grade DESC, avg_attendance DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create a second query to generate more executor work\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            g.subject,\n",
    "            COUNT(*) as student_count,\n",
    "            AVG(g.grade) as avg_subject_grade,\n",
    "            AVG(a.attendance_pct) as avg_subject_attendance\n",
    "        FROM grades g\n",
    "        JOIN attendance a ON g.student_id = a.student_id AND g.subject = a.subject\n",
    "        GROUP BY g.subject\n",
    "    \"\"\").show()\n",
    "    \n",
    "    # Show the main result\n",
    "    result.show()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run it!\n",
    "school_analysis = analyze_school_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc53777-5f89-446e-a696-ee2d29c41a46",
   "metadata": {},
   "source": [
    "# Your mission - investigate ALL the tabs:\n",
    "1. Check the Jobs tab - how many jobs were created?\n",
    "2. Look at the Stages tab - which stage took longest?\n",
    "3. Check the SQL tab - can you understand the query plan?\n",
    "4. Look at the Executors tab:\n",
    "    - How many tasks did each executor process?\n",
    "    - Is memory usage balanced across executors?\n",
    "    - Do you see any data being spilled to disk?\n",
    "5. Check the Environment tab:\n",
    "    - Find the custom configurations we set (hint: shuffle partitions)\n",
    "    - What's the value of spark.executor.memory?\n",
    "    - How many cores is Spark using?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b8c255-2652-4600-8fc7-dc298d0c7938",
   "metadata": {},
   "source": [
    "##  Cleaning Up ðŸ§¹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1c8c65-40c6-43fe-b08d-084ab5f6d39c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Stop Spark (like turning off your car)\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
