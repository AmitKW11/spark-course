{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a62d8cbf-5eda-47b1-a611-afd6d635d3b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spark SQL\n",
    "\n",
    "Remember that Spark is a data processsing engine, not a database.\n",
    "\n",
    "See https://spark.apache.org/docs/latest/sql-programming-guide.html \n",
    "\n",
    "Most of the text here is taken from  [SDG] chapter 10 \"Spark SQL\" .\n",
    "\n",
    "Spark SQL is a Spark module for structured data processing.\n",
    "\n",
    "Do not confuse with reading/writing to an RDBMS. \n",
    "You can run SQL query on a dataframe that you created from any data source.\n",
    "\n",
    "In a nutshell, with Spark SQL you can run SQL queries against views or tables organized into\n",
    "databases. You also can use system functions or define user functions and analyze query plans in\n",
    "order to optimize their workloads. This integrates directly into the DataFrame and Dataset API,\n",
    "and as we saw in previous chapters, you can choose to express some of your data manipulations\n",
    "in SQL and others in DataFrames and they will **compile to the same underlying code**. [SDG]\n",
    "\n",
    "## What is Apache Hive?\n",
    "Before Sparkâ€™s rise, Hive was the de facto big data SQL access layer. Originally developed at Facebook, Hive became an incredibly popular tool across industry for *performing SQL operations on big data*. In many ways it helped propel Hadoop into different industries because analysts could run SQL queries[SDG]\n",
    "\n",
    "\n",
    "## NOTE\n",
    "Spark SQL is intended to operate as an online **analytic** processing (OLAP) database, not an online transaction processing (OLTP) database. This means that it is not intended to perform extremely low-latency queries. [SDG]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9254547-4479-4552-afe0-c0ca42d8e7b0",
   "metadata": {},
   "source": [
    "TODO: take from \"streaming_book.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58f9abd-64cd-44cc-b09b-14a9037bee19",
   "metadata": {},
   "source": [
    "You can completely interoperate between SQL and DataFrames, as you see\n",
    "fit. For instance, you can create a DataFrame, manipulate it with SQL, and then manipulate it\n",
    "again as a DataFrame."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d6535ee-031d-459a-a183-fb0e890ca9ab",
   "metadata": {},
   "source": [
    "spark.read.json(\"/data/flight-data/json/2015-summary.json\")\\\n",
    ".createOrReplaceTempView(\"some_sql_view\") # DF => SQL\n",
    "spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count)\n",
    "FROM some_sql_view GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\\\n",
    ".where(\"DEST_COUNTRY_NAME like 'S%'\").where(\"`sum(count)` > 10\")\\\n",
    ".count() # SQL => DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f3456-c8fe-4bd1-9944-83b0e4e631ec",
   "metadata": {},
   "source": [
    "# Views\n",
    "\n",
    "To an end user, views are displayed as tables, except rather than rewriting all of the data to a new\n",
    "location, they simply perform a transformation on the source data at query time.\n",
    "\n",
    "Views are created in the `default` database"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e2c37d5-1bb5-4a51-8c73-129966b73a88",
   "metadata": {},
   "source": [
    "CREATE VIEW just_usa_view AS\n",
    "    SELECT * FROM flights WHERE dest_country_name = 'United States'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6241f734-06d4-461f-8407-b78f7356d2dc",
   "metadata": {},
   "source": [
    "A view is effectively a **transformation** and Spark will perform it only at query time. This means\n",
    "that it will only apply that filter after you actually go to query the table (and not earlier).\n",
    "Effectively, views are equivalent to creating a new DataFrame from an existing DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e3d010b-8a1a-4b60-b707-9e91ae7a9718",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/02/26 09:38:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/26 09:38:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/02/26 09:38:13 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/02/26 09:38:13 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/02/26 09:38:13 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "datapath = \"../data/sdg/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "008d176d-1a1f-43c9-80eb-ae3eeca20ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>537226</td>\n",
       "      <td>22811</td>\n",
       "      <td>SET OF 6 T-LIGHTS CACTI</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-12-06 08:34:00</td>\n",
       "      <td>2.95</td>\n",
       "      <td>15987.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>537226</td>\n",
       "      <td>21713</td>\n",
       "      <td>CITRONELLA CANDLE FLOWERPOT</td>\n",
       "      <td>8</td>\n",
       "      <td>2010-12-06 08:34:00</td>\n",
       "      <td>2.10</td>\n",
       "      <td>15987.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  InvoiceNo StockCode                  Description  Quantity  \\\n",
       "0    537226     22811     SET OF 6 T-LIGHTS CACTI          6   \n",
       "1    537226     21713  CITRONELLA CANDLE FLOWERPOT         8   \n",
       "\n",
       "           InvoiceDate  UnitPrice  CustomerID         Country  \n",
       "0  2010-12-06 08:34:00       2.95     15987.0  United Kingdom  \n",
       "1  2010-12-06 08:34:00       2.10     15987.0  United Kingdom  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(datapath + \"/retail-data/by-day/2010*.csv\")\n",
    "df.createOrReplaceTempView(\"retail_data\")\n",
    "schema = df.schema\n",
    "df.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e6ff3-f916-43e1-b9dd-871a853ee55c",
   "metadata": {},
   "source": [
    "The 'retail_data' is a temporary view. It will live as long as the current SparkSession. <br>\n",
    "This view cannot be shared with other Spark applications or databases. There are ways to do this, but not covered here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3a7490-0313-4dfe-b4e1-dd028b74ab73",
   "metadata": {},
   "source": [
    "# Let's run some code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a560d3b6-95ae-441f-a0c5-fcb43884bcc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   13408.0|{2010-12-01 00:00...|1024.6800000000003|\n",
      "|   17460.0|{2010-12-01 00:00...|              19.9|\n",
      "|   15235.0|{2010-12-01 00:00...|              79.5|\n",
      "|   13495.0|{2010-12-06 00:00...|510.94999999999993|\n",
      "|   14769.0|{2010-12-17 00:00...|            347.01|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, column, desc, col\n",
    "df\\\n",
    ".selectExpr(\n",
    "\"CustomerId\",\n",
    "\"(UnitPrice * Quantity) as total_cost\",\n",
    "\"InvoiceDate\")\\\n",
    ".groupBy(\n",
    "col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    ".sum(\"total_cost\")\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ee7e541-597a-4f5f-b27a-e816db738f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-------------------+\n",
      "|CustomerId|        total_cost|        InvoiceDate|\n",
      "+----------+------------------+-------------------+\n",
      "|   15987.0|17.700000000000003|2010-12-06 08:34:00|\n",
      "|   15987.0|              16.8|2010-12-06 08:34:00|\n",
      "|   15987.0|              11.9|2010-12-06 08:34:00|\n",
      "|   15987.0| 9.899999999999999|2010-12-06 08:34:00|\n",
      "|   15987.0|              10.5|2010-12-06 08:34:00|\n",
      "+----------+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql(\"select 1+1\").show() \n",
    "\n",
    "# TODO: fix the SQL syntax to represent the same query as above\n",
    "spark.sql(\"\"\"select  CustomerId ,  UnitPrice * Quantity as total_cost, InvoiceDate FROM retail_data\n",
    "          \"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e41a2-646f-49a8-a8b1-21d20014c6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a4afaa4-07a9-45f0-95af-74cb7366e902",
   "metadata": {},
   "source": [
    "# Complex Types\n",
    "Complex types are a departure from standard SQL and are an incredibly powerful feature that\n",
    "does not exist in standard SQL. Understanding how to manipulate them appropriately in SQL is\n",
    "essential. There are three core complex types in Spark SQL: **structs, lists, and maps**.\n",
    "\n",
    "This is an advanced topic.<br>\n",
    "For examples, check the book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2652b8d9-e1e1-4d38-b20a-edf0de38b92e",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "\n",
    "When performing queries such as `groupby(\"column\").sum()`, all the data has to be scanned, using sequential read.\n",
    "\n",
    "What is we have `select a,b where b=\"wine\"` and there are few matching rows? \n",
    "\n",
    "Spark does not support indexing of the data (not to be confused with indexing of the database that we read to create the dataframe!).\n",
    "\n",
    "Instead, you should rely on *partitioning* by the columns you plan to query. This should provide the needed speed.\n",
    "\n",
    "* can a partition be read in parallel by several threads?\n",
    "\n",
    "See interesting disucssion in StackOverflow:\n",
    "https://stackoverflow.com/questions/36938976/why-spark-sql-considers-the-support-of-indexes-unimportant .\n",
    "\n",
    "Microsoft implemented a prototype indexer, but I don't know if it was integrated into Spark: https://www.databricks.com/session_na20/hyperspace-an-indexing-subsystem-for-apache-spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd02b98-cc97-46ab-a9db-1aaf779c5e54",
   "metadata": {},
   "source": [
    "# Check youreself\n",
    "* Where are database file stored?\n",
    "* Can Spark do UPDATE TABLE? Why?\n",
    "* What is the role of View?\n",
    "* is Indexing needed? if not, what should be used instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8f3f36-d2bd-45f2-b815-c59525ef3eba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
